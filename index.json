[{"content":"Here we go again for a new post üòÅ After publishing my Quarkus book last September, I got many requests about creating reactive applications with the Quarkus Framework.\nToday, I will show you how to make a Reactive CRUD Application with the Quarkus Framework and backed by a PostgreSQL Database.\nBuilding our the Reactive Quarkus Application The full source code is available on my Github.\nWe will generate our project using the code.quarkus.io. Our application will need 8 extensions from the portal:\n RESTEasy Reactive RESTEasy Reactive Jackson Hibernate Reactive with Panache Hibernate Validator Reactive PostgreSQL client JDBC Driver - PostgreSQL Flyway SmallRye OpenAPI   Other than these dependencies, we will need some more:\n Lombok for simplifying the code and avoiding all boilerplate stuff like getters/setter, constructors, etc. Mapstruct is used to implement a simplified mapping mechanism between JPA entities and DTOs. Testcontainers is used to provide lightweight database instances for JUnit Tests.  We will need manually to the pom.xml:\n1 2 3 4 5 6 7 8 9 10  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.22\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.2.Final\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   We will need also Test scope dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testcontainers\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testcontainers\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;testcontainers\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testcontainers\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;   In this tutorial, we will use a great feature available in Quarkus called DevServices,¬†which helps you to provision sevices (like DB, Keycloak, Cache Engines, Kafka, etc.) for Dev and Test purposes.\nBased on the Quarkus Guides:\n Quarkus supports the automatic provisioning of unconfigured services in development and test mode. We refer to this capability as Dev Services. From a developer‚Äôs perspective this means that if you include an extension and don‚Äôt configure it then Quarkus will automatically start the relevant service (usually using¬†Testcontainers¬†behind the scenes) and wire up your application to use this service.\n\u0026ndash; Quarkus - Dev Services Overview\n ‚ö†Ô∏è ‚ö†Ô∏è You need to have Docker installed in order to enjoy the Quarkus Dev Services capability.\nWe will start by configuring DevServices in the application.properties :\n1 2 3 4 5 6 7 8 9 10 11  quarkus.datasource.db-kind=postgresql quarkus.datasource.devservices.enabled=true quarkus.datasource.devservices.image-name=postgres:13 quarkus.datasource.devservices.port=5432 quarkus.datasource.username=postgresx quarkus.datasource.password=postgresx quarkus.datasource.jdbc=false quarkus.flyway.migrate-at-start=false quarkus.datasource.reactive.url=postgresql://localhost:5432/postgres quarkus.datasource.reactive.max-size=20   The line(s):\n 2, 3 and 4 will activate the DevServices capability for the Datasource, and define the container image as postgres:13 and define the container port to 5432. 5 and 6 will define the Database container credentials 7 will disable the JDBC access to the database 8 will disable the automatic Flyway migration at startup as it\u0026rsquo;s not correctly working in the reactive mode. There is already an issue Hibernate reactive + Flyway extension causes UnsatisfiedResolutionException ¬∑ Issue #10716 ¬∑ quarkusio/quarkus (github.com) 10 and 11 will configure the reactive database connection URL and properties  We will be developing a Reactive version of our famous Books CRUD Application. Our Book entity looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  @Data @NoArgsConstructor @Entity @Table(name = \u0026#34;books\u0026#34;) public class Book { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String title; @Size(min = 13, max = 20) private String isbn; private String author; @DecimalMin(\u0026#34;0.0\u0026#34;) private BigDecimal price; public Book(String title, String isbn, String author, BigDecimal price) { this.title = title; this.isbn = isbn; this.author = author; this.price = price; } }   We will start by defining the initial Flyway migration script in the resources/db/migration folder:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  CREATESEQUENCEhibernate_sequenceSTARTWITH1INCREMENTBY1;CREATETABLEbooks(idBIGINTNOTNULLPRIMARYKEYDEFAULTnextval(\u0026#39;hibernate_sequence\u0026#39;),titleVARCHAR(100),isbnVARCHAR(20)CONSTRAINTisbn_size_between_13_20CHECK(char_length(isbn)\u0026gt;=13),authorVARCHAR(100),priceNUMERIC(6,2)CONSTRAINTpositive_priceCHECK(price\u0026gt;=0));INSERTINTObooks(title,isbn,author,price)VALUES(\u0026#39;Pairing Apache Shiro and Java EE 7\u0026#39;,\u0026#39;978-1-365-12404-4\u0026#39;,\u0026#39;Nebrass Lamouchi\u0026#39;,0);INSERTINTObooks(title,isbn,author,price)VALUES(\u0026#39;Playing with Java Microservices on Kubernetes and OpenShift\u0026#39;,\u0026#39;9782956428510\u0026#39;,\u0026#39;Nebrass Lamouchi\u0026#39;,9.18);INSERTINTObooks(title,isbn,author,price)VALUES(\u0026#39;Pro Java Microservices with Quarkus and Kubernetes\u0026#39;,\u0026#39;9781484271698\u0026#39;,\u0026#39;Nebrass Lamouchi\u0026#39;,65);  Then, we need to create a Flyway migration component that will use the DevServices credentials to load this script to our Database:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  @ApplicationScoped public class FlywayMigrationService { @ConfigProperty(name = \u0026#34;quarkus.datasource.reactive.url\u0026#34;) String dbUrl; @ConfigProperty(name = \u0026#34;quarkus.datasource.username\u0026#34;) String dbUsername; @ConfigProperty(name = \u0026#34;quarkus.datasource.password\u0026#34;) String dbPassword; public void runFlywayMigration(@Observes StartupEvent event) { Flyway flyway = Flyway.configure() .dataSource(\u0026#34;jdbc:\u0026#34; + dbUrl, dbUsername, dbPassword) .load(); flyway.migrate(); } }   The FlywayMigrationService will grab the DevService Database credentials and will load all the migration scripts stored in the Flyway directory.\nNext, we will create a Reactive Repository to manage our Books in our DB:\n1 2 3  @ApplicationScoped public class BookRepository implements PanacheRepositoryBase\u0026lt;Book, Long\u0026gt; { }    ‚ÑπÔ∏è The PanacheRepositoryBase represents a Repository for a specific type of entity Entity. Implementing this repository will gain you similar useful methods that are on Spring Data JpaRepository like:\n  findAll() findById() persist() update() delete() etc..  Then, we will create the BookService component with an injected BookRepository:\n1 2 3 4 5  @ApplicationScoped @RequiredArgsConstructor public class BookService { private final BookRepository bookRepository; }   Then, in the Service layer, we will implement the CRUD operations thru the Repository in the Reactive mode.\nImplementing the findAll() method To grab all entities of the Book entity, we will be call the findAll() method of the BookRepository. In the PanacheRepositoryBase interface, the findAll method returns a PanacheQuery\u0026lt;Entity\u0026gt;.\n ‚ÑπÔ∏è A PanacheQuery is an interface representing an entity query, which abstracts the use of paging, getting the number of results, and operating on List or Stream. The PanacheQuery interface will get its implementation based on the shipped Panache dependency that we have. In our case it will be implemented in the PanacheQueryImpl class from the io.quarkus.hibernate.reactive.panache.runtime package from the quarkus-hibernate-reactive-panache extension.\n The list() method from the PanacheQueryImpl class returns a Uni\u0026lt;List\u0026lt;T\u0026gt;\u0026gt;.\nBut, what is the Uni type ?\n A¬†Uni\u0026lt;T\u0026gt;¬†is a specialized stream that emits only an item or a failure. Typically,¬†Uni\u0026lt;T\u0026gt;¬†are great to represent asynchronous actions such as a remote procedure call, an HTTP request, or an operation producing a single result.\nUni\u0026lt;T\u0026gt;¬†provides many operators that create, transform, and orchestrate¬†Uni sequences.\n\u0026ndash; SmallRye Mutiny documentation\n In the BookService, we can use the list() method from the PanacheQueryImpl class to grab all the books. The findAll() will look like:\n1 2 3  public Uni\u0026lt;List\u0026lt;Book\u0026gt;\u0026gt; findAll() { return bookRepository.findAll().list(); }   ‚ÑπÔ∏è Actually, there is a shortcut to the findAll().list() in the PanacheRepositoryBase interface, but I wanted to take you in a tour to show you how things are already made.\nThen, we will create the BookResource to expose the findAll() operation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  @RequiredArgsConstructor @Produces(MediaType.APPLICATION_JSON) @Path(\u0026#34;/api/books\u0026#34;) @RequestScoped public class BookResource { private final BookService bookService; @GET public Uni\u0026lt;Response\u0026gt; findAll() { return bookService.findAll() .map(data -\u0026gt; ok(data).build()); } }   We are wrapping the List\u0026lt;Book\u0026gt; inside the Uni\u0026lt;Response\u0026gt;.\nThen, if you run the Quarkus application using the mvn quarkus:dev command, then we can test the findAll REST API using the Swagger UI available on http://localhost:8080/q/swagger-ui/:\nfindAll REST API - Swagger UI\n Implementing the save() method After implementing the findAll() method, we need to create the one that inserts records in the database üòÖ\nIn our REST API, the save() method is called with JSON Payload holding a new book details. These details can be defined as a DTO Model class that I will call BookDTO:\n1 2 3 4 5 6 7 8 9 10  @lombok.Data @lombok.NoArgsConstructor @lombok.AllArgsConstructor public class BookDTO { private Long id; private String title; private String isbn; private String author; private BigDecimal price; }   In the BookResource class we need to add a new method that is called using the HTTP Post Verb and that consumes as JSON Payload the BookDTO:\n1 2 3 4 5 6 7  @POST @Consumes(MediaType.APPLICATION_JSON) public Uni\u0026lt;Response\u0026gt; create(BookDTO bookDTO) { return bookService.save(bookDTO) \u0026lt;1\u0026gt; .onItem().ifNotNull().transform(book -\u0026gt; ok(book).build()) \u0026lt;2\u0026gt; .onItem().ifNull().continueWith(status(BAD_REQUEST)::build); \u0026lt;3\u0026gt; }   The line(s):\n 1: will delegate the saving task to the bookService.save() method. 2: if the save() succeeds then we will return the created Book entry in the Response body. 3: if the save() fails then we will return an HTTP Bad Request (400) in the Response body.  Now, we need to create the save() method in the BookService. This is will be obviously calling the BookRepository save method:\n1 2 3 4 5 6 7 8  public Uni\u0026lt;Book\u0026gt; save(BookDTO dto) { return bookRepository.persistAndFlush(new Book( dto.getTitle(), dto.getIsbn(), dto.getAuthor(), dto.getPrice() )); }   You can notice that the persistAndFlush() method returns a Uni\u0026lt;Book\u0026gt; which we already discovered in the previous part of this tutorial.\nHmm.. I have something to modify in my BookService after introducing the BookDTO class. I wonder:\n Why do I need to return the all the Book entity records in my findAll() method ? Why I dont return instead the entity records wrapped as DTO instances ? Is this will be better or it\u0026rsquo;s just an overhead ?  The RESPONSE is: THIS IS MUST BE DONE ‚öîÔ∏èüõ°‚öîÔ∏è But why ?\nThe DTO pattern is highly useful in many cases:\n Returning the plain data directly from the database to the user can be dangerous when the entity records hold sensitive data such as password, payments credentials, adresses, etc. So records mapping to DTOs is a must - as you cannot obfuscate data on the record instance itself üòÅ The DTO offers the ability to provide the data that is needed in different contexts. For example, the findAll() method in a page that needs only the books titles and prices is not the same as a page that needs the books titles and isbns only. DTOs here will be offering the possibility to create a separate DTO dedicated for each need.   üí° It\u0026rsquo;s true that in our example none of these needs are present, but I will use the DTO for learning purposes ü•π\n My findAll() method with DTOs will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13  public Uni\u0026lt;List\u0026lt;BookDTO\u0026gt;\u0026gt; findAll() { return bookRepository.findAll() .stream() .map(bookEntity -\u0026gt; new BookDTO( bookEntity.getId(), bookEntity.getTitle(), bookEntity.getIsbn(), bookEntity.getAuthor(), bookEntity.getPrice() )) .collect() .asList(); }   Hhmmmmmmmm.. in the save() method I instanciated a new Book instance and I passed the attributes to the constructor, and here I passed the attributes from the entity record to the instanciated DTO constructor. Ok this is clear, but headache and it\u0026rsquo;s not beautiful ü§Æ\nOne idea, is to create Mapping methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  private java.util.function.Function\u0026lt;Book, BookDTO\u0026gt; mapBookEntityToDTO() { return bookEntity -\u0026gt; new BookDTO( bookEntity.getId(), bookEntity.getTitle(), bookEntity.getIsbn(), bookEntity.getAuthor(), bookEntity.getPrice() ); } private java.util.function.Function\u0026lt;BookDTO, Book\u0026gt; mapBookDtoToEntity() { return bookDTO -\u0026gt; new Book( bookDTO.getTitle(), bookDTO.getIsbn(), bookDTO.getAuthor(), bookDTO.getPrice() ); }   Then my findAll() and save() methods will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13  public Uni\u0026lt;List\u0026lt;BookDTO\u0026gt;\u0026gt; findAll() { return bookRepository.findAll() .stream() .map(mapBookEntityToDTO()) .collect() .asList(); } public Uni\u0026lt;BookDTO\u0026gt; save(BookDTO dto) { return bookRepository.persistAndFlush( mapBookDtoToEntity().apply(dto)) .map(mapBookEntityToDTO()); }   Ouuuf üò© This is better, but it will be better if there are any cleaner way.. I know it will be crazy to create 2 x N methods if we will have N different DTO for each entity. The good news is that there is an excellent solution to do this: Java bean mappings utility classes !\nHmm üòí the name is no so funky ! I know ! But this is what is called. I won\u0026rsquo;t reinvent the wheel üòÅ\nImplementing the Mapping mechanism In the Java World, there are many libraries that helps to do dynamic mapping from an entity to its corresponding DTO classes. Since 2016, I was trying many of them like: MapStruct, Dozer, ModelMapper, JMapper, etc. But the best one that suited my needs is MapStruct.\nYou can read about benchmarking and performance of the different available libraries here @Baeldung: https://www.baeldung.com/java-performance-mapping-frameworks.\nTo add MapStruct to our application, we start by adding the MapStruct Maven dependency to our pom.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \u0026lt;properties\u0026gt; ... \u0026lt;mapstruct.version\u0026gt;1.4.2.Final\u0026lt;/mapstruct.version\u0026gt; \u0026lt;lombok-mapstruct-binding.version\u0026gt;0.2.0\u0026lt;/lombok-mapstruct-binding.version\u0026gt; ... \u0026lt;/properties\u0026gt; ... \u0026lt;dependencies\u0026gt; ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt;   Then, in the Maven POM build section, we need to configure the compiler to take into consideration the MapStruct processor along with the Lombok processor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${compiler-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;compilerArgs\u0026gt; \u0026lt;arg\u0026gt;-parameters\u0026lt;/arg\u0026gt; \u0026lt;/compilerArgs\u0026gt; \u0026lt;annotationProcessorPaths\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct-processor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;!-- other annotation processors --\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok-mapstruct-binding\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok-mapstruct-binding.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/annotationProcessorPaths\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/build\u0026gt;   Now, we need to create the MapStruct Mapper class that will reference the Entity and the DTO:\n1 2 3 4 5 6 7 8 9 10  @Mapper(componentModel = \u0026#34;cdi\u0026#34;) public interface BookMapper { BookDTO toBookDto(Book book); Book toBookEntity(BookDTO dto); List\u0026lt;BookDTO\u0026gt; toBookDtoList(List\u0026lt;Book\u0026gt; book); void updateBookEntityFromDto(BookDTO dto, @MappingTarget Book book); }   This interface defines:\n the generated mapper as an application-scoped CDI bean and can be retrieved via @Inject annotation. different methods that maps:  an entity to a DTO a DTO to an entity an entities list to a DTOs list an update Entity from a DTO definition    Excellent ! Now, we can add the BookMapper reference for injection in the BookService class:\n1 2 3 4 5 6 7 8 9  @ApplicationScoped @RequiredArgsConstructor public class BookService { private final BookRepository bookRepository; private final BookMapper bookMapper; ... }   Then we can use the BookMapper in our findAll() and save() methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13  public Uni\u0026lt;List\u0026lt;BookDTO\u0026gt;\u0026gt; findAll() { return bookRepository.findAll() .stream() .map(bookMapper::toBookDto) .collect() .asList(); } public Uni\u0026lt;BookDTO\u0026gt; save(BookDTO book) { return bookRepository.persistAndFlush( bookMapper.toBookEntity(book)) .map(bookMapper::toBookDto); }    üí° In the save() method, we used the mapper to create a Book instance from the DTO, and then convert the stored Book record to a DTO instance.\n That\u0026rsquo;s all tale! The MapStruct will do all the magic!\nBehind the scenes, the Maven Compiler will use the MapStruct Processor to implement the MapStruct Mapper via the BookMapperImpl.java class, available after compilation, in the /target/generated-sources/annotations/ directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  @Generated( value = \u0026#34;org.mapstruct.ap.MappingProcessor\u0026#34;, date = \u0026#34;2022-05-27T11:20:38+0200\u0026#34;, comments = \u0026#34;version: 1.4.2.Final, compiler: javac, environment: Java 17.0.3 (GraalVM Community)\u0026#34; ) @ApplicationScoped public class BookMapperImpl implements BookMapper { @Override public BookDTO toBookDto(Book book) { if ( book == null ) { return null; } BookDTO bookDTO = new BookDTO(); bookDTO.setId( book.getId() ); bookDTO.setTitle( book.getTitle() ); bookDTO.setIsbn( book.getIsbn() ); bookDTO.setAuthor( book.getAuthor() ); bookDTO.setPrice( book.getPrice() ); return bookDTO; } @Override public Book toBookEntity(BookDTO dto) { if ( dto == null ) { return null; } Book book = new Book(); book.setId( dto.getId() ); book.setTitle( dto.getTitle() ); book.setIsbn( dto.getIsbn() ); book.setAuthor( dto.getAuthor() ); book.setPrice( dto.getPrice() ); return book; } @Override public List\u0026lt;BookDTO\u0026gt; toBookDtoList(List\u0026lt;Book\u0026gt; book) { if ( book == null ) { return null; } List\u0026lt;BookDTO\u0026gt; list = new ArrayList\u0026lt;BookDTO\u0026gt;( book.size() ); for ( Book book1 : book ) { list.add( toBookDto( book1 ) ); } return list; } @Override public void updateBookEntityFromDto(BookDTO dto, Book book) { if ( dto == null ) { return; } book.setId( dto.getId() ); book.setTitle( dto.getTitle() ); book.setIsbn( dto.getIsbn() ); book.setAuthor( dto.getAuthor() ); book.setPrice( dto.getPrice() ); } }   Great! I feel very comfortable for delegating the DTO Mapping load to MapStruct üòÅ As I always say: \u0026ldquo;A Good Developer is a Lazy Developer\u0026rdquo; üòÇ\nImplementing the findById, findByAuthor and deleteById methods Now, we will continue to implement our CRUD methods - like:\n findById(): find a book using a given book ID. findByAuthor(): find books using a given author name. deleteById(): delete a book using a given book ID.  The implementation looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  public Uni\u0026lt;BookDTO\u0026gt; findById(Long id) { return bookRepository.findById(id) .map(bookMapper::toBookDto); } public Uni\u0026lt;List\u0026lt;BookDTO\u0026gt;\u0026gt; findByAuthor(String author) { return bookRepository .find(\u0026#34;lower(author) like lower(CONCAT(\u0026#39;%\u0026#39;, \u0026#39;\u0026#34; + author + \u0026#34;\u0026#39;, \u0026#39;%\u0026#39;)) \u0026#34;) \u0026lt;1\u0026gt; .stream() .map(bookMapper::toBookDto) .collect() .asList(); } @ReactiveTransactional \u0026lt;2\u0026gt; public Uni\u0026lt;Void\u0026gt; deleteById(Long id) { return bookRepository.deleteById(id).replaceWithVoid(); }   The line:\n 1: defines the Hibernate Query that is used to search by author name with ignore case. 2: runs the method into a reactive Mutiny.Session.Transation.  Then, their REST APIs look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  @RequiredArgsConstructor @Produces(MediaType.APPLICATION_JSON) @Path(\u0026#34;/api/books\u0026#34;) @RequestScoped public class BookResource { private final BookService bookService; ... @GET @Path(\u0026#34;/{id}\u0026#34;) public Uni\u0026lt;Response\u0026gt; findById(@PathParam(\u0026#34;id\u0026#34;) Long id) { return bookService.findById(id) .onItem().ifNotNull().transform(book -\u0026gt; ok(book).build()) .onItem().ifNull().continueWith(status(NOT_FOUND)::build); } @GET @Path(\u0026#34;/author/{name}\u0026#34;) public Uni\u0026lt;Response\u0026gt; findByAuthor(@PathParam(\u0026#34;name\u0026#34;) String name) { return bookService.findByAuthor(name) .onItem().ifNotNull().transform(book -\u0026gt; ok(book).build()) .onItem().ifNull().continueWith(status(NOT_FOUND)::build); } @DELETE @Path(\u0026#34;/{id}\u0026#34;) public Uni\u0026lt;Void\u0026gt; deleteById(@PathParam(\u0026#34;id\u0026#34;) Long id) { return bookService.deleteById(id); } }   That\u0026rsquo;s all tale ! üòÅ We made our first Quarkus Reactive application ! You can now write some integration tests using RestAssured to be sure that all is working as expected ü§©\nIf you have questions, please feel free to get in touch with me üòé\n","permalink":"https://blog.nebrass.fr/playing-with-reactive-quarkus/","summary":"Here we go again for a new post üòÅ After publishing my Quarkus book last September, I got many requests about creating reactive applications with the Quarkus Framework.\nToday, I will show you how to make a Reactive CRUD Application with the Quarkus Framework and backed by a PostgreSQL Database.\nBuilding our the Reactive Quarkus Application The full source code is available on my Github.\nWe will generate our project using the code.","title":"Playing with Reactive Quarkus"},{"content":"","permalink":"https://blog.nebrass.fr/my-upcoming-book-playing-with-authentication-and-authorization-in-owasp-barbarus/","summary":"","title":"My upcoming book: Playing with Authentication and Authorization in OWASP Barbarus"},{"content":"During many career talks I attended, I noticed that some questions come back every time: \u0026quot; As a final year student, how can I get an internship? What can I mention in my CV to make it attractive? \u0026quot; ü§î\nThe problem is that always we start being concerned about internships too late, only some weeks before starting the search operation üí£ Although, finding a good internship needs to be prepared many months before\u0026hellip;\nThe best approach to start working hard since the beginning to avoid any bad surprises:\n Learning Data structures and Algorithms: the most important in the fundamentals that every developer needs to know. Many great books cover this part:  Algorithms by Robert Sedgewick (goodreads.com) Introduction to Algorithms, 3rd Edition (The MIT Press) by Thomas H. Cormen, The MIT Press by Thomas H. Cormen (goodreads.com)   Learning programming languages: choose one main language + one or more second-level languages:  The main language will be your primary language for example Java, Golang, JS, Python, DotNet As it\u0026rsquo;s impossible to live with only one language, you need to know more languages that you will use along with the primary one. For example: if you are doing Java mainly, you will need to learn some JS for front-end development and Python for Datascience/AI.   Learning coding and design best principles: many books and courses are available:  The Robert C. Martin Clean Code Collection by Robert C. Martin (goodreads.com) Release It!: Design and Deploy Production-Ready Software by Michael T. Nygard (goodreads.com) TDD, DDD, BDD, UML, Enterprise Patterns, etc..   Learning networking and databases fundamentals: we always forget to discuss these points, but in real world, these are mandatory\u0026hellip; Learning Cloud concepts and technologies: Docker¬†+ Kubernetes is a must for everyone\u0026hellip; Learning more: Getting started with one of the most popular cloud providers: for example, Microsoft is regularly offering free vouchers to pass the Azure Fundamentals Certification AZ-900 for free üòÅ  But, after learning all of this, how to prove to recruiters that you really know what you are mentioning as skills in your CV ? the obvious answer is to pass certifications. They always have been the first proof of knowledge for a given technology or solution. But, this can be costly for students to afford. The other alternative exists and for free üòÅ making \u0026ldquo;PERSONAL PROJECTS\u0026rdquo; ü§™\nPersonal projects have been always the best way for learning technologies: when we are following tutorials and guidelines, we are developing personal projects. These projects can have different sizes depending on the context and the covered technologies. But it‚Äôs an opportunity to work on real use cases that can cover day-to-day situations; these projects can be covering many areas of knowledge that can be orchestrated. I wanted to make this post to list some examples of projects that can be done by students to cover a good set of technologies and solutions; something that can be considered as interesting assets in a student‚Äôs CV.\nI will make this README in a Github repository, if you want to contribute, you are welcome ü§ó üôè\n","permalink":"https://blog.nebrass.fr/projects-ideas-for-software-engineering-students/","summary":"During many career talks I attended, I noticed that some questions come back every time: \u0026quot; As a final year student, how can I get an internship? What can I mention in my CV to make it attractive? \u0026quot; ü§î\nThe problem is that always we start being concerned about internships too late, only some weeks before starting the search operation üí£ Although, finding a good internship needs to be prepared many months before\u0026hellip;","title":"Projects ideas for software engineering students üí°"},{"content":"Quarkus is one of the current trends of the Java ecosystem. I\u0026rsquo;m already in love with it. I\u0026rsquo;m using it along with Spring Boot, and when I choose, I choose Quarkus. üòÅ\nThis week, I was working on a Batch POC using Azure Functions and Azure Automation. So I thought it will be useful to share the exercise with you. üòÅ\nWhat I want to do? I will be taking a small use case: I need a batch that will be invoked at some specific time or manually, checking all the available VMs and starting or shutting down all those that we choose.\nWhat will I be using? We will be using:\n Azure Functions with Quarkus Azure Java SDK Azure Cosmos DB  What do we need? For this tutorial, we will need:\n Java 11 Maven 3.6.3 An Azure Subscription - you can get a free 12-month Azure Subscription along with 200$ credit here Azure Functions Core Tools v3  Building the Azure Functions with Quarkus Quarkus already has a Maven Archetype that creates a skull for an Azure Function in Java with Quarkus. Let\u0026rsquo;s use it, and we will discover in detail what it will bring. To generate a new project, just do:\n1 2 3 4 5 6 7 8 9 10 11 12  $ mvn archetype:generate -B \\  -DarchetypeGroupId=io.quarkus \\  -DarchetypeArtifactId=quarkus-azure-functions-http-archetype \\  -DarchetypeVersion=1.12.0.Final \\  -DjavaVersion=11 \\  -DgroupId=com.targa.labs.dev \\  -DartifactId=azure-quarkus-func \\  -Dversion=1.0.0-SNAPSHOT \\  -DappName=azure-resources-handler \\  -DappRegion=northeurope \\  -Dfunction=azure-resources-handler \\  -DresourceGroup=helloworld-rg   This Maven command will generate, in a batch mode (non-interactive), a Java Azure Function project with:\n Quarkus version: 1.12.0.Final Java version: 11 The Group Id: com.targa.labs.dev The Artifact Id: azure-quarkus-func The project version: 1.0.0-SNAPSHOT  There are also some specifications for the Azure Function Application:\n Resource Group: helloworld-rg Azure Function Application Region: North Europe Azure Function Application Name: azure-resources-handler  After generating the project, its structure will look like:\n You can notice here that there is an extra folder that we are not used to in regular Maven projects. Ok, what is this azure-config folder?\nIn the azure-config folder, we have three JSON files:\n function.json: binding configuration file - used to define how the function will be triggered. The default generated config is for HTTP Trigger: means that the function will be triggered using HTTP Requests host.json: function configuration file - used to active/deactivate the Extensions Bundle local.settings.json: credentials and settings file - used to   For more information about these files, you can check this previous post about Java Azure Functions: Playing with Java in Azure Functions\n Next, in the Java classes, you can notice these 4 Greeting_SOMETHING_ classes. These four classes are the four ways of exposing REST APIs:\n Using JAX-RS: Java EE compliant implementation Using Servlets: Java EE compliant implementation Using the Eclipse Vert.x: The Event-Driven application framework used for creating Reactive Applications Using the Funky Framework üòÇ no ! seriously, its name is Funqy Framework: a Java library from the Quarkus family used for creating Serverless Functions that can be deployed to many Function-as-Service Solutions like Azure Function or AWS Lambda\u0026hellip;  These four classes come with their respective Maven Dependencies, listed already in the pom.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \u0026lt;dependencies\u0026gt; \u0026lt;!-- remove if not using jaxrs --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- remove if not using servlets --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-undertow\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- remove if not using vertx web --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-vertx-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- remove if not using funqy --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-funqy-http\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt;   Cool! üòÅ I will be using JAX-RS only in this tutorial.\nGood! Now, we will test this sample project with this Maven command:\n1  $ mvn clean install -DskipTests azure-functions:run   The build will start, and the most important part is the one from the azure-functions-maven-plugin:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  [INFO] Scanning for projects... [INFO] [INFO] ---------------\u0026lt; com.targa.labs.dev:azure-quarkus-func \u0026gt;---------------- [INFO] Building azure-quarkus-func 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... [INFO] --- azure-functions-maven-plugin:1.9.2:run (default-cli) @ azure-quarkus-func --- ... [INFO] Azure Function App\u0026#39;s staging directory found at: /home/nebrass/java/functions/azure-quarkus-function/target/azure-functions/quarkus-function [INFO] Azure Functions Core Tools found. Azure Functions Core Tools Core Tools Version: 3.0.3284 Commit hash: 98bc25e668274edd175a1647fe5a9bc4ffb6887d Function Runtime Version: 3.0.15371.0 ... Functions: azure-resources-handler: [GET,POST,HEAD,PUT,OPTIONS,DELETE] http://localhost:7071/api/{*path} [2021-02-26T22:34:05.752Z] Worker process started and initialized. [2021-02-26T22:34:10.399Z] Host lock lease acquired by instance ID \u0026#39;000000000000000000000000D5E3A1F8\u0026#39;.   You can notice that the plugin listed the \u0026ldquo;azure-resources-handler\u0026rdquo; function is available under http://localhost:7071/api/{*path}, and the support HTTP verbs are GET, POST, HEAD, PUT, OPTION, DELETE. This list of verbs is provided from the function.json file, in the bindings configuration section:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;scriptFile\u0026#34; : \u0026#34;../${project.build.finalName}.jar\u0026#34;, \u0026#34;entryPoint\u0026#34; : \u0026#34;io.quarkus.azure.functions.resteasy.runtime.Function.run\u0026#34;, \u0026#34;bindings\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;httpTrigger\u0026#34;, \u0026#34;direction\u0026#34; : \u0026#34;in\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;req\u0026#34;, \u0026#34;route\u0026#34; : \u0026#34;{*path}\u0026#34;, \u0026#34;methods\u0026#34; : [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;DELETE\u0026#34; ], \u0026#34;dataType\u0026#34; : \u0026#34;binary\u0026#34;, \u0026#34;authLevel\u0026#34; : \u0026#34;ANONYMOUS\u0026#34; }, { \u0026#34;type\u0026#34; : \u0026#34;http\u0026#34;, \u0026#34;direction\u0026#34; : \u0026#34;out\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;$return\u0026#34; } ] }   If you want to restrict access to a specific HTTP verb, it\u0026rsquo;s here where you can tune your HTTP Triggered function. üòÅ\nWe will now implement the first API, which will be returning the list of¬†VMs information: VM Name and its Resource Group and Status (running or not). We will create a DTO class that will represent the VM info that we want:\n1 2 3 4 5 6 7 8  @Data @AllArgsConstructor @NoArgsConstructor public class VmDTO { private String name; private String status; private String resourceGroup; }   ‚ö†Ô∏è We need to add the Lombok Framework library to the pom.xml\nTo do that, we need to use the Azure SDK Java Libraries. We start by adding the Azure Resource Manager SDK. The Resource Manager in Azure is the main component that we ask for handling (creating, removing, etc..) any given resource.\nThe Azure SDK team provides developer docs. You can know more about the Resource Manager SDK at https://azure.github.io/azure-sdk-for-java/resources.html.\nSo, first of all, we need to add this dependency to the pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.azure.resourcemanager\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;azure-resourcemanager\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Now we will create the VmManager: a class that will be accessing the Azure Resource Manager API to do the VMs operations. But this communication with the API will need security credentials like a Token. So, we need to add another library that can help the VmManager authenticate to the Azure API. This library will be the Azure Identity. So we need to add its dependency to the pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;azure-identity\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Good üòÅ This Identity library will help the application access the API using some credentials that I need to provide to the application, like a Login and Password. But this is an application, not a human. This is why I will create an Azure Service Principal that will identify and authenticate our application to access the Azure Resources. Based on the Microsoft Docs:\n An Azure service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources. This access is restricted by the roles assigned to the service principal, giving you control over which resources can be accessed and at which level. For security reasons, it\u0026rsquo;s always recommended to use service principals with automated tools rather than allowing them to log in with a user identity.\n First of all, you need to be authenticated to Azure CLI¬†using the az login command. Then, to create the Service Principal, just do:\n1  $ az ad sp create-for-rbac --name QuarkusServicePrincipal   The command output will look like:\n1 2 3 4 5 6 7  { \u0026#34;appId\u0026#34;: \u0026#34;856f5c37-a639-4e1e-b294-e6391c396698\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;QuarkusServicePrincipal\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://QuarkusServicePrincipal\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;M6_6OF4q9t-kNrKISZG-0p0qv-FnZ8EdM~\u0026#34;, \u0026#34;tenant\u0026#34;: \u0026#34;2cf6bd3b-ef71-47c4-9d5d-06abe42bb3a6\u0026#34; }   Our application will use these credentials to authenticate against the Azure API:\n The appId is the Client ID üëâÔ∏è needs to be defined in the Environment Variables as AZURE_CLIENT_ID The password is the Client Secret üëâÔ∏è needs to be defined in the Environment Variables as AZURE_CLIENT_SECRET The tenant üëâÔ∏è needs to be defined in the Environment Variables as AZURE_TENANT_ID  These credentials need to be defined in the Environment Variables, as our Azure Identity components will seek these values from the Environment. This design is better than inserting the credentials directly in the source code or in the properties files.\nTo create an AzureResourceManager instance using these credentials, just use this snippet:\n1 2 3 4 5 6 7 8  AzureProfile profile = new AzureProfile(AzureEnvironment.AZURE); TokenCredential credential = new DefaultAzureCredentialBuilder() .authorityHost(profile.getEnvironment().getActiveDirectoryEndpoint()) .build(); AzureResourceManager azureResourceManager = AzureResourceManager .authenticate(credential, profile) .withDefaultSubscription();   Now we can implement the getAvailableVms() method:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  @ApplicationScoped public class AzureVmManager { private final AzureResourceManager azureResourceManager; public AzureVmManager() { AzureProfile profile = new AzureProfile(AzureEnvironment.AZURE); TokenCredential credential = new DefaultAzureCredentialBuilder() .authorityHost(profile.getEnvironment().getActiveDirectoryEndpoint()) .build(); azureResourceManager = AzureResourceManager .authenticate(credential, profile) .withDefaultSubscription(); } public List\u0026lt;VmDTO\u0026gt; getAvailableVMs() { List\u0026lt;String\u0026gt; rgList = azureResourceManager.resourceGroups() .list() .stream() .map(HasName::name) .collect(Collectors.toList()); List\u0026lt;VmDTO\u0026gt; vmList = new ArrayList\u0026lt;\u0026gt;(); for (String resourceGroup : rgList) { azureResourceManager.virtualMachines() .listByResourceGroup(resourceGroup) .forEach(vm -\u0026gt; vmList.add( new VmDTO( vm.name(), vm.powerState().toString(), resourceGroup ) )); } return vmList; } }   In the getAvailableVms() method, we list the resource groups, and for every resource group, we list its VMs.\nGood üòÑ Now let\u0026rsquo;s edit the REST Resource to send the List of VmDTOs:\n1 2 3 4 5 6 7 8 9 10 11 12  @Path(\u0026#34;/vms\u0026#34;) public class VMsResource { @Inject AzureVmManager azureVmManager; @GET @Produces(MediaType.APPLICATION_JSON) public List\u0026lt;VmDTO\u0026gt; getAvailableVMs() { return azureVmManager.getAvailableVMs(); } }   Let\u0026rsquo;s test the project now. Just do this Maven command:\n1  $ mvn clean install -DskipTests azure-functions:run   Then, if you open the http://localhost:7071/api/vms URL, the result will look like:\nDon\u0026rsquo;t be scared if you got an empty array üòÖ Be sure that you already have some VMs in your subscription.\nNext, we will implement the Start and Stop VM methods in the AzureVmManager class:\n1 2 3 4 5 6 7 8 9  ... public void startVM(VmDTO vmDTO) { azureResourceManager.virtualMachines().start(vmDTO.getResourceGroup(), vmDTO.getName()); } public void stopVM(VmDTO vmDTO) { azureResourceManager.virtualMachines().deallocate(vmDTO.getResourceGroup(), vmDTO.getName()); } }   Then, we need to expose them in the REST API:\n1 2 3 4 5 6 7 8 9 10 11 12 13  @POST @Path(\u0026#34;/start\u0026#34;) @Consumes(MediaType.APPLICATION_JSON) public void startVM(VmDTO vmDTO){ azureVmManager.startVM(vmDTO); } @POST @Path(\u0026#34;/stop\u0026#34;) @Consumes(MediaType.APPLICATION_JSON) public void stopVM(VmDTO vmDTO){ azureVmManager.stopVM(vmDTO); }   Excellent! ü•≥ Now we will add the scenario of storing the start/stop events to the Azure Cosmos DB.\nBut before, we need to create the Event model class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  @Data @AllArgsConstructor @NoArgsConstructor public class VmEvent { private VmEventType type; private String id; private String name; private String resourceGroup; private Date date; public VmEvent(VmEventType type, String name, String resourceGroup) { this.type = type; this.name = name; this.resourceGroup = resourceGroup; this.id = UUID.randomUUID().toString(); this.date = new Date(); } } enum VmEventType { START, STOP; }   Let\u0026rsquo;s play with CosmosDB now! ü§©\nAdding Azure CosmosDB to the Game The first step is to create:\n a new CosmosDB account if you don\u0026rsquo;t have one already a new CosmosDB database and name it events-db a new CosmosDB container and name it events with /name as Partition key  Then keep aside the Connection String as our application will use it in order to access the Cosmos DB.\nIf you didn\u0026rsquo;t that before, you can find a guided tutorial about Azure Cosmos DB here.\nThe Connection String needs to be added to the local.settings.json file, under the \u0026ldquo;CosmosDbConnection\u0026rdquo; entry.\nNow, we will need to add the CosmosDB Client to the AzureVmManager class. But, before that, we need to have a method that can be parsing the Connection String to extract the required credentials which are needed for authenticating the Cosmos DB Client.\nThe Connection String has the format: AccountEndpoint=https://xxxxxxxx.documents.azure.com:443/;AccountKey=RpiUBg4mK0xBIKqPRpiUBg4mK0xBIKqP==;\nTo parse it, I will split the AccountEndpoint and AccountKey parts using the ; sign, and then I will split the KEY=VALUE expression using the = sign. Then I will be storing these credentials in a Map.\nMy getCosmosDbCredentials() method looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13  private Map\u0026lt;String, String\u0026gt; getCosmosDbCredentials() { Map\u0026lt;String, String\u0026gt; credentials = new HashMap\u0026lt;\u0026gt;(); String cosmosDbConnection = System.getenv(\u0026#34;CosmosDbConnection\u0026#34;); String[] elements = cosmosDbConnection.split(\u0026#34;;\u0026#34;); for (String element : elements) { String[] split = element.split(\u0026#34;=\u0026#34;); credentials.put(split[0], split[1]); } return credentials; }   The System.getenv(\u0026quot;CosmosDbConnection\u0026quot;) instruction will grab the CosmosDbConnection from the System Environment Variables. This is will work, as we added the \u0026ldquo;CosmosDbConnection\u0026rdquo; entry to the local.settings.json, then, Azure Function Runtime will make this entry available as an Environment Variable.\nExcellent! Now, if to instantiate the CosmosClient we will do:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  AzureResourceManager azureResourceManager = AzureResourceManager .authenticate(credential, profile) .withDefaultSubscription(); Map\u0026lt;String, String\u0026gt; credentials = getCosmosDbCredentials(); CosmosClient cosmosClient = new CosmosClientBuilder() .endpoint(credentials.get(\u0026#34;AccountEndpoint\u0026#34;)) .key(credentials.get(\u0026#34;AccountKey\u0026#34;)) .buildClient(); cosmosContainer = cosmosClient .getDatabase(\u0026#34;events-db\u0026#34;) .getContainer(\u0026#34;events\u0026#34;);   Then, we will use the cosmosContainer to do the CRUD operations in the Azure Cosmos DB. To insert a VmEvent into CosmosDB, we create the method sendData():\n1 2 3  public void sendData(VmEvent vmEvent) { cosmosContainer.createItem(vmEvent); }   We will include this method into the start/stop VM methods:\n1 2 3 4 5 6 7 8 9 10 11 12 13  public void startVM(VmDTO vmDTO) { azureResourceManager.virtualMachines().start(vmDTO.getResourceGroup(), vmDTO.getName()); sendData( new VmEvent(VmEventType.START, vmDTO.getName(), vmDTO.getResourceGroup()) ); } public void stopVM(VmDTO vmDTO) { azureResourceManager.virtualMachines().deallocate(vmDTO.getResourceGroup(), vmDTO.getName()); sendData( new VmEvent(VmEventType.STOP, vmDTO.getName(), vmDTO.getResourceGroup()) ); }   Then, we will create a method that grabs the stored events - in the AzureVmManager:\n1 2 3 4 5 6 7 8  public List\u0026lt;VmEvent\u0026gt; getLogs() { String SELECT_ALL_LOGS_QUERY = \u0026#34;SELECT * FROM c\u0026#34;; return cosmosContainer .queryItems(SELECT_ALL_LOGS_QUERY, new CosmosQueryRequestOptions(), VmEvent.class) .stream() .collect(Collectors.toList()); }   Next, we will expose the getLogs() in the REST Resource:\n1 2 3 4 5 6  @GET @Path(\u0026#34;/logs\u0026#34;) @Produces(MediaType.APPLICATION_JSON) public List\u0026lt;VmEvent\u0026gt; logs(){ return azureVmManager.getLogs(); }   Excellent! Now, let\u0026rsquo;s run this function locally using the command:\n1  $ mvn clean install -DskipTests azure-functions:run   After the application starts, we will use the list available VMs API, then we will choose a VM, and will start or stop many times, then we will check the get Logs API in order to get the events.\n‚ÑπÔ∏è I will use the HTTPie as a REST CLI\nGet the VMs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  http http://localhost:7071/api/vms HTTP/1.1 200 OK Content-Length: 265 Content-Type: application/json Date: Tue, 02 Mar 2021 16:53:15 GMT Server: Kestrel [ { \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PowerState/deallocated\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;wordpress-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;blog-rg\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PowerState/running\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;agent-builder\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkus-book-rg\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PowerState/deallocated\u0026#34; } ]   Start the quarkus-vm machine:\n1 2 3 4 5 6 7  echo \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34; }\u0026#39; \\ | http POST http://localhost:7071/api/vms/start --timeout=300 HTTP/1.1 204 No Content Content-Length: 0 Date: Tue, 02 Mar 2021 16:47:53 GMT Server: Kestrel   Stop it:\n1 2 3 4 5 6 7  echo \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34; }\u0026#39; \\ | http POST http://localhost:7071/api/vms/stop --timeout=300 HTTP/1.1 204 No Content Content-Length: 0 Date: Tue, 02 Mar 2021 16:48:51 GMT Server: Kestrel   ‚ö†Ô∏è Be sure to have the --timeout=300 in the HTTPie command, because the REST call can take more than 30 seconds, which is the default HTTPie timeout.\nThen we will get the Logs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  http http://localhost:7071/api/vms/logs HTTP/1.1 200 OK Content-Length: 1366 Content-Type: application/json Date: Tue, 02 Mar 2021 16:59:12 GMT Server: Kestrel [ { \u0026#34;date\u0026#34;: 1614703368791, \u0026#34;id\u0026#34;: \u0026#34;f33e9ed4-086d-4ccf-88d5-f4f4f32ba74e\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;START\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703445766, \u0026#34;id\u0026#34;: \u0026#34;bb03fb09-e07a-4182-8429-baa48cf43b35\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STOP\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703604793, \u0026#34;id\u0026#34;: \u0026#34;ce8c06fe-d00b-47ba-a0e3-38670fc2878e\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;START\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703674713, \u0026#34;id\u0026#34;: \u0026#34;a14f24e8-2b23-4f61-bb6c-c9154d839410\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STOP\u0026#34; } ]   Excellent, this data can be then used in your monitoring dashboards or monitoring services. Cool! Let\u0026rsquo;s deploy now our Azure Function Application üòÅ\nDeploying the Quarkus Azure Function As usual, it\u0026rsquo;s a very easy task: only one Maven Task will do the job ü§© Just be sure that the Azure CLI is authenticated:\n1  $ mvn clean install -DskipTests azure-functions:deploy   The process will take some time, then you will get an output that looks like:\n1 2 3 4 5 6 7 8 9 10  .... [INFO] Syncing triggers and fetching function information (Attempt 1/3)... [INFO] HTTP Trigger Urls: [INFO] quarkus-function : https://quarkus-function.azurewebsites.net/api/{*path} [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 01:59 min [INFO] Finished at: 2021-03-02T18:12:39+01:00 [INFO] ------------------------------------------------------------------------   In theory, the application will work now üòÜ but it\u0026rsquo;s not the case, yet!\nWe need to provide the AZURE_CLIENT_ID, AZURE_CLIENT_SECRET, AZURE_TENANT_ID along with the CosmosDbConnection to the Function Environment. To do that, just go to the Azure portal and then go to the Function App menu.¬†Then go to the Configuration menu in order to access the Function Environment Variables, here where you need to add the required items:\nExcellent! Now, our Quarkus Function is ready for receiving our requests, let\u0026rsquo;s test it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  http https://quarkus-function.azurewebsites.net/api/vms/logs HTTP/1.1 200 OK Content-Encoding: gzip Content-Type: application/json Date: Tue, 02 Mar 2021 17:25:27 GMT Request-Context: appId=cid-v1:a3251f24-8fcf-429f-8f94-07eae258f122 Transfer-Encoding: chunked Vary: Accept-Encoding [ { \u0026#34;date\u0026#34;: 1614703368791, \u0026#34;id\u0026#34;: \u0026#34;f33e9ed4-086d-4ccf-88d5-f4f4f32ba74e\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;START\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703445766, \u0026#34;id\u0026#34;: \u0026#34;bb03fb09-e07a-4182-8429-baa48cf43b35\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STOP\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703604793, \u0026#34;id\u0026#34;: \u0026#34;ce8c06fe-d00b-47ba-a0e3-38670fc2878e\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;START\u0026#34; }, { \u0026#34;date\u0026#34;: 1614703674713, \u0026#34;id\u0026#34;: \u0026#34;a14f24e8-2b23-4f61-bb6c-c9154d839410\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;quarkushop-vm\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;quarkushop-rg\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STOP\u0026#34; } ]   Enjoy the game!\nConclusion In this tutorial, we learned how to use the Quarkus Azure Functions HTTP library and how to use Quarkus to create a powerful Azure Function üòÖ\nThe source code of this application is available here.\n","permalink":"https://blog.nebrass.fr/playing-with-azure-functions-and-quarkus/","summary":"Quarkus is one of the current trends of the Java ecosystem. I\u0026rsquo;m already in love with it. I\u0026rsquo;m using it along with Spring Boot, and when I choose, I choose Quarkus. üòÅ\nThis week, I was working on a Batch POC using Azure Functions and Azure Automation. So I thought it will be useful to share the exercise with you. üòÅ\nWhat I want to do? I will be taking a small use case: I need a batch that will be invoked at some specific time or manually, checking all the available VMs and starting or shutting down all those that we choose.","title":"Playing with Azure Functions and Quarkus"},{"content":"","permalink":"https://blog.nebrass.fr/announcing-playing-with-quarkus-workshop-edition/","summary":"","title":"Announcing Playing with Quarkus -  Workshop Edition ü•≥"},{"content":" Today is the big day ü•≥ü•≥ My third book \u0026ldquo;Playing with Java Microservices with Quarkus and Kubernetes\u0026rdquo; is available in Early Access üéâüéäüéà‚ú® Book cover\n I started working on the book starting from June üòÅ¬†I spent nearly 5 months presenting the most interesting and powerful features of the Quarkus Framework. When I started the Proof-of-concept, I didn\u0026rsquo;t have the idea of writing a book on Quarkus, I was just discovering. I found myself spending more and more time reading the documentation, trying some features and extensions, and then I got the idea to write the Quarkus version of my \u0026ldquo;Playing with Java Microservices\u0026rdquo; Book üòÅ\nThe content of book:\n Chapter 0:¬†Getting started with Containerization, presents the containerization with¬†Docker¬†\u0026amp;¬†Podman. Chapter 1:¬†Introduction to the Monolithic architecture, provides an introduction to the¬†Monolithic architecture¬†with a focus on its¬†advantages¬†and¬†drawbacks. Chapter 2:¬†Coding the Monolithic application, offers a step-by-step tutorial for modeling and creating the monolith using¬†Maven,¬†Java 11¬†\u0026amp;¬†Quarkus. Chapter 3:¬†Upgrading the Monolithic application, lists some upgrades for our¬†Monolithic application¬†such as adding tests and most-wanted features. Chapter 4:¬†Building \u0026amp; Deploying the Monolithic application, covers how to build and package our example application before showing how to deploy it in the runtime environment. Chapter 5:¬†Adding the anti-disasters layers, introduces how to implement the¬†Security¬†and¬†Monitoring¬†layers to help the application to avoid disasters. Chapter 6:¬†Microservices Architecture Pattern, presents the drawbacks that can be faced in a typical¬†Monolithic architecture¬†and shows the motivations for seeking something new, like the¬†Microservices Architecture Pattern. Chapter 7:¬†Splitting the Monolith: Bombarding the domain, presents the¬†Domain Driven Design concepts¬†to successfully understand the¬†Business Domain¬†of the¬†Monolithic Application, to be able to split it into sub-domains. Chapter 8:¬†Applying DDD to the code, shows how to apply all the¬†DDD concepts¬†to the Monolithic Application source code and to successfully¬†split the monolith into Bounded Contexts. Chapter 9:¬†Meeting the microservices concerns and patterns, covers a deep presentation of the concerns and the cloud patterns related to the¬†Microservices Architecture Pattern. Chapter 10:¬†Getting started with Kubernetes, covers how to build our real standalone business microservices. Chapter 11:¬†Implementing the Cloud Patterns, presents in deep the greatest container orchestrator:¬†Kubernetes Chapter 12:¬†Building the Kubernetized Microservices, shows how to migrate the monolith code to build microservices while applying¬†Kubernetes¬†concepts and features. Chapter 13:¬†Flying all over the Sky with Quarkus and Kubernetes¬†will introduce you to how implement additional cloud patterns using¬†Quarkus¬†and¬†Kubernetes.  The remaining chapters:\n Chapter 14:¬†Playing with Quarkus in Azure shows how¬†Microsoft Azure¬†can help you to modernize applications faster and how you can use¬†Serverless Architecture¬†to add additional great features. Chapter 15: Bringing Dapr into the game presents Dapr \u0026amp; covers how a Distributed Application Runtime can improve a microservices architecture. and more üòÅ  I made the book available under Early Access because I didn\u0026rsquo;t finish the Azure part: where I will be covering many great Azure services useful to boost the efficiency of my Quarkus microservices. I will also cover more topics and tools that I see that can be a great add-on to the actual content. The eBook version of the book will be continuously updated. Stay tuned üòé\nThe book is available in Gumroad and Leanpub, and soon will be available in Amazon and Google Play ü•≥\n","permalink":"https://blog.nebrass.fr/playing-with-java-microservices-with-quarkus-and-kubernetes-is-available-in-early-access/","summary":"Today is the big day ü•≥ü•≥ My third book \u0026ldquo;Playing with Java Microservices with Quarkus and Kubernetes\u0026rdquo; is available in Early Access üéâüéäüéà‚ú® Book cover\n I started working on the book starting from June üòÅ¬†I spent nearly 5 months presenting the most interesting and powerful features of the Quarkus Framework. When I started the Proof-of-concept, I didn\u0026rsquo;t have the idea of writing a book on Quarkus, I was just discovering.","title":"\"Playing with Java Microservices with Quarkus and Kubernetes\" is available in Early Access"},{"content":"Today, I will not be sharing a tutorial or a review, I will share with you my new book news ! ü•≥ I just finished typing the fourth chapter.¬†These four chapters belong to the first part of the story: the MoNoLITHiC dedicated part ü§™ ..\nWhat\u0026rsquo;s new¬†? In my previous book \u0026ldquo;Playing with Java Microservices on Kubernetes and OpenShift\u0026rdquo; there was a Docker-only chapter in the middle of the book. Now, in the book, there is no Docker-only chapter, there is a Chapter Zero dedicated for Containerization, with Docker and alternatives ! üòÑ\n\u0026ldquo;Playing with Java Microservices with Quarkus and Kubernetes\u0026rdquo; comes with many many new cool content:\n Containerization as mandatory skill for every Java Developer Demystifying Java 11 new features (Unit \u0026amp; Integration) Testing best practices and code quality measurements Designing Continuous Integration and Continuous Deployment pipelines Benchmarking and load testing labs Demystifying many complex JVM components such as compilers üò± I know that they are the most loved part of Java üòÜ  and more to be added in the next chapters of the Monoliths part:\n Demystifying the authentication and authorization using KeyCloak Reaching the limits of the monolithic architecture  I will not forget to mention that during the first chapter, I found and I fixed the bug in the Spring Data JPA extension for Quarkus. Which is now delivered in the Release v1.7.0.Final ü•≥\n","permalink":"https://blog.nebrass.fr/just-finished-the-first-four-chapters../","summary":"Today, I will not be sharing a tutorial or a review, I will share with you my new book news ! ü•≥ I just finished typing the fourth chapter.¬†These four chapters belong to the first part of the story: the MoNoLITHiC dedicated part ü§™ ..\nWhat\u0026rsquo;s new¬†? In my previous book \u0026ldquo;Playing with Java Microservices on Kubernetes and OpenShift\u0026rdquo; there was a Docker-only chapter in the middle of the book.","title":"Just finished the first four chapters.."},{"content":"Today, I would like to share with you the title of my new book: Playing with Java Microservices with Quarkus and Kubernetes. I started working on it, since the July 2020.\nPlaying with Java Microservices with Quarkus Kubernetes will teach you how to build and design microservices using Java and the Red Hat Quarkus Framework.\nThis book covers topics related to creating Java microservices using Quarkus and deploying them to Kubernetes.\nTraditionally, Java developers have been used to developing large, complex monolithic applications. The experience of developing and deploying monoliths has been always slow and painful. This book will help Java developers to quickly get started with the features and the concerns of the microservices architecture. It will introduce Docker and Kubernetes to help them deploying their microservices.\nThe book is written for Java developers who wants to build microservices using the Red Hat Quarkus and who wants to deploy them in Kubernetes.\nYou will be guided on how to install the appropriate tools to work properly. For those who are new to Enterprise Development using Quarkus, you will be introduced to its core principles and main features thru a deep step-by-step tutorial on many components. For experts, this book offers some recipes that illustrate how to split monoliths and implement microservices and deploy them as containers to Kubernetes.\nThe following are some of the key challenges that we will address in this book:\n Introducing Quarkus and GraalVM for beginners Splitting a monolith using the Domain Driven Design approach Implementing the cloud \u0026amp; microservices patterns Rethinking the deployment process Introducing containerization, Docker and Kubernetes  By the end of reading this book, you will have practical hands-on experience of building microservices using Quarkus and you will master deploying them as containers to Kubernetes.\nThis book will be published on Leanpub. I hope that it will be fully published by December 2020. I think of making an Early Access by the end of September 2020 üòÅ\nI hope that it will succeed as did the previous titles: Pairing Apache Shiro with Java EE 7 and Playing with Java Microservices on Kubernetes and OpenShift ü§©\n","permalink":"https://blog.nebrass.fr/my-upcoming-book-playing-with-java-microservices-with-quarkus-and-kubernetes/","summary":"Today, I would like to share with you the title of my new book: Playing with Java Microservices with Quarkus and Kubernetes. I started working on it, since the July 2020.\nPlaying with Java Microservices with Quarkus Kubernetes will teach you how to build and design microservices using Java and the Red Hat Quarkus Framework.\nThis book covers topics related to creating Java microservices using Quarkus and deploying them to Kubernetes.","title":"My upcoming book: Playing with Java Microservices with Quarkus and Kubernetes"},{"content":"Reactive programming ! Wow ! What a fancy buzzy word ! I waited so much to write a blog post about this trend üòÉ¬†I was waiting for the landscape to be mature to made a one-shot tutorial üòÅ\nToday, I will show you how to make a Reactive Spring Boot application, with a Reactive CRUDs using Spring Data Reactive Relational Database Connectivity (R2DBC) with¬†PostgreSQL and for sure the famous Webflux. Don\u0026rsquo;t be scared if you don\u0026rsquo;t know any of these topics. We will be introducing all of them smoothly !¬†üòä\nBut before digging into the practice, I will be making small story-telling about the fundamentals of the reactive programming. üòé\nLet\u0026rsquo;s discover the reactive programming Reactive programming is development paradigm based around asynchronous data streams. I think this is the shortest complete definition.\nIn the reactive programming, data streams are the first class citizens. They can be events, messages, calls, and even failures that we can observe and we can react when a value is emitted. You subscribe to the event, and you will be notified when your event is fired. Moving to an events-driven style will make our application asynchronous and non blocking. Many frameworks and libraries are regularly appearing.. one of the most powerful projects is Reactor:¬†a fourth-generation reactive library, based on the Reactive Streams specification, for building non-blocking applications on the JVM. Reactor is created and maintained by the Spring Framework teams üí™üòÅ\nReactor logo\n Mastering the fundamentals of the reactive programming I found a FREE wonderful interactive tutorial about Reactive programming with Reactor 3. I highly recommend that you take it before continuing this tutorial üòÅ the link.\nFree tutorial - Reactive Programming with Reactor 3\n Building our First Reactive Application The full source code is available on my Github.\nWe will generate our project using the Spring Initializr. Our application will have 6 dependencies:\n Spring Boot DevTools Lombok Spring Reactive Web Spring Data R2DBC PostgreSQL Driver Flyway Migration  Generating the project skull on Spring Initializr\n Before digging into the code, we need to have a PostgreSQL database üòÖ I will be using a Dockerized DB. Just run this command to have a local PostgreSQL DB running into a Docker container :\n1 2 3 4  $ docker run -d --name demo-postgres \\  -e POSTGRES_USER=developer -e POSTGRES_PASSWORD=p4SSW0rd \\  -e POSTGRES_DB=demo \\  -p 5432:5432 postgres:latest   Good ! Now we can start working on my code. We will be developing a Reactive Books CRUD application.\nFirst of all we need to create the Model class which will be used to map the table structure, which is the equivalent to an Entity in JPA.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  @Data @Table(\u0026#34;book\u0026#34;) public class Book { @Id private Long id; private String title; private String isbn; private String author; private BigDecimal price; public Book(String title, String isbn, String author, BigDecimal price) { this.title = title; this.isbn = isbn; this.author = author; this.price = price; } }   Next, we will create a Reactive Repository to manage our Books in our DB:\n1 2 3  @Repository public interface BookRepository extends ReactiveCrudRepository\u0026lt;Book, Long\u0026gt; { }   As you see here, we are not extending from a JpaRepository like in the classic Spring Data. But we are extending from the ReactiveCrudRepository interface from the Spring Data Reactive Relational Database Connectivity (R2DBC) library.\nWe can even create a custom method in our Repository using the Spring Data Pattern. For example I will need a findBy method using an Author name and I want that the request ignore the case and it will return a Reactive Stream of 0-N Books:\n1  Flux\u0026lt;Book\u0026gt; findBooksByAuthorContainingIgnoreCase(String author);   Next, we will create a Spring Service class that will be a glue layer between the Repository and the RestController, and where business logic will be living.\nOur typical Service will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  @Service @RequiredArgsConstructor public class BookService { private final BookRepository bookRepository; public Flux\u0026lt;Book\u0026gt; findAll() { return bookRepository.findAll(); } public Mono\u0026lt;Book\u0026gt; findById(Long id) { return bookRepository.findById(id); } public Mono\u0026lt;Book\u0026gt; save(BookDTO book) { return bookRepository.save( new Book( book.getTitle(), book.getIsbn(), book.getAuthor(), book.getPrice() ) ).subscribe(); } public Flux\u0026lt;Book\u0026gt; findByAuthor(String author) { return bookRepository.findBooksByAuthorContainingIgnoreCase(author); } public void deleteById(Long id) { bookRepository.deleteById(id).subscribe(); } }   As you see, we have findAll() method from the Repository that is used to grab \u0026ldquo;reactively\u0026rdquo; all the records from the DB, this why it\u0026rsquo;s return type is a Flux\u0026lt;Book\u0026gt; and not our classic List\u0026lt;Book\u0026gt;. But what is a Flux ? A Flux is an Asynchronous Sequence of 0-N Items. Good ! The same way, I think you already saw the Mono, which is an Asynchronous 0-1 Result, which\u0026rsquo;s somehow like the Optional Java 8 object.\nAs you see in the save() and deleteById() methods, we are calling subscribe(). This is always done with a reactive streams, because for many actions, we need to have a subscriber for an operation to be done.\nNow, we need to create our RestController:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  @RestController @RequestMapping(\u0026#34;/books\u0026#34;) @RequiredArgsConstructor public class BookRestController { private final BookService bookService; @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE) public Flux\u0026lt;Book\u0026gt; list() { return bookService.findAll(); } @GetMapping(\u0026#34;/{id}\u0026#34;) public Mono\u0026lt;Book\u0026gt; findById(@PathVariable Long id) { return bookService.findById(id); } @PostMapping @ResponseStatus(HttpStatus.CREATED) public Mono\u0026lt;Book\u0026gt; create(@RequestBody BookDTO bookDTO) { return bookService.save(bookDTO); } @GetMapping(\u0026#34;/author\u0026#34;) public Flux\u0026lt;Book\u0026gt; findByAuthor(@RequestParam String name) { return bookService.findByAuthor(name); } @DeleteMapping(\u0026#34;/{id}\u0026#34;) public void deleteById(@PathVariable Long id) { bookService.deleteById(id); } }   We have introduced a BookDTO class that will be a wrapping the records sent/received while interacting with the REST consumers:\n1 2 3 4 5 6 7  @Data public class BookDTO { private String title; private String isbn; private String author; private BigDecimal price; }   Now, before running our code, we need to configure PostgreSQL for R2DBC:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  @Configuration @EnableR2dbcRepositories public class PostgresConfiguration extends AbstractR2dbcConfiguration { @Value(\u0026#34;${spring.r2dbc.url}\u0026#34;) private String url; @Value(\u0026#34;${spring.r2dbc.username}\u0026#34;) private String username; @Value(\u0026#34;${spring.r2dbc.password}\u0026#34;) private String password; @Bean @Override public ConnectionFactory connectionFactory() { return new PostgresqlConnectionFactory( PostgresqlConnectionConfiguration.builder() .host(JdbcParser.getJdbcHost(url)) .port(JdbcParser.getJdbcPort(url)) .username(username) .password(password) .database(JdbcParser.getDbName(url)) .build()); } @Bean ReactiveTransactionManager transactionManager(ConnectionFactory connectionFactory) { return new R2dbcTransactionManager(connectionFactory); } }   Our application.properties file needs to have our DB credentials:\n1 2 3  spring.r2dbc.url=jdbc:postgresql://localhost:5432/demo spring.r2dbc.username=developer spring.r2dbc.password=p4SSW0rd   The JdbcParser utility class looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  public class JdbcParser { public static String getJdbcHost(String jdbc) { return getUri(jdbc).getHost(); } public static int getJdbcPort(String jdbc) { return getUri(jdbc).getPort(); } public static String getDbName(String jdbc) { return getUri(jdbc).getPath().substring(1); } private static URI getUri(String jdbc) { String substring = jdbc.substring(5); return URI.create(substring); } }   I already added Flyway to my Spring Boot dependencies. I will now configure the Flyway execution on startup using a CommandRunner:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  @Component public class FlywayConfiguration implements CommandLineRunner { @Value(\u0026#34;${spring.r2dbc.url}\u0026#34;) private String url; @Value(\u0026#34;${spring.r2dbc.username}\u0026#34;) private String username; @Value(\u0026#34;${spring.r2dbc.password}\u0026#34;) private String password; @Override public void run(String... args) { Flyway.configure() .dataSource(url, username, password) .load() .migrate(); } }   I just did this to avoid retyping PostgreSQL credentials under the spring.r2dbc.* and flyway.*\nGood ! Now we need to have the V1_0__init.sql script in the src/main/resources/db/migration folder, where Flyway is seeking SQL scripts. This initialization script will create the tables and insert some sample data:\n1 2 3 4 5 6 7 8 9 10 11 12  CREATETABLEbook(idSERIALNOTNULLPRIMARYKEY,titleVARCHAR(100),isbnVARCHAR(20),authorVARCHAR(100),priceNUMERIC(6,2));INSERTINTObook(title,isbn,author,price)values(\u0026#39;Pairing Apache Shiro and Java EE 7\u0026#39;,\u0026#39;9781365124044\u0026#39;,\u0026#39;Nebrass Lamouchi\u0026#39;,0);INSERTINTObook(title,isbn,author,price)values(\u0026#39;Playing with Java Microservices on Kubernetes and OpenShift\u0026#39;,\u0026#39;9782956428510\u0026#39;,\u0026#39;Nebrass Lamouchi\u0026#39;,9.18);  Now, let\u0026rsquo;s run the application and request the findAll() REST API:\n1 2 3 4 5 6 7 8 9  http http://localhost:8080/books HTTP/1.1 200 OK Content-Type: text/event-stream;charset=UTF-8 transfer-encoding: chunked data:{\u0026#34;id\u0026#34;:1,\u0026#34;title\u0026#34;:\u0026#34;Pairing Apache Shiro and Java EE 7\u0026#34;,\u0026#34;isbn\u0026#34;:\u0026#34;9781365124044\u0026#34;,\u0026#34;author\u0026#34;:\u0026#34;Nebrass Lamouchi\u0026#34;,\u0026#34;price\u0026#34;:0.00} data:{\u0026#34;id\u0026#34;:2,\u0026#34;title\u0026#34;:\u0026#34;Playing with Java Microservices on Kubernetes and OpenShift\u0026#34;,\u0026#34;isbn\u0026#34;:\u0026#34;9782956428510\u0026#34;,\u0026#34;author\u0026#34;:\u0026#34;Nebrass Lamouchi\u0026#34;,\u0026#34;price\u0026#34;:9.18}   Now, we will add the Swagger support to our application. First of all we need to add the SpringFox Swagger 3.0-Snapshot dependencies, as the final version is not yet released:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-spring-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   These dependencies are not available on the Maven Central repository, but in JFrog OSS Snapshot repository. We need just to add it to you pom.xml:\n1 2 3 4 5 6 7  \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jcenter-snapshots\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;jcenter\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://oss.jfrog.org/artifactory/oss-snapshot-local/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt;   Now, we need to configure Swagger in our application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  @Configuration @EnableSwagger2WebFlux public class SwaggerConfiguration { @Bean RouterFunction\u0026lt;ServerResponse\u0026gt; swaggerRouterFunction() { Mono\u0026lt;ServerResponse\u0026gt; build = ServerResponse.temporaryRedirect(URI.create(\u0026#34;swagger-ui.html\u0026#34;)).build(); return RouterFunctions.route(RequestPredicates.GET(\u0026#34;/swagger\u0026#34;), request -\u0026gt; build); } @Bean public Docket api() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(\u0026#34;Playing with Spring Webflux\u0026#34;) .description(\u0026#34;Sample application for my blog post \u0026#39;Playing with Reactive Spring Boot\u0026#39;\u0026#34;) .contact(new Contact(\u0026#34;Nebrass Lamouchi\u0026#34;, \u0026#34;https://blog.nebrass.fr\u0026#34;, \u0026#34;lnibrass@gmail.com\u0026#34;)) .version(\u0026#34;1.0\u0026#34;) .build(); } }   There is some new stuff here. Keep calm üëª we will cover everything:\n The @EnableSwagger2Webflux annotation Indicates that Swagger support for Webflux should be enabled. The swaggerRouterFunction() method is used to define a RouterFunction for the Swagger UI ressources. A RouterFunction represents a function that routes an HTTP request to a Handling Function that will be dealing with that request.  Good ! Now our application has Swagger enabled. Just run the application and visit the http://localhost:8080/swagger-ui.html:\nSwagger UI\n Good üòä We made our first Spring WebFlux application ! You can now dig more into the Reactive Programming style and you can for example try to create some Websockets to stream some of your data reactively from a REST API and a Websocket ü§©\nIf you have questions, please feel free to get in touch with me üòé\n","permalink":"https://blog.nebrass.fr/playing-with-reactive-spring-boot/","summary":"Reactive programming ! Wow ! What a fancy buzzy word ! I waited so much to write a blog post about this trend üòÉ¬†I was waiting for the landscape to be mature to made a one-shot tutorial üòÅ\nToday, I will show you how to make a Reactive Spring Boot application, with a Reactive CRUDs using Spring Data Reactive Relational Database Connectivity (R2DBC) with¬†PostgreSQL and for sure the famous Webflux.","title":"Playing with Reactive Spring Boot"},{"content":"Good ! This post has a conclusion into its title ! Udacity was the biggest discovery I\u0026rsquo;ve made during the COVID-19 lockdown ü§© I was seeking for an interactive learning platform like DataCamp, where I finished all the trainings that I\u0026rsquo;m interested on. By the end of the February, I found Udacity while I was searching for some AI/ML intensive course.\nUdacity Logo\n ‚ö†Ô∏è Disclaimer: This is not a marketing/referral post. This is a review for an e-learning website for my readers. I wish always to share the best learning materials/sources üòá feel free to click on any link üòÇ\nFor sure, you are asking yourself, why you just didn\u0026rsquo;t take some course in Edx or Coursera ? Good, many of my friends asked me that question. I have been learning IT since my age of 15yo. So I have experienced many learning materials and I know that one of my weaknesses in my learning process: if the learning experience is no AMAZING, I will not enjoy the trip, and maybe I will not be able to get the knowledge that I\u0026rsquo;m seeking. This point is well known by teachers and trainers, to be sure that the students get the most of the courses, they need to be pleased while learning. Which is not the case for Edx \u0026amp; Coursera. The UX is not so good. There is the gray old template that keeps appear for each course, problem in the responsive design, and very cold process. Nothing interactive, just some QUIZs if we want to call that interactive. I took many courses with Edx \u0026amp; Coursera, the content was extremely nice, no doubt, but the learning experience is something that I\u0026rsquo;m always seeking.\nUdacity has a wonderful UI \u0026amp; UX.. very relaxing color (many blue degrees üòç my favourite):\nUdacity UI\n diversified learning materials in the same chapter : shorts videos (to guarantee that you will not fall asleep while watching üòÇ), text content, quizzes and exercices, but the biggest UP is the PROJECTS ü§© yes ! Udacity validates the courses parts only in practical complex projects that you have to do by yourself. Guess what, there are professionals that will do the code reviews and they will reject your work if it\u0026rsquo;s not compliant with the coding standards or the market standards üòÅ¬†take this example, I submitted a project with some bugs in the C++ Course and my reviewer refused my work üòÖ:\nProject needs attention üòÖ\n and you can get all the information that you need about the weaknesses in your code:\nCode review - Explanation of the weaknesses\n A very professional code review, with examples, screenshots and guideline to forward you in the correct path.\nThe content of the courses is really impressive. Very nice learning paths and topics are covered in a very coherent way, which helps a scared newbie to dig directly into the course and start gathering the needed knowledge. For example; I never did Deep Learning before, when I took the course, the content and the examples were very nice and I could get the knowledge I was seeking for. This is due to the skills of the trainers; many of them are Research Engineers working in Google, Apple, Amazon or Professors working in world-wide known universities (Standford, Toronto, etc..) .\nAdd to that, there are great forums and chat rooms for students, so you if you have some issue and you want help, you can get it from your mentor or also from your peers üòá\nUdacity - Help Section\n To optimize the journey, Udacity offers ready workspaces, for example for AI/ML/DL courses, you have free Jupyter Notebooks environment with GPU enabled to make the learning experience COMPLETE.\nThere is also a very handy service included in each course: the Career Service üèÜ We know all that there is no good professional without soft skills, and we also know that unfortunately IT students \u0026amp; professionals are one of the most missing these skills.\nMaybe because of the long hours spent alone üòÇ This service comes out to help you :\n Improve your Linkedin profile and your Github account ü§© Research Your Career Options: It is critical you research your field to discover what skills are in-demand, and what\u0026rsquo;s expected of job candidates. You need to explore open roles, determine which align with your career goals, and identify the jobs you‚Äôre qualified for. Network: You never know where your next opportunity will come from, so don‚Äôt miss any chance to make genuine connections with those in your desired field. Don‚Äôt be afraid to reach out, and remember, networking is a \u0026ldquo;professional\u0026rdquo; activity, even when it feels casual. Optimize Your Application Materials: Great resumes and cover letters don‚Äôt guarantee you get the job, but poor ones guarantee you don‚Äôt. Make sure to check every detail to ensure you‚Äôre telling the story you want to tell. At this stage of the hiring process, recruiters need to narrow the applicant pool. Don\u0026rsquo;t give them a reason to take you out of consideration. Apply To Jobs: The secret to increasing your odds of getting hired is applying to as many roles as possible, that you‚Äôre able to customize application materials for. Submitting 3 applications that are fully optimized, and tailored to specific roles, is better than submitting 200 generic applications. Rehearse For Your Interviews: Confidence is critical to succeeding in an interview, and the more you practice, the more confidence you‚Äôll have. This applies to everything from coding challenges to questions about culture fit. The ability to clearly and confidently enunciate your value to a prospective employer can be the tipping point that ultimately gets you the job.  There is more üòÜ you can have 1:1 coaching sessions with an experienced Career Coach.\nUdacity Career Portal\n All of that is totally included in the price you pay for the Nanodegree. Yes ! we call the certificate of validation a Nanodegree. I personally consider it\u0026rsquo;s a real degree. You have an approved learning curriculum + exercices + end of module projects + mentors and reviewers + dedicated learning infrastructure + covering soft skills + full technical \u0026amp; academic support. There are all the requirements to be a standalone degree üòÅ personally, I added the FOUR nanodegrees that I took in my CV as trainings, in the same section and importance as my Bachelor and my MSc.\nUntil now, I took 4 nanodegrees:\n  Intro to Machine Learning with TensorFlow covering TensorFlow, Deep Learning, scikit-learn, Supervised Learning, Unsupervised Learning\n  Intro to Machine Learning with PyTorch covering Introduction to Machine Learning, Supervised Learning, Deep Learning, Unsupervised Learning\n  Deep Learning covering Deep Learning, Neural Networks, Jupyter Notebooks, CNNs, GANs\n  Computer Vision covering Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Simultaneous Localization and Mapping (SLAM), Object Tracking, Image Classification, Deep Learning\n  Ladies and gentlemen, I am glad to present you my nanodegrees wall üòÜ You can click on the picture to check the credential page ü§ì\n Intro to Machine Learning with TensorFlow\n Intro to Machine Learning with Pytorch\n Deep Learning\n Computer Vision\n I highly recommend Udacity for all these reasons and more. There are always new topics covered and courses are regularly updated.. for example the Android Developer Nanodegree is today on its 10th version ! So you are sure that the content is always fresh üòÅ\nThe best for the end: Udacity is offering FREE ACCESS for one month ! üòÅ Have fun !\n","permalink":"https://blog.nebrass.fr/falling-in-love-with-udacity-%EF%B8%8F/","summary":"Good ! This post has a conclusion into its title ! Udacity was the biggest discovery I\u0026rsquo;ve made during the COVID-19 lockdown ü§© I was seeking for an interactive learning platform like DataCamp, where I finished all the trainings that I\u0026rsquo;m interested on. By the end of the February, I found Udacity while I was searching for some AI/ML intensive course.\nUdacity Logo\n ‚ö†Ô∏è Disclaimer: This is not a marketing/referral post.","title":"Falling in love with Udacity ‚ö°Ô∏è"},{"content":"Today is my birthday and what a wonderful day to celebrate my life, but not only. Today I celebrate my first year in Microsoft, one of the most wonderful years in my career.. a year plenty of great new people, amazing challenges, wellness and happiness. üéâüéä\nWhen I started working for Microsoft last year, I was the Java developer joining the .Net Gurus town.¬†It was a very exceptional situation for my Java friends, because since I started working in 2012, I was always dealing with the open source world, and Microsoft at that time wasn\u0026rsquo;t in the same planet as us. I even added a disclaimer to tell my contacts in Linkedin that I will not be leaving Java while joining Microsoft üòÇ I will be back to this point later.\nMy Linkedin post announcing the greatest event in my career\n I thought that being a Java Engineer in Microsoft will be a strange situation. But I found very fast that this was only in my mind: Laurent Ellerbach who is my direct manager is one biggest contributors to the open source .NET Core IoT Libraries and one of my hierarchical managers was a Java Architect in Sun Microsystems ü§ì I even many teams dedicated to developing Java solutions on Azure, helping Java customers to land on Azure and even contributing to the JDK ü§©ü•≥\nHaving a direct manager who is coding \u0026amp; open source contributor is a wonderful chance and a very big plus in my job. You can share whatever technical issue with him, and he will not be looking only to Excel üòù he is fully operational as any Software Engineer. I even got the chance to work on the same project as my manager üôå\nWhat is the CSE Team ? First of all let me present you my home, the Commercial Software Engineering team:\nThe Commercial Software Engineering team (CSE) is a global engineering organization that works directly with the largest companies and not-for-profits in the world to tackle their most significant technical challenges. We‚Äôve helped customers do everything from using AI and deep learning to create sustainable farming practices, protecting rhinos in South Africa with facial recognition, to using blockchain to accelerate and secure payments in the travel industry; all in partnership with their developer teams. ü§©\nEvery day, our global team helps other developers create and utilize technology to achieve more. We will work on just about anything: blockchain, mobile apps, cloud services, big data ingestion problems, artificial intelligence, and machine learning ‚Äì you name it. Our wide range of backgrounds and experiences, which like our customers, makes us unique in how we approach customer problems. The Microsoft CSE team works closely with clients to find creative solutions while also upskilling their developers to carry the project forward to a production-ready solution. üí™üèÜüéñÔ∏è\nOur team, CSE, is working on many Azure related technologies: Dotnet, Python, Golang and for sure Java and many many other exciting topics and technologies. Our team works on a pretty broad range of projects, from intelligent robots to the trendy chatbots. We are working side-by-side with our customers to deal with the constant change. This is why we need to know many technologies and solutions üëà and this is the most wonderful advantage of our job. We permanently keep doing new and interesting stuff ü§© we never fall in monotony. Even more, we have dedicated time and ressources for learning new stuff even thru the internal MSFT materials or thru many external libraries and e-learning subscriptions that MSFT offers for us üòç There is more¬†and better, we have great workshop events called \u0026ldquo;Openhack\u0026rdquo; organized by CSE and the Product Teams. An Openhack is a 3-days workshop dedicated for developers interested in improving their technical skills within a specific leading-edge field (Serverless, AI/ML, DevOps, Containers, Big Data, etc..). This event is open to everyone and not only for MSFTees ü§ó You can learn more about Openhacks here.\nHow CSE changed my life ? As¬†I mentioned before, in CSE we do lot of things, we play with many technologies, we work massively on sharing knowledge, practices and feedbacks between teams. This multitude started calling me to try to do new things. I started my journey in MSFT by taking the Azure Fundamentals AZ-900 training and certification. As I never got the chance to dig into it so deeper, I found the great opportunity to play with Java on it, and the experience was amazing.\nI got the chance to work on many amazing projects, and attended many presentations about projects written in other than Java, and the great level of engineering and the crazy topics kept appealing me.. but I had the feeling of resisting to \u0026ldquo;leaving Java\u0026rdquo;.. it was that strange feeling, do I came here to lose Java? Will I be unfaithful while leaving my lovely language? I was always scared even of making a \u0026ldquo;Hello World\u0026rdquo; in C# üòÇ\nBy the end of the year, I got the chance to join an interesting project covering Golang \u0026amp; Terraform. I never did them before, so I started the upskilling on these technologies. I hadn\u0026rsquo;t that fear to do Golang, as it\u0026rsquo;s not .Net, so it\u0026rsquo;s Ok..\nIn one of the discussions with my manager, he told me about the new features of the .Net Framework, and how he is using it in SBCs.. and he asked me a question: why don\u0026rsquo;t you give it a try? ü§î\nHere I started thinking seriously to break the rule and to try the .Net ü§≠ Yes I did it ! üòÅ I installed the .Net Core 3.1 on my Mac and tried some \u0026ldquo;Hello World\u0026rdquo;. It was a very similar to every programming language.. nothing bad and nothing strange.. and more.. I wasn\u0026rsquo;t hurt and everything was ok.. üòÅ Even more, when I discussed that with my manager, he gave me the chance to join an active project where I got the chance to practice .Net programming in a real world project. My team mates helped me a lot to do the job. I was extremely happy because I could help my team to satisfy our customer request. I was extremely happy to attend the Demo sessions and to see my .Net code running and being built into the Azure DevOps CI/CD pipelines.\nMy manager helped me to defeat my .Net fear and to enjoy the real pleasure of having wide range of technologies and to be able to easily make the customer happy. üòÑ even more, my colleagues were happy to help me on that and they are keeping encourage me to discover and achieve more.\nBefore joining MSFT, I consider myself as a Java Developer,¬†hardly tagged with my favorite technology. Although I was also working on Angular, React, NodeJS, Docker, Kubernetes and many other technologies. After my first year spent here in Microsoft, I\u0026rsquo;m the new Nebrass Lamouchi, Software Engineer who ‚ù§Ô∏è Java but who can be working on any cool and exciting technology. üòç\nThis is can be small while talking about it, but this is a very big achievement I unlocked in my first year in MSFT. I\u0026rsquo;m happy that other than the technical skills, my manager helped me to improve my engineering maturity.\nIt\u0026rsquo;s not a matter of a language, all is related of solving a problem in a specific context. I was and I will always be the Java crazy lover, and I will also work hard to achieve a success and resolve the problem. I will always work hard to be a \u0026ldquo;Software Engineer\u0026rdquo;. After one year spent here, I\u0026rsquo;m now coding in Python, .Net Core, Golang and even C++ üòç I did some Machine Learning and Deep Learning, Terraform, and certainly so much Azure..\nFinally, I wish to share with you my birthday cake; a homemade Java based cake, made by my wife:\nBirthday compliant Java Cake üòç\n This is was my first step, in the long MSFT¬†career path. I wish to have the chance to write a blog post about my 5th, 10th or even my 25th year in MSFT, a company where we take care of people.\n","permalink":"https://blog.nebrass.fr/celebrating-my-30th-birthday-my-1st-year-in-microsoft/","summary":"Today is my birthday and what a wonderful day to celebrate my life, but not only. Today I celebrate my first year in Microsoft, one of the most wonderful years in my career.. a year plenty of great new people, amazing challenges, wellness and happiness. üéâüéä\nWhen I started working for Microsoft last year, I was the Java developer joining the .Net Gurus town.¬†It was a very exceptional situation for my Java friends, because since I started working in 2012, I was always dealing with the open source world, and Microsoft at that time wasn\u0026rsquo;t in the same planet as us.","title":"Celebrating my 30th birthday \u0026 my 1st year in Microsoft"},{"content":"In an enterprise level, it\u0026rsquo;s obvious for applications to be based on messaging for communication. This is done using a middleware between these applications as a Message Bus that enables them to work together.\nOne of the most used Messaging solutions is Apache Kafka: Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies. Kafka Clusters can be deployed in bare metal or in the Cloud.\nMicrosoft provides a great Azure Service for Kafka customers: Azure Event Hubs ü§©\nAzure Event Hubs is a fully managed, real-time data ingestion service that‚Äôs simple, trusted, and scalable. It provides streaming millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges. Azure Event Hubs keeps processing data during emergencies using the geo-disaster recovery and geo-replication features.\nAzure Event Hubs allows existing Apache Kafka clients and applications to talk to Event Hubs without any code changes‚Äîyou get a managed Kafka experience without having to manage your own clusters.\nIn this tutorial, I will try to make two small Spring Boot applications that will communicate thru the Azure Event Hubs.\nThe source code of the sample application that we will be developing in this post is available on Github.\nCreating an azure event hubs namespace First of all, we need to start by creating the Event Hubs Namespace:\n Name will be used for creating the Event Hubs Namespace URL Pricing Tier: Standard - The Kafka support is enabled for the Standard and Dedicated pricing tiers only.  Authorizing the access to the azure event hubs namespace Each Event Hubs namespace and each Event Hubs entity (an event hub instance or a Kafka topic) has a shared access authorization policy made up of rules. The policy at the namespace level applies to all entities inside the namespace, irrespective of their individual policy configuration. For each authorization policy rule, you decide on three pieces of information: name, scope, and rights. The name is a unique name in that scope. The scope is the URI of the resource in question. For an Event Hubs namespace, the scope is the fully qualified domain name (FQDN), such as¬†https://\u0026lt;yournamespace\u0026gt;.servicebus.windows.net/.\nThe rights provided by the policy rule can be a combination of:\n Send¬†‚Äì Gives the right to send messages to the entity Listen¬†‚Äì Gives the right to listen or receive to the entity Manage¬†‚Äì Gives the right to manage the topology of the namespace, including creation and deletion of entities  In our case we need to create a Shared Access Authorization Policy that can be Sending and Listening to our Event Hubs namespace:\nCreating an azure event hub Next, in my nebrass namespace, I will create a new Event Hub called topic-exchange¬†ü•≥\ngenerating our sample application project As usual, we will generate our project using the Spring Initializr. Our application will have 3 dependencies:\n Web Kafka Lombok  As you see here, there is no specific Azure dependency or specific library.\n In our example, we will have an application that is producing/consuming Kafka messages. Generally, in many tutorials you will find separated producer and consumer applications. But in our application, I wanted to have to have both features in the same application, which is a real world use case. ‚ö†Ô∏è It\u0026rsquo;s not mandatory that the producer application is not listening to the same source üòÖ\n Now we will be creating our Kafka application üòÅ\nLet\u0026rsquo;s start by renaming the application.properties to application.yaml - I like to use YAML üòÅ and we will insert the first value inside, which is the name of the Topic we want to write to:\n1 2  topic:name:exchange-topic  This topic name can be injected into the code using :\n1 2  @Value(\u0026#34;${topic.name}\u0026#34;) private String topicName;   Now, let\u0026rsquo;s move to defining the structure of our Kafka message, that we will be producing and consuming - our Message will be a small POJO with only one String body attribute :\n1 2 3 4 5 6  @Getter @NoArgsConstructor @AllArgsConstructor public class SimpleMessage { private String body; }   With the message, we need to add a JSON Serializer, which obviously ü§£ will be used to Serialize our message to JSON ü•∂\n1 2  public class ProducerMessageSerializer extends JsonSerializer\u0026lt;SimpleMessage\u0026gt; { }   Next, we can now create our Kafka producer, which will be a Spring Service:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  @Slf4j @Service @RequiredArgsConstructor public class KafkaProducer { private final KafkaTemplate\u0026lt;String, SimpleMessage\u0026gt; kafkaTemplate; @Value(\u0026#34;${topic.name}\u0026#34;) private String topicName; public void send(SimpleMessage message) { this.kafkaTemplate.send(topicName, message); log.info(\u0026#34;Published the message [{}] to the kafka queue: [{}]\u0026#34;, message.getBody(), topicName ); } }   But where the data will be sent ? üò± We need to go back to the application.yaml file to add some (many) configuration details: ü§ì\n1 2 3 4 5 6 7 8 9 10 11 12  spring:kafka:bootstrap-servers:nebrass.servicebus.windows.net:9093client-id:first-serviceproperties:sasl.jaas.config:org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#34;$ConnectionString\u0026#34; password=\u0026#34;Endpoint=sb://nebrass.servicebus.windows.net/;SharedAccessKeyName=SendReceiveOnly;SharedAccessKey=XXXX\u0026#34;;sasl.mechanism:PLAINsecurity.protocol:SASL_SSLproducer:value-serializer:com.targa.labs.dev.kafkaonazure.ProducerMessageSerializertopic:name:exchange-topic  Wow üò± There are many items added to the list. Keep calm, it\u0026rsquo;s very simple configuration:\n spring.kafka.bootstrap-servers: Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster üëâ This value is extracted by the Event Hubs name, in my case it\u0026rsquo;s: nebrass.servicebus.windows.net:9093 spring.kafka.client-id: ID to pass to the server when making requests. Used for server-side logging üëâ first-service spring.kafka.properties.sasl.mechanism: PLAIN üëâ PLAIN (also known as SASL/PLAIN) is a simple username/password authentication mechanism that is typically used with TLS for encryption to implement secure authentication. spring.kafka.properties.security.protocol: SASL_SSL üëâ this property ensures that all broker/client communication is encrypted and authenticated using SASL/PLAIN spring.kafka.properties.sasl.jaas.config: Configure the JAAS configuration property to describe how the clients like producer and consumer can connect to the Kafka Brokers. The properties username and password are used by clients to configure the user for client connections. In our example, clients connect to the broker as the username is \u0026ldquo;$ConnectionString\u0026rdquo; and the password will be our Azure EventHubs ConnectionString which is in our case the connection string of our Shared Access Authorization Policy. We will be using org.apache.kafka.common.security.plain.PlainLoginModule as the login module implementation which should provide username as the public credential and password as the private credential üëâ so our property value will be: org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026quot;$ConnectionString\u0026quot; password=\u0026ldquo;Endpoint=sb://nebrass.servicebus.windows.net/;SharedAccessKeyName=SendReceiveOnly;SharedAccessKey=XXXX\u0026rdquo;; spring.kafka.producer.value-serializer: Serializer class for values üëâ com.targa.labs.dev.kafkaonazure.ProducerMessageSerializer  Expose the kafka producer We will be inserting Kafka messages with a content received from a REST API. So for this, we will create a new RestController:\n1 2 3 4 5 6 7 8 9 10 11 12  @RestController @RequestMapping(\u0026#34;/api\u0026#34;) @RequiredArgsConstructor public class KafkaSender { private final KafkaProducer kafkaProducer; @PostMapping(\u0026#34;send\u0026#34;) public void sendData(@RequestBody SimpleMessage message) { this.kafkaProducer.send(message); } }   We can send some messages using Postman/Insomnia or even via command line, for example, using cURL:\n1  curl -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Hello there !\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://localhost:8080/api/send   Creating the Kafka consumer The same way we did the Kafka Producer, our Kafka Consumer will be a classic Spring Service with a method annotated with @KafkaListener on our Event Hub:\n1 2 3 4 5 6 7 8 9  @Service @Slf4j public class KafkaConsumer { @KafkaListener(topics = \u0026#34;${topic.name}\u0026#34;) public void receive(SimpleMessage consumerMessage) { log.info(\u0026#34;Received message from kafka queue: {}\u0026#34;, consumerMessage.getBody()); } }   So simple ! üòÅ but this will not be working, unless we add the Spring Kafka Consumer configuration in our application.yaml file:\n1 2 3 4 5 6 7 8 9  spring:kafka:...consumer:group-id:$Defaultproperties:spring.json:use.type.headers:falsevalue.default.type:com.targa.labs.dev.kafkaonazure.SimpleMessage  The listed configuration is:\n spring.kafka.consumer.group-id: required and unique string that identifies the consumer group this consumer belongs to. üëâ in our case it will be $Default to use the basic consumer group that was created when we created our Azure Event Hub. spring.kafka.consumer.properties.spring.json.use.type.headers: to prevent the Consumer even looking for headers üëâ in our case it will be false spring.kafka.consumer.properties.spring.json.value.default.type: the message to be consumed üëâ com.microsoft.cse.labs.kafkaeventhub.SimpleMessage  Cool ! Now our application will be producing a message to the Azure Event Hub, and it will be receiving it as soon as it is available üòÅ the final diagram of our ecosystem is:\nFinal words As you can see in our application, we don\u0026rsquo;t have any specific library or connector of Azure. We are developing a plain Spring Kafka application and we are just plugging it to Azure Event Hubs instead of a classic Kafka broker. The facility of developing without any constraints of the target broker is a huge add-on for the customers willing to move their applications to Azure Event Hubs. This abstraction and portability is one of the most wanted facility of the Java EE specifications, such as the abstraction for the JPA specification and the ability to be plugged to any Relational DBMS.\n","permalink":"https://blog.nebrass.fr/playing-with-spring-boot-and-kafka-on-azure-event-hub/","summary":"In an enterprise level, it\u0026rsquo;s obvious for applications to be based on messaging for communication. This is done using a middleware between these applications as a Message Bus that enables them to work together.\nOne of the most used Messaging solutions is Apache Kafka: Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.","title":"Playing with Spring Boot and Kafka on Azure Event Hub"},{"content":"The workshop \u0026ldquo;Playing with Java Microservices on Kubernetes\u0026rdquo; was held from 02/01 to 04/01 - 2020, at the National School of Computer Sciences, in Manouba, Tunisia.\nGreat gift üòç\n The workshop was done on 21 Hours of training about Java, Spring Boot, DDD, Docker, Kubernetes, Cloud Patterns, Azure AKS \u0026amp; Functions‚Ä¶\nSome Github repositories used in the sessions:\n Monolith Example Microservices Examples Speaker Deck  The event was sponsored by¬†Microsoft¬†üòçü§©ü•≥ !!\nMicrosoft üòç\n I want to express many thanks for Mrs Rim Drira for the invitation and for the great organization.\nGreat moments with Ahmed \u0026amp; Mrs Drira\n I got a very wonderful flowers bouquet and a very special gift¬†üòçAn other surprise ! I met my friend¬†Ahmed Mhenni¬†one of my greatest students of the previous edition of this workshop¬†ü§©ü•≥\nGreat gift üòç\n ","permalink":"https://blog.nebrass.fr/workshop-report-ensi-january-2020/","summary":"The workshop \u0026ldquo;Playing with Java Microservices on Kubernetes\u0026rdquo; was held from 02/01 to 04/01 - 2020, at the National School of Computer Sciences, in Manouba, Tunisia.\nGreat gift üòç\n The workshop was done on 21 Hours of training about Java, Spring Boot, DDD, Docker, Kubernetes, Cloud Patterns, Azure AKS \u0026amp; Functions‚Ä¶\nSome Github repositories used in the sessions:\n Monolith Example Microservices Examples Speaker Deck  The event was sponsored by¬†Microsoft¬†üòçü§©ü•≥ !","title":"Workshop Report: ENSI ‚Äì January 2020"},{"content":"Since the release of Java SE 8, all the developers were under the charm of the Lambdas, Streams and even there those who fell in love with Nashorn for years (yes there are somewhere in this globe üòÖ).. With the crazy growth of the Enterprise Development context thru Spring Boot/Cloud, Docker, Kubernetes and the unlimited number of JS frameworks, the infinite patterns and styles of architectures, many developers lost the frequency of being up-to-date with the upstream ü§™\nI was one of the unsynchronized Java developers that couldn\u0026rsquo;t get up-to-date with all this crazy new comers everyday ü•∫ü§Ø I got the idea to write a new post, to help Java developers get upToDate Quickly on the last five Java Releases üòÅ\nJava logo\n ‚ö†Ô∏è‚ö†Ô∏è I will try to list the most significant new features. This listing is inclusive but not exhaustive. ‚ö†Ô∏è‚ö†Ô∏è\nJava 9 - July 27, 2017 JEP 222: jshell: The Java Shell (Read-Eval-Print Loop) The JShell API and tool will provide a way to interactively evaluate declarations, statements, and expressions of the Java programming language within the JShell state. The JShell state includes an evolving code and execution state. To facilitate rapid investigation and coding, statements and expressions need not occur within a method, and variables and method need not occur within a class.\nThe¬†jshell¬†tool will be a command-line tool with features to ease interaction including: a history with editing, tab-completion, automatic addition of needed terminal semicolons, and configurable predefined imports and definitions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $ jshell | Welcome to JShell -- Version 9 | For an introduction type: /help intro jshell\u0026gt; int x = 2 x ==\u0026gt; 2 jshell\u0026gt; int y = 3 y ==\u0026gt; 3 jshell\u0026gt; System.out.println(\u0026#34;x + y = \u0026#34;+ (x+y)); x + y = 5 jshell\u0026gt; /exit | Goodbye   JSR 376: Java Platform Module System (JSR 376) This is the biggest new feature of Java 9.\nThe primary goals of this project are to:\n Make it easier for developers to construct and maintain libraries and large applications; Improve the security and maintainability of Java¬†SE Platform Implementations in general, and the JDK in particular; Enable improved application performance; Enable the Java SE Platform, and the JDK, to scale down for use in small computing devices and dense cloud deployments.  To achieve these goals, the JCP designed and implemented a standard module system for the Java 9 and applied that system to the Platform itself and to its Reference Implementation, JDK¬†9. The module system is powerful enough to modularize the JDK and other large legacy code bases, yet is still approachable by all developers.\nA Module is a group of Java classes and resources packaged together with a descriptor file called module-info.java.\nFor sure, this JSR cannot be covered in this quick post. There are many books dedicated for this huge new feature:\n The Java Module System - Manning Java 9 Modularity Revealed - Apress Java 9 Modularity - O\u0026rsquo;Reilly Modular Programming in Java 9 - Packtpub  Private methods in Interfaces Since Java 8, we have a new feature: Functional Interface which is an interface class that contains only a single abstract (unimplemented) method. A functional interface can contain default public and static methods have an implementation, in addition to the single unimplemented method. Starting from Java 9, these methods can be private.\nFor example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  @FunctionalInterface public interface MyFunctionalInterface { Logger log = Logger.getLogger(MyFunctionalInterface.class.getName()); void find(); default void init() { this.logOperation(\u0026#34;init\u0026#34;); } private void logOperation(String operation) { log.info(\u0026#34;Operation [\u0026#34; + operation + \u0026#34;] is called.\u0026#34;); } }   JEP 110: the new HTTP/2 Client Java 9 comes with a brand new native HTTP client that supports¬†HTTP/2 with backward compatibility with the previous versions of HTTP. With this new HTTP client, we will not need additional libraries to call a REST API:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  public static void main(String[] args) throws Exception { String url = \u0026#34;https://api.exchangeratesapi.io/latest\u0026#34;; HttpClient client = HttpClient.newHttpClient(); HttpRequest httpRequest = HttpRequest.newBuilder(new URI(url)) .header(\u0026#34;Accept\u0026#34;, \u0026#34;application/json\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .GET() .build(); HttpResponse\u0026lt;String\u0026gt; httpResponse = client.send(httpRequest, HttpResponse.BodyHandlers.ofString()); String jsonResponse = httpResponse.body(); System.out.println(jsonResponse); }   The code listed here was updated after the release Java 11: my actual runtime üòÅ\nEnhanced Try-with-Resources: Java 7 brought a new feature called Try-with-Resource statement which was a very useful feature for handling/closing a resource when an exception occurs. This Try-with-Resource statement got updated in Java 9 to be more simplified and less verbose. Let\u0026rsquo;s take this example:\nThe Java 7 version:\n1 2 3 4 5 6 7 8 9  public static void main(String[] args) throws Exception { FileOutputStream fileOutputStream = new FileOutputStream(\u0026#34;document.txt\u0026#34;); try(FileOutputStream fileOutputStream2 = fileOutputStream){ String content = \u0026#34;Hello World from Java 7\u0026#34;; fileOutputStream2.write(content.getBytes()); }catch(Exception e) { log.error(e.getMessage()); } }   The Java 9 version:\n1 2 3 4 5 6 7 8 9  public static void main(String[] args) throws Exception { FileOutputStream fileOutputStream = new FileOutputStream(\u0026#34;document.txt\u0026#34;); try (fileOutputStream) { String content = \u0026#34;Hello World from Java 9\u0026#34;; fileOutputStream.write(content.getBytes()); } catch (Exception e) { log.error(e.getMessage()); } }   JEP 102: Process API Updates The Process API got also updated in Java 9: new classes and methods are introduced to ease the controlling and managing of OS processes.\nFor example, to show the current PID:\n1  System.out.println(\u0026#34;Current PID: \u0026#34; + ProcessHandle.current().getPid());   JEP 269: Convenience Factory Methods for Collections Java 9 has introduced some convenient factory methods to create Immutable List, Set, Map and Map.Entry objects. These utility methods are used to create empty or non-empty Collection objects. Let\u0026rsquo;s take the example of creating an immutable List:\nBefore Java 9:\n1 2 3 4 5  List\u0026lt;String\u0026gt; letters = new ArrayList\u0026lt;\u0026gt;(); letters.add(\u0026#34;A\u0026#34;); letters.add(\u0026#34;B\u0026#34;); letters.add(\u0026#34;C\u0026#34;); List\u0026lt;String\u0026gt; immutableLettersList = Collections.unmodifiableList(letters);   In Java 9:\n1  List\u0026lt;String\u0026gt; immutableLettersList = List.of(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;);   Enhanced Optional class The Optional class got some new methods in Java 9:\n  ifPresentOrElse(): If a value is present, performs the given action with the value, otherwise performs the given empty-based action.\n  or(): If a value is present, returns an Optional describing the value, otherwise returns an Optional produced by the supplying function.\n  stream(): If a value is present, returns a sequential Stream¬†containing only that value, otherwise returns an empty¬†Stream.\n  Enhanced Stream API In Java 9, we got four new methods to java.util.Stream interface:\n takeWhile(): Returns, if this stream is ordered, a stream consisting of the longest prefix of elements taken from this stream that match the given predicate. Otherwise returns, if this stream is unordered, a stream consisting of a subset of elements taken from this stream that match the given predicate. dropWhile(): Returns, if this stream is ordered, a stream consisting of the remaining elements of this stream after dropping the longest prefix of elements that match the given predicate. Otherwise returns, if this stream is unordered, a stream consisting of the remaining elements of this stream after dropping a subset of elements that match the given predicate. ofNullable(): Returns a sequential Stream containing a single element, if non-null, otherwise returns an empty Stream. iterate(): Returns a sequential ordered Stream produced by iterative application of the given next function to an initial element, conditioned on satisfying the given hasNext predicate. The stream terminates as soon as the hasNext predicate returns false.  For example: this code will show the numbers less than 50:\n1 2 3 4 5  Stream\u0026lt;Integer\u0026gt; numbersStream = Stream.iterate(1, n -\u0026gt; n * 10) .limit(10); numbersStream.takeWhile(num -\u0026gt; num \u0026lt; 50) .forEach(System.out::println);   Enhanced Diamond operator Java 9 improved the use of diamond operator and allows us to use the diamond operator with anonymous inner classes. Let\u0026rsquo;s take this example, which was not possible to run before Java 9:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  abstract class Calculator\u0026lt;T\u0026gt; { abstract T add(T num, T num2); abstract T sub(T num, T num2); abstract T mult(T num, T num2); abstract T div(T num, T num2); } public static void main(String[] args) { Calculator\u0026lt;Integer\u0026gt; obj = new Calculator\u0026lt;\u0026gt;() { Integer add(Integer x, Integer y) { return x + y; } Integer sub(Integer x, Integer y) { return x - y; } Integer mult(Integer x, Integer y) { return x * y; } Integer div(Integer x, Integer y) { return x / y; } }; Integer sum = obj.add(2, 4); System.out.println(sum); Integer sub = obj.sub(100, 10); System.out.println(sub); Integer mult = obj.mult(5, 6); System.out.println(mult); Integer div = obj.div(100, 3); System.out.println(div); }   JEP 266: More Concurrency Updates Java 9 comes with some changes to the¬†CompletableFuture class to solve some problems raised since its introduction in Java 8. Now, it supports delays and timeouts, and a better support for subclassing.\nThe new class methods:\n Executor defaultExecutor() CompletableFuture\u0026lt;U\u0026gt; newIncompleteFuture() CompletableFuture\u0026lt;T\u0026gt; copy() CompletionStage\u0026lt;T\u0026gt; minimalCompletionStage() CompletableFuture\u0026lt;T\u0026gt; completeAsync(Supplier\u0026lt;? extends T\u0026gt; supplier, Executor executor) CompletableFuture\u0026lt;T\u0026gt; completeAsync(Supplier\u0026lt;? extends T\u0026gt; supplier) CompletableFuture\u0026lt;T\u0026gt; orTimeout(long timeout, TimeUnit unit) CompletableFuture\u0026lt;T\u0026gt; completeOnTimeout(T value, long timeout, TimeUnit unit)  The new static methods:\n Executor delayedExecutor(long delay, TimeUnit unit, Executor executor) Executor delayedExecutor(long delay, TimeUnit unit) \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; completedStage(U value) \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; failedStage(Throwable ex) \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; failedFuture(Throwable ex)  The full list of the Java 9 new features.\nJava 10 - March 20, 2018 JEP 286: Local-Variable Type Inference Local-Variable Type Inference is the biggest new feature in Java 10 that many many developers are waiting for ü•≥ It adds type inference to declarations of local variables with initializers. Now, we can do this in Java üòÅ\n1 2 3 4 5 6 7 8 9 10 11  public static void main(String[] args) { var numbers = List.of(1, 2, 3, 4, 5); for (var number : numbers) { System.out.println(number); } var letters = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); for (var letter : letters) { System.out.println(letter); } }   In this code:\n numbers will be inferred as ArrayList\u0026lt;Integer\u0026gt; number will be inferred as Integer letters will be inferred as ArrayList\u0026lt;String\u0026gt; letter will be inferred as String  Java Improvements for Docker Containers Java 10 came with many new features to improve the execution and configurability of Java running in Docker containers. I covered this great feature in a dedicated blog post: Playing with the JVM inside Docker Containers.\nJEP 304: Garbage-Collector Interface Java 10 improves the source code isolation of different garbage collectors by introducing a clean garbage collector (GC) interface that helps:\n Better modularity for HotSpot internal GC code Make it simpler to add a new GC to HotSpot without perturbing the current code base Make it easier to exclude a GC from a JDK build  JEP 307: Parallel Full Garbage Collector for G1 Since Java 9, G1 garbage collector is the default garbage collector. The JEP 307 improves G1 worst-case latencies by making the full GC parallel. The G1 garbage collector is designed to avoid full collections, but when the concurrent collections can\u0026rsquo;t reclaim memory fast enough a fall back full GC will occur. The old implementation of the full GC for G1 used a single threaded mark-sweep-compact algorithm. With JEP 307 the full GC has been parallelized and now use the same amount of parallel worker threads as the young and mixed collections.\nJEP 310: Application Class-Data Sharing The goal of this feature is to improve the startup footprint, extends the existing Class-Data Sharing (\u0026ldquo;CDS\u0026rdquo;) feature to allow application classes to be placed in the shared archive.\nClass-Data Sharing, introduced in JDK 5, allows a set of classes to be pre-processed into a shared archive file that can then be memory-mapped at runtime to reduce startup time. It can also reduce dynamic memory footprint when multiple JVMs share the same archive file.\nCurrently CDS only allows the bootstrap class loader to load archived classes. Application CDS allows the built-in system class loader, the built-in platform class loader, and custom class loaders to load archived classes.\nSpecify the¬†-XX:+UseAppCDS¬†command-line option to enable class data sharing for the system class loader, the platform class loader, and other user-defined class loaders.\nJEP 312: Thread-Local Handshakes This internal JVM feature aims to improve performance. A handshake operation is a callback that is executed for each JavaThread while that thread is in a safepoint state. The callback is executed either by the thread itself or by the VM thread while keeping the thread in a blocked state.\nThis feature provides a way to execute a callback on threads without performing a global VM safepoint. Make it both possible and cheap to stop individual threads and not just all threads or none.\nJEP 313: Remove the Native-Header Generation Tool (javah) Tool javah has been removed from Java 10 which generated C headers and source files which were required to implement native methods ‚Äì now, javac -h can be used instead.\nJEP 314: Additional Unicode Language-Tag Extensions It‚Äôs goal is to enhance java.util.Locale and related APIs to implement additional Unicode extensions of BCP 47 language tags.\nSupport for BCP 47 language tags was was initially added in Java 7, with support for the Unicode locale extension limited to calendars and numbers. This JEP will implement more of the extensions specified in the latest LDML specification, in the relevant JDK classes.\nThis JEP will add support for the following additional extensions:\n cu (currency type) fw (first day of week) rg (region override) tz (time zone)  Related APIs which got modified are:\n java.text.DateFormat::get*Instance java.text.DateFormatSymbols::getInstance java.text.DecimalFormatSymbols::getInstance java.text.NumberFormat::get*Instance java.time.format.DateTimeFormatter::localizedBy java.time.format.DateTimeFormatterBuilder::getLocalizedDateTimePattern java.time.format.DecimalStyle::of java.time.temporal.WeekFields::of java.util.Calendar::getFirstDayOfWeek java.util.Calendar::getMinimalDaysInWeek java.util.Currency::getInstance java.util.Locale::getDisplayName java.util.spi.LocaleNameProvider  JEP 317: Experimental Java-Based JIT Compiler This JEP will enable Graal: the Java-based Just-in-Time compiler, to be used as an experimental JIT compiler on the Linux/x64 platform.\nGraal was introduced in Java 9. It‚Äôs an alternative to the JIT compiler which we have been used to, which was written in C++. It‚Äôs an addon to the JVM, which means that the JIT compiler is not tied to JVM and it can be dynamically plugged in and replaced with any another plugin. It also brings :\n Ahead of Time (AOT) compilation in Java Applications the support of polyglot language interpretation  JEP 319: Root Certificates This JEP will open-source the root certificates in Oracle\u0026rsquo;s Java SE Root CA program in order to make OpenJDK builds more attractive to developers, and to reduce the differences between those builds and Oracle JDK builds. This means that both Oracle JDK \u0026amp; OpenJDK binaries will be functionally the same.\nJEP 322: Time-Based Release Versioning This JEP will revise the version-string scheme of the Java SE Platform and the JDK, and related versioning information, for present and future time-based release models. Especially that since Java 10, we intend to ship new releases of the Java¬†SE Platform and the JDK on a¬†strict, six-month cadence.\nThe new format will be:\n[1-9][0-9]*((.0)*.[1-9][0-9]*)*\nThe sequence may be of arbitrary length but the first four elements are assigned specific meanings, as follows:\n$FEATURE.$INTERIM.$UPDATE.$PATCH\n $FEATURE¬†‚Äî The feature-release counter, incremented for every feature release regardless of release content. Features may be added in a feature release; they may also be removed, if advance notice was given at least one feature release ahead of time. Incompatible changes may be made when justified. (Formerly¬†$MAJOR.) $INTERIM¬†‚Äî The interim-release counter, incremented for non-feature releases that contain compatible bug fixes and enhancements but no incompatible changes, no feature removals, and no changes to standard APIs. (Formerly¬†$MINOR.) $UPDATE¬†‚Äî The update-release counter, incremented for compatible update releases that fix security issues, regressions, and bugs in newer features. (Formerly¬†$SECURITY, but with a non-trivial incrementation rule.) $PATCH¬†‚Äî The emergency patch-release counter, incremented only when it\u0026rsquo;s necessary to produce an emergency release to fix a critical issue. (Using an additional element for this purpose minimizes disruption to both developers and users of in-flight update releases.)  JEP 296: Consolidate the JDK Forest into a Single Repository For many years, the full JDK code base has been splitted into numerous Mercurial repositories. In Java 9, there are eight repos:\n root corba, hotspot jaxp jaxws jdk langtools nashorn  The individual repos don\u0026rsquo;t have a development cycle separate from the JDK as a whole; all the repos advance in lockstep with the JDK promotion cycle. The multiplicity of repos presents a larger than necessary barrier to entry to new developers and has lead to workarounds such as the \u0026ldquo;get source\u0026rdquo; script.\nStarting from Java 10, the numerous repositories of the JDK forest will be combined into a single repository in order to simplify the development.\nThe full list of the Java 10 new features.\nJava 11 - September 25, 2018 JEP 330: Launch Single-File Source-Code Programs One major change is that you don‚Äôt need to compile the java source file with¬†javac¬†tool first. You can directly run the file with¬†java command and it will implicitly compile before running.\nNew String methods Java 11 brings new methods in the String class:\n lines(): Returns a stream of lines extracted from this string, separated by line terminators. isBlank(): Returns true if the string is empty or contains only white spaces, otherwise false. repeat(): Returns a string whose value is the concatenation of this string repeated count times. strip(): Returns a string whose value is this string, with all leading and trailing¬†white space¬†removed. stripLeading(): Returns a string whose value is this string, with all leading¬†white space¬†removed. stripTrailing(): Returns a string whose value is this string, with all trailing¬†white space¬†removed. lines(): Returns a stream of lines extracted from this string, separated by line terminators.  JEP 323: Local-Variable Syntax for Lambda Parameters This JEP allows the keyword var to be used when declaring the formal parameters of implicitly typed lambda expressions. For example, we can do this in Java 11:\n(var x, var y) -\u0026gt; x.process(y)\nNew methods added in java.nio.file.Files Java 11 introduced new methods in the java.nio.file.Files class:\n readString(): Reads all content from a file into a string, decoding from bytes to characters. writeString(): Write a CharSequence to a file. isSameFile(): Tests if two paths locate the same file.  Working example using these new methods:\n1 2 3 4 5 6 7 8 9  public static void main(String[] args) throws IOException { Path tmpFilePath = Files.writeString(Files.createTempFile(\u0026#34;test\u0026#34;, \u0026#34;.txt\u0026#34;), \u0026#34;Hello World\u0026#34;); System.out.println(\u0026#34;File name: [\u0026#34; + tmpFilePath.toUri() + \u0026#34;]\u0026#34;); String content = Files.readString(tmpFilePath); System.out.println(\u0026#34;File content: [\u0026#34; + content + \u0026#34;]\u0026#34;); Path anOtherTmpFilePath = Files.createTempFile(\u0026#34;test\u0026#34;, \u0026#34;.txt\u0026#34;); boolean sameFile = Files.isSameFile(tmpFilePath, anOtherTmpFilePath); System.out.println(\u0026#34;Are two tmp files equals? : \u0026#34; + sameFile); }   This sample code will have an output similar to:\nFile name: [file:///var/folders/0v/m4k0wtm521xfsmwyfr1_zslm0000gn/T/test303123248058134079.txt] File content: [Hello World] Are two tmp files equals? : false\nNew ToArray() method in the Collection Class The new toArray(java.util.function.IntFunction): Returns an array containing all of the elements in this collection, using the provided generator function to allocate the returned array. For example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  public static void main(String[] args) { List\u0026lt;String\u0026gt; letters = new ArrayList\u0026lt;\u0026gt;(); letters.add(\u0026#34;A\u0026#34;); letters.add(\u0026#34;B\u0026#34;); letters.add(\u0026#34;C\u0026#34;); letters.add(\u0026#34;D\u0026#34;); //Before Java 11  String[] lettersArray = letters.toArray(new String[letters.size()]); //Since Java 11  String[] anOtherLettersArray = letters.toArray(String[]::new); }   New isEmpty() method in the Optional class The method Optional.isEmpty() returns true if the value of any object is null and else returns false. This can be done in Java 11:\n1 2 3 4  public static void main(String[] args) { Optional notSure = Optional.ofNullable(null); System.out.println(notSure.isEmpty()); }   JEP 321: HTTP Client (Standard) Java 11 has a native HTTP Client supporting WebSocket connections! The client was already introduced in Java 9. But in the Java 11, the HTTP Client API get standardized ü•≥\nHere is a Java 11 WebSocket example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  @Slf4j public class Main { public static void main(String[] args) throws InterruptedException { ExecutorService executor = Executors.newFixedThreadPool(4); HttpClient httpClient = HttpClient.newBuilder().executor(executor).build(); WebSocket.Builder webSocketBuilder = httpClient.newWebSocketBuilder(); WebSocket webSocket = webSocketBuilder.buildAsync(URI.create(\u0026#34;wss://echo.websocket.org\u0026#34;), new WebSocket.Listener() { @Override public void onOpen(WebSocket webSocket) { log.info(\u0026#34;CONNECTED\u0026#34;); webSocket.sendText(\u0026#34;This is a message\u0026#34;, true); WebSocket.Listener.super.onOpen(webSocket); } @Override public CompletionStage\u0026lt;?\u0026gt; onText(WebSocket webSocket, CharSequence data, boolean last) { log.info(\u0026#34;onText received with data \u0026#34; + data); if (!webSocket.isOutputClosed()) { webSocket.sendText(\u0026#34;This is a message\u0026#34;, true); } return WebSocket.Listener.super.onText(webSocket, data, last); } @Override public CompletionStage\u0026lt;?\u0026gt; onClose(WebSocket webSocket, int statusCode, String reason) { log.info(\u0026#34;Closed with status \u0026#34; + statusCode + \u0026#34;, reason: \u0026#34; + reason); executor.shutdown(); return WebSocket.Listener.super.onClose(webSocket, statusCode, reason); } }).join(); log.info(\u0026#34;WebSocket created\u0026#34;); Thread.sleep(1000); webSocket.sendClose(WebSocket.NORMAL_CLOSURE, \u0026#34;ok\u0026#34;).thenRun(() -\u0026gt; log.info(\u0026#34;Sent close\u0026#34;)); } }   JEP 328: Flight Recorder Flight Recorder is a profiling tool used to gather diagnostics and profiling data from a running Java application. Its performance overhead is negligible and that‚Äôs why it can be used in production.\nFlight Recorder records events originating from applications, the JVM and the OS. Events are stored in a single file that can be attached to bug reports and examined by support engineers, allowing after-the-fact analysis of issues in the period leading up to a problem. Tools can use an API to extract information from recording files.\nJEP 331: Low-Overhead Heap Profiling This JEP provides a way to get information about Java object heap allocations from the JVM that:\n Is low-overhead enough to be enabled by default continuously Is accessible via a well-defined, programmatic interface Can sample all allocations Can be defined in an implementation-independent way (i.e., not limited to a particular GC algorithm or VM implementation) Can give information about both live and dead Java objects.  JEP 335: Deprecate the Nashorn JavaScript Engine This JEP is coming to deprecate the Nashorn JavaScript Engine introduced in Java 8, with the intent to remove them in a future release üò±\nThe full list of the Java 11 new features.\nJava 12 - March 19, 2019 JEP 325: Switch Expressions (Preview) I think this is the most important feature coming in Java 12. This is a preview feature that extends the switch statement so that it can be used as either a statement or an expression. Let\u0026rsquo;s say what\u0026rsquo;s new:\n There is no need for break statement - your case will not be falling into the next case. We can define multiple constants in the same label. default¬†case is now compulsory in Switch Expressions. break¬†is used in Switch Expressions to return values from a case itself.  These changes will simplify everyday coding ü§© for example, instead of doing this before Java 12:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  public class Main { public static void main(String[] args) { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = new Scanner(System.in).nextLine(); String price; switch (choice) { case \u0026#34;1\u0026#34;: case \u0026#34;3\u0026#34;: case \u0026#34;7\u0026#34;: price = \u0026#34;5$\u0026#34;; break; case \u0026#34;2\u0026#34;: case \u0026#34;4\u0026#34;: case \u0026#34;9\u0026#34;: price = \u0026#34;15$\u0026#34;; break; case \u0026#34;5\u0026#34;: case \u0026#34;6\u0026#34;: case \u0026#34;8\u0026#34;: price = \u0026#34;35$\u0026#34;; break; default: price = \u0026#34;Unknown product..\u0026#34;; break; } System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   We can now do it in the easy way:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  public class Main { public static void main(String[] args) { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = new Scanner(System.in).nextLine(); String price = switch (choice) { case \u0026#34;1\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;7\u0026#34; -\u0026gt; \u0026#34;5$\u0026#34;; case \u0026#34;2\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;9\u0026#34; -\u0026gt; \u0026#34;15$\u0026#34;; case \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;8\u0026#34; -\u0026gt; \u0026#34;35$\u0026#34;; default -\u0026gt; \u0026#34;Unknown product..\u0026#34;; }; System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   As it\u0026rsquo;s a preview feature, you will need to add --enable-preview parameter to the javac command in order to compile this code under JDK 12.\nJEP 230: Microbenchmark Suite This JEP adds a basic suite of microbenchmarks to the JDK source code, and make it easy to run existing microbenchmarks and create new ones. It is based on the¬†Java Microbenchmark Harness (JMH).\nLet\u0026rsquo;s test JMH; first of all, you can generate the sample project using the command:\n1 2 3 4 5 6 7  $ mvn archetype:generate \\  -DinteractiveMode=false \\  -DarchetypeGroupId=org.openjdk.jmh \\  -DarchetypeArtifactId=jmh-java-benchmark-archetype \\  -DgroupId=org.sample \\  -DartifactId=test \\  -Dversion=1.0   This command will create a Maven project, containing only one class called org.sample.MyBenchmark. We will use it to benchmark our code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  public class MyBenchmark { @Benchmark public void testMethod() { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = \u0026#34;5\u0026#34;; String price; switch (choice) { case \u0026#34;1\u0026#34;: case \u0026#34;3\u0026#34;: case \u0026#34;7\u0026#34;: price = \u0026#34;5$\u0026#34;; break; case \u0026#34;2\u0026#34;: case \u0026#34;4\u0026#34;: case \u0026#34;9\u0026#34;: price = \u0026#34;15$\u0026#34;; break; case \u0026#34;5\u0026#34;: case \u0026#34;6\u0026#34;: case \u0026#34;8\u0026#34;: price = \u0026#34;35$\u0026#34;; break; default: price = \u0026#34;Unknown product..\u0026#34;; break; } System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   Now, compile the project using: mvn clean install - this command will build two jars:\n  test-1.0.jar üëâ the compiled project\n  benchmarks.jar üëâ the JMH Jar file üòÅ to run the benchmarking, run this jar using the java -jar target/benchmarks.jar command.\n  The output of running the benchmarks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.openjdk.jmh.util.Utils (file:/Users/nebrass/temp/test/target/benchmarks.jar) to field java.io.Console.cs WARNING: Please consider reporting this to the maintainers of org.openjdk.jmh.util.Utils WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release # JMH version: 1.22 ... # VM options: \u0026lt;none\u0026gt; # Warmup: 5 iterations, 10 s each # Measurement: 5 iterations, 10 s each # Timeout: 10 min per iteration # Threads: 1 thread, will synchronize iterations # Benchmark mode: Throughput, ops/time # Benchmark: org.sample.MyBenchmark.testMethod # Run progress: 0,00% complete, ETA 00:08:20 # Fork: 1 of 5 # Warmup Iteration 1: Enter your product number ... 80421,846 ops/s Result \u0026#34;org.sample.MyBenchmark.testMethod\u0026#34;: 60122,841 ¬±(99.9%) 18195,223 ops/s [Average] (min, avg, max) = (21484,177, 60122,841, 82116,041), stdev = 24290,103 CI (99.9%): [41927,618, 78318,065] (assumes normal distribution) # Run complete. Total time: 00:08:22 REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial experiments, perform baseline and negative tests that provide experimental control, make sure the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts. Do not assume the numbers tell you what you want them to tell. Benchmark Mode Cnt Score Error Units MyBenchmark.testMethod thrpt 25 60122,841 ¬± 18195,223 ops/s   JEP 189: Shenandoah: A Low-Pause-Time Garbage Collector (Experimental) This feature adds a new garbage collection (GC) algorithm named Shenandoah which reduces GC pause times by doing evacuation work concurrently with the running Java threads. Pause times with Shenandoah are independent of heap size, meaning you will have the same consistent pause times whether your heap is 200 MB or 200 GB.\nNew Teeing() method in Collectors Class Java 12 introduces a new teeing collector in the Collectors class. This collector forwards its input to two other collectors before merging their results with a function.\nFor example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  public class Main { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; numbers = IntStream.range(1, 100) .boxed() .collect(Collectors.toList()); Double average = numbers.stream() .collect(Collectors.teeing( Collectors.summingDouble(i -\u0026gt; i), Collectors.counting(), (sum, n) -\u0026gt; sum / n)); System.out.println(average); } }   JEP 341 Default CDS Archives This JEP enhances the build process to generate a Class Data-Sharing (CDS) archive, using the default class list, on 64-bit platforms. This aims to improve startup time. From Java 12, CDS is by default ON.\nJEP 344 : Abortable Mixed Collections for G1 This JEP bring improvements in G1 efficiency include making G1 mixed collections abortable if they might exceed the defined pause target. This is done by splitting the mixed collection set into mandatory and optional. Thus the G1 collector can prioritize on collecting the mandatory set first to meet the pause time goal.\nNew MISMATCH() method in the java.nio.files class Based on the javadoc: Finds and returns the position of the first mismatched byte in the content of two files, or¬†-1L if there is no mismatch. For example:\n1 2 3 4 5 6 7 8  public class Main { public static void main(String[] args) throws IOException { Path tmpFilePath = Files.writeString(Files.createTempFile(\u0026#34;test\u0026#34;, \u0026#34;.txt\u0026#34;), \u0026#34;Hello World\u0026#34;); Path anOtherTmpFilePath = Files.writeString(Files.createTempFile(\u0026#34;test\u0026#34;, \u0026#34;.txt\u0026#34;), \u0026#34;HelloWorld\u0026#34;); long mismatch = Files.mismatch(tmpFilePath, anOtherTmpFilePath); System.out.println(\u0026#34;Mismatch: [\u0026#34; + mismatch + \u0026#34;]\u0026#34;); } }   This code will output Mismatch: [5] as the first different character between \u0026quot;Hello World\u0026quot; and \u0026quot;HelloWorld\u0026quot; will be on the 5th position.\nJEP 305: Pattern Matching for instanceof (Preview) This is a preview of a new feature. This JEP will enhance the Java programming language with pattern matching¬†for the¬†instanceof¬†operator.¬†Pattern matching¬†allows common logic in a program, namely the conditional extraction of components from objects, to be expressed more concisely and safely.\nBefore Java 12:\n1 2 3 4  if (obj instanceof String) { String s = (String) obj; // use s }   The new way is :\n1 2 3  if (obj instanceof String s) { // can use s directly here }   This will improve the readability of the code üòÅ\nNew methods in the String class Java 12 introduced many new useful methods:\n indent(): Adjusts the indentation of each line of this string based on the value of n, and normalizes line termination characters. transform(): This method allows the application of a function to this string. describeConstable(): Returns an Optional containing the nominal descriptor for this instance, which is the instance itself. resolveConstantDesc(): Resolves this instance as a ConstantDesc, the result of which is the instance itself.  The full list of the Java 12 new features.\nJava 13 - September 17, 2019 JEP 354: Switch Expressions (Preview) Switch expressions were¬†proposed in December 2017¬†by¬†JEP 325. They were¬†targeted to JDK 12 in August 2018¬†as a¬†preview feature. Feedback was sought initially on the design of the feature, and later on the experience of using¬†switch¬†expressions and the enhanced¬†switch¬†statement. Based on that feedback, this JEP makes one change to the feature:\n To yield a value from a¬†switch¬†expression, the¬†break¬†with value statement is dropped in favor of a¬†yield¬†statement.\n This Java 12 code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  public class Main { public static void main(String[] args) { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = new Scanner(System.in).nextLine(); String price = switch (choice) { case \u0026#34;1\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;7\u0026#34; -\u0026gt; \u0026#34;5$\u0026#34;; case \u0026#34;2\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;9\u0026#34; -\u0026gt; \u0026#34;15$\u0026#34;; case \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;8\u0026#34; -\u0026gt; \u0026#34;35$\u0026#34;; default -\u0026gt; \u0026#34;Unknown product..\u0026#34;; }; System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   Can be written in Java 13 to be like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  public class Main { public static void main(String[] args) { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = new Scanner(System.in).nextLine(); String price = switch (choice) { case \u0026#34;1\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;7\u0026#34; : yield \u0026#34;5$\u0026#34;; case \u0026#34;2\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;9\u0026#34; : yield \u0026#34;15$\u0026#34;; case \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;8\u0026#34; : yield \u0026#34;35$\u0026#34;; default : yield \u0026#34;Unknown product..\u0026#34;; }; System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   ‚ö†Ô∏è‚õîÔ∏è‚ö†Ô∏è‚õîÔ∏è The Value Break is no more compilable in Java 13:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  public class Main { public static void main(String[] args) { System.out.println(\u0026#34;Enter your product number\u0026#34;); String choice = new Scanner(System.in).nextLine(); String price = switch (choice) { case \u0026#34;1\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;7\u0026#34; : break \u0026#34;5$\u0026#34;; case \u0026#34;2\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;9\u0026#34; : break \u0026#34;15$\u0026#34;; case \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;8\u0026#34; : break \u0026#34;35$\u0026#34;; default : break \u0026#34;Unknown product..\u0026#34;; }; System.out.println(\u0026#34;Price is: \u0026#34; + price); System.out.println(\u0026#34;Thank you for your visit\u0026#34;); } }   As it‚Äôs a preview feature, you will need to add¬†--enable-preview¬†parameter to the¬†javac¬†command in order to compile this code under¬†JDK 13.\nJEP 355: Text Blocks (Preview) Add¬†text blocks¬†to the Java language. A text block is a multi-line string literal that avoids the need for most escape sequences, automatically formats the string in a predictable way, and gives the developer control over format when desired.\nThis feature will improve the readability of the code and will remove the Strings boilerplate that hurt the developers when dealing with multilines Strings.\nFor example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  public class Main { public static void main(String[] args) { String value = \u0026#34;line 1\\n\u0026#34; + \u0026#34;line 2\\n\u0026#34; + \u0026#34;line 3\\n\u0026#34;; String multiLineValue = \u0026#34;\u0026#34;\u0026#34; line 1 line 2 line 3 \u0026#34;\u0026#34;\u0026#34;; System.out.println(value.equals(multiLineValue)); } }   This is a¬†preview language feature in JDK 13, you will need to add --enable-preview¬†parameter to the¬†javac command in order to compile this code.\nNew Methods in THE String Class There are three new methods in the String class that complement the Text Blocks feature:\n formatted(): Formats using this string as the format string, and the supplied arguments. stripIndent(): Returns a string whose value is this string, with incidental white space removed from the beginning and end of every line. translateEscapes(): Returns a string whose value is this string, with escape sequences translated as if in a string literal.  New Factory methods in the FileSystems class This JEP introduces three new methods in the FileSystems class to make it easier to use file system providers, which treats the contents of a file as a file system.\n newFileSystem‚Äã(Path path): Constructs a new FileSystem to access the contents of a file as a file system. newFileSystem‚Äã(Path path, Map\u0026lt;String,‚Äã?\u0026gt; env): Constructs a new FileSystem to access the contents of a file as a file system. newFileSystem‚Äã(Path path, Map\u0026lt;String,‚Äã?\u0026gt; env, ClassLoader loader): Constructs a new FileSystem to access the contents of a file as a file system.  JEP 353: Reimplement the Legacy Socket API The underlying implementation of the java.net.Socket and java.net.ServerSocket APIs have been rewritten. The new implementation, NioSocketImpl, is a drop-in replacement for PlainSocketImpl.\nIt uses java.util.concurrent locks rather than synchronized methods. If you want to use the legacy implementation, use the java option¬†-Djdk.net.usePlainSocketImpl.\nJEP 350: Dynamic CDS Archives This JEP enhanced the¬†JEP 310 Application Class-Data Sharing¬†which was introduced in Java 10, by simplifying the process of creating CDS archives.\nThe CDS archives will be created if the program exists with¬†-XX:ArchiveClassesAtExit\n1  $ java -XX:ArchiveClassesAtExit=hello.jsa -cp hello.jar Hello   To run the program with the CDS archives above:\n1  $ java -XX:SharedArchiveFile=hello.jsa -cp hello.jar Hello   The idea behind Class Data Sharing (CDS) is a feature to improve startup performance by creating class-data archive once and then reuse it, so that the JVM need not recreate it again.\nJEP 351: ZGC: Uncommit Unused Memory This JEP has enhanced ZGC to return unused heap memory to the operating system. The Z Garbage Collector was introduced in¬†Java 11. It adds a short pause time before the heap memory cleanup. But, the unused memory was not being returned to the operating system. This was a concern for devices with small memory footprint such as IoT and microchips. Now, it has been enhanced to return the unused memory to the operating system.\nThe full list of the Java 13 new features.\nFinal Words As you see, Java is bringing a large number of new features and APIs. And it will continue to deliver incremental evolutions of the language, ensuring it remains the most popular and the most wonderful programming language on the planet üòç\n","permalink":"https://blog.nebrass.fr/playing-with-java-from-9-to-13/","summary":"Since the release of Java SE 8, all the developers were under the charm of the Lambdas, Streams and even there those who fell in love with Nashorn for years (yes there are somewhere in this globe üòÖ).. With the crazy growth of the Enterprise Development context thru Spring Boot/Cloud, Docker, Kubernetes and the unlimited number of JS frameworks, the infinite patterns and styles of architectures, many developers lost the frequency of being up-to-date with the upstream ü§™","title":"Playing with Java from 9 to 13"},{"content":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), from the 2nd to 4th January 2020, to animate a 20 Hours workshop, about Microservices in Java and how to deploy them as Docker Containers to Kubernetes. The event is organized by Mrs Rim Drira.\nThe workshop content:\n Part 1: The Monolithics Era Part 2: Coding the monolith Part 3: Microservices Era Part 4: Applying DDD to the code Part 5: Meeting \u0026amp; Implementing the ¬µservices concerns and patterns Part 6: Building the standalone ¬µservices Part 7: Packaging ¬µservices in containers Part 8: Falling in ü•∞ with container orchestrator: KUBERNETES üòç Part 9: Applying the Kubernetes Style Part 10: Meeting Azure ‚òÅÔ∏è Part X: What ‚Äôs next ?  Like every edition üòÅ I would like to thank Mrs Rim Drira¬†for the invitation and for all the great organization of the previous events.\n","permalink":"https://blog.nebrass.fr/playing-with-java-microservices-on-kubernetes-ensi-2020/","summary":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), from the 2nd to 4th January 2020, to animate a 20 Hours workshop, about Microservices in Java and how to deploy them as Docker Containers to Kubernetes. The event is organized by Mrs Rim Drira.\nThe workshop content:\n Part 1: The Monolithics Era Part 2: Coding the monolith Part 3: Microservices Era Part 4: Applying DDD to the code Part 5: Meeting \u0026amp; Implementing the ¬µservices concerns and patterns Part 6: Building the standalone ¬µservices Part 7: Packaging ¬µservices in containers Part 8: Falling in ü•∞ with container orchestrator: KUBERNETES üòç Part 9: Applying the Kubernetes Style Part 10: Meeting Azure ‚òÅÔ∏è Part X: What ‚Äôs next ?","title":"Playing with Java Microservices on Kubernetes - ENSI 2020"},{"content":"The Microservices Architecture World, we can meet many concepts and patterns, like the Centralized Configuration, Circuit Breaker, Service Registry and Discovery, etc.. Two of these patterns are the CQRS and the Event Sourcing patterns, coming from the Domain Driven Design planet üåè In the most of the use-cases, these two patterns are sold together üòÅ in this new tutorial, we will discover what does each one ? why they are usually used together ? and for sure we will implement these two patterns in Java ‚òïÔ∏è obviously ü§ì\nLet\u0026rsquo;s start with some definitions and literature üòá\nWhat is the cqrs pattern ? CQRS stands for Command Query Responsibility Segregation is a design pattern that aims to separate the Read and Write operations. In the CQRS distinguishes the operations as:\n Queries: a Read only operation - no state is updated after executing queries Commands: a Writing operation - state is updated after executing commands  The schema describes the CQRS pattern:\nCQRS pattern\n A Query is a Read operation, that does not update any the state of the application. A Query is handled by the Reading Components that will interact with the DB, parses the DB response, creates a Data Transfert Object that will be returned to the User.\nA Command is a Business Action that the Application\u0026rsquo;s user want to do, for example: RegisterStudent, MakeDeposit, etc..\nEvery Command has a Handling Layer¬†that knows how to apply the Business Action. Generally, commands are inserted in a Queue to be processed asynchronously, so technically speaking, a Command Handler is invoked by a Queue Listener..\nWhat is the Event Sourcing Pattern ? Event Sourcing aims to persist the state of a business entity (BankAccount for example) as a sequence of state-changing events. Every action performed on a business entity should be persisted. The application reconstructs an entity‚Äôs current state by replaying the events.\nFor example, to reconstruct a given BankAccount current state, we need to replay all the events occurred on this business entity. It means we do not store the state of the BankAccount.\nApplications persist events in a database of events called event store. The store has an API for adding and retrieving an entity‚Äôs events. The event store also behaves like a message broker. It provides an API that enables services to subscribe to events. When a service saves an event in the event store, it is delivered to all interested subscribers.\nSome entities, such as a BankAccount, can have a large number of events. In order to optimize loading, an application can periodically save a snapshot of an entity‚Äôs current state. To reconstruct the current state, the application finds the most recent snapshot and the events that have occurred since that snapshot. As a result, there are fewer events to replay.\nEvent Sourcing (very) simplified diagram\n Why we are always coupling these patterns ? CQRS separates the responsibilities, typically into different components. The first component covers CUD operations (without the Reading), while a second component will ensure the Read operation.\nReads and writes from different places can create a timing issue. Most database theory focuses on consistency. It should be possible to keep a log of every data change. That way, at any point in time, the values that the queries display are logically correct. Here comes the Event Sourcing, which will ensure consistency. Event Sourcing stores a record of every action in a dedicated database. From there, an event handler reads these changes in order, applies them appropriately and marks them as complete once the transaction is complete. This event handler does not need to be complex \u0026ndash; it can be as simple as an API endpoint. Once the event handler creates an event record, a central service messaging system can push notifications every time it discovers about a new event.\nCoupling CQRS and Event Sourcing Diagram\n CQRS and Event Sourcing patterns are frequently used together. Coupling these two patterns means that each event on the Writing part of our application. Obviously ü§ì the Reading part is made by playing the events.\nLet\u0026rsquo;s implement the CQRS \u0026amp; Event Sourcing in a typical Java application We will create a small Spring Boot application on which we will implement CQRS and Event Sourcing patterns using Axon.\nWhat is Axon ? ü§î\nBased on the official documentation:\n Axon provides a unified, productive way of developing Java applications that can evolve without significant refactoring from a monolith to Event-Driven microservices. Axon includes both a programming model as well as specialized infrastructure to provide enterprise ready operational support for the programming model - especially for scaling and distributing mission critical business applications. The programming model is provided by the popular Axon Framework while Axon Server is the infrastructure part of Axon, all open sourced.\n Axon Framework and Server - Official documentation\n Let\u0026rsquo;s dig into the coding part; we will start by generating the Spring Boot application using the Spring Initializr with these dependencies:\n Web H2 Actuator H2 Database Lombok  Generating the Spring Boot application\n After generating the project. We will add these dependencies to the pom.xml:\n  SpringFox Swagger2 and Swagger UI:\n1 2 3 4 5 6 7 8 9 10  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;     Axon Spring Boot Starter:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.axonframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;axon-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;     Axon Test module ü§ì We will be testing our code ü§¨\n1 2 3 4 5 6  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.axonframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;axon-test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;     Our sample application will be a Bank Account manager. Our application will have these features:\n Create a new account for a given Owner with a given Initial Balance Credit an amount on a given account Debit an amount from a given account Read information about a given account  A BankAccount will look like:\n1 2 3 4 5 6 7 8 9 10  @Data // Lombok @NoArgsConstructor // Lombok @AllArgsConstructor // Lombok @Entity public class BankAccount { @Id private UUID id; private String owner; private BigDecimal balance; }   commands and queries Now, we need to list the Reading and Writing actions related to the application features:\n   Feature Command Query     Create a new account Yes No   Credit an amount from account Yes No   Debit an amount from account Yes No   Get Account information No Yes    Based on this table, our commands will be:\n CreateAccountCommand CreditMoneyCommand DebitMoneyCommand  For the queries, we will have only one:\n FindAccountQuery  - What\u0026rsquo;s next ? ü§î\n- Did you forgot that the CQRS and Event Sourcing are two patterns that belong to the DDD paradigm? ü§î As it\u0026rsquo;s a Domain Driven, we need to start designing our Domain üòÅ\nWe will start by implementing the Command model for our CQRS segments, using Aggregates üò±\nAggregate I have found two definitions of Aggregates in the Axon Documentation:\n  An Aggregate is a regular object, which contains state and methods to alter that state. An Aggregate is an entity or group of entities that is always kept in a consistent state (within a single ACID transaction). The Aggregate Root is the entity within the aggregate that is responsible for maintaining this consistent state.   ‚ö†Ô∏è¬†I have a small objection on these definition: I dont like to have the word entity - because like everyone, I will directly think on JPA Entity - but keep in mind, Aggregate is a pattern in Domain-Driven Design, and in this level (the design) we dont talk about technical details. üëâ This is why I would like to say business entity instead entity.\nThe updated definition that I like üòÅ\n  An Aggregate is a regular object, which contains state and methods to alter that state. An Aggregate is a business entity or group of business entities that is always kept in a consistent state (within a single ACID transaction). The Aggregate Root is the business entity within the aggregate that is responsible for maintaining this consistent state.   For example: an aggregate can be an e-Commerce Order with the related OrderItems and Customer information. Here, the Order class is the Aggregate Root:\nOrder Aggregate example\n In our application, our Aggregate is the BankAccountAggregate will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13  @AllArgsConstructor // Lombok @NoArgsConstructor // Lombok @Getter // Lombok @Aggregate // 1 public class BankAccountAggregate { @AggregateIdentifier // 2  private UUID id; private BigDecimal balance; private String owner; ... }    The @Aggregate annotation informs Axon\u0026rsquo;s auto configurer for Spring that this class is an Aggregate instance. The @AggregateIdentifier identifies the field as the identifier of the Aggregate.  ‚ö†Ô∏è‚ö†Ô∏è For sure that you are saying now that the BookAccountAggregate and the BookAccount JPA Entity looks like the same structure. Why are we duplicating the code? Why don\u0026rsquo;t we use the BookAccountAggregate class as the JPA Entity class? The answer is that the BookAccountAggregate will contain more Axon boilerplate code which cannot fit to a JPA Entity class, which is used only to represent data stored in a DB ü§ì\nLet\u0026rsquo;s continue to code our BookAccountAggregate class.\nNow we will code the constructor. We already said that we have a Command that will create a new account: CreateAccountCommand. Here will come the first glue between the Commands and the Aggregate: the CreateAccountCommand will be passed to the Aggregate constructor:\n@CommandHandler public BankAccountAggregate(CreateAccountCommand command) { }\nThe @CommandHandler will mark this method (constructor) as a Handler of the CreateAccountCommand. The command needs to bring the data needed by to construct the BankAccount instance. Think of it as a Data Transfert Object used to wrap data received and sent via REST APIs. Obviously a CreateAccountCommand will have the same content like the BankAccount JPA Entity and the BookAccountAggregate. It will look like:\n1 2 3 4 5 6 7 8 9 10  @Data // Lombok @NoArgsConstructor // Lombok @AllArgsConstructor // Lombok public class CreateAccountCommand { @TargetAggregateIdentifier private UUID accountId; private BigDecimal initialBalance; private String owner; }   The @TargetAggregateIdentifier will identify the field as the identifier of the targeted aggregate.\nWe said before that in the CQRS and Event Sourcing based applications, for every Command made, we dispatch an Event.\nFor example; for the CreateAccountCommand we need to create an AccountCreatedEvent that will be used to say that a Command has been received.\n You can notice that the Command is formed by an Action + Command suffix while the Event is PastAction + Event suffix\n Guess what üòÅ the AccountCreatedEvent will look like:\n1 2 3 4 5 6 7  @Data // Lombok public class AccountCreatedEvent { private final UUID id; private final BigDecimal initialBalance; private final String owner; }   Now, the CommandHandler will look like:\n1 2 3 4 5 6 7 8 9 10 11  @CommandHandler public BankAccountAggregate(CreateAccountCommand command) { AggregateLifecycle.apply( new AccountCreatedEvent( command.getAccountId(), command.getInitialBalance(), command.getOwner() ) ); }   The AggregateLifecycle component is used to notify the Aggregate that a new BankAccount was created by publishing the AccountCreatedEvent.\nGood ! The same way, if we dispatched a Command, we defined its CommandHandler. Now, as we dispatched an Event, we need to define the EventHandler:\n1 2 3 4 5 6  @EventSourcingHandler public void on(AccountCreatedEvent event) { this.id = event.getId(); this.owner = event.getOwner(); this.balance = event.getInitialBalance(); }   The @EventSourcingHandler will define the annotated method as a handler for Events generated by that Aggregate.\nNow, we will define the two remaining Commands:\n  The CreditMoneyCommand:\n1 2 3 4 5 6 7 8 9  @Data @NoArgsConstructor @AllArgsConstructor public class CreditMoneyCommand { @TargetAggregateIdentifier private UUID accountId; private BigDecimal creditAmount; }     The DebitMoneyCommand:\n1 2 3 4 5 6 7 8 9  @Data @NoArgsConstructor @AllArgsConstructor public class DebitMoneyCommand { @TargetAggregateIdentifier private UUID accountId; private BigDecimal debitAmount; }     The remaining two Events:\n  The MoneyCreditedEvent:\n1 2 3 4 5 6  @Value public class MoneyCreditedEvent { private final UUID id; private final BigDecimal creditAmount; }     The MoneyDebitedEvent:\n1 2 3 4 5 6  @Value public class MoneyDebitedEvent { private final UUID id; private final BigDecimal debitAmount; }     The BankAccountAggregate will finally look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  @AllArgsConstructor @NoArgsConstructor @Getter @Aggregate public class BankAccountAggregate { @AggregateIdentifier private UUID id; private BigDecimal balance; private String owner; @CommandHandler public BankAccountAggregate(CreateAccountCommand command) { AggregateLifecycle.apply( new AccountCreatedEvent( command.getAccountId(), command.getInitialBalance(), command.getOwner() ) ); } @EventSourcingHandler public void on(AccountCreatedEvent event) { this.id = event.getId(); this.owner = event.getOwner(); this.balance = event.getInitialBalance(); } @CommandHandler public void handle(CreditMoneyCommand command) { AggregateLifecycle.apply( new MoneyCreditedEvent( command.getAccountId(), command.getCreditAmount() ) ); } @EventSourcingHandler public void on(MoneyCreditedEvent event) { this.balance = this.balance.add(event.getCreditAmount()); } @CommandHandler public void handle(DebitMoneyCommand command) { AggregateLifecycle.apply( new MoneyDebitedEvent( command.getAccountId(), command.getDebitAmount() ) ); } @EventSourcingHandler public void on(MoneyDebitedEvent event) throws InsufficientBalanceException { if (this.balance.compareTo(event.getDebitAmount()) \u0026lt; 0) { throw new InsufficientBalanceException(event.getId(), event.getDebitAmount()); } this.balance = this.balance.subtract(event.getDebitAmount()); } }   I defined an InsufficientBalanceException for handling an error while debiting money:\n1 2 3 4 5 6  public class InsufficientBalanceException extends Throwable { public InsufficientBalanceException(UUID accountId, BigDecimal debitAmount) { super(\u0026#34;Insufficient Balance: Cannot debit \u0026#34; + debitAmount + \u0026#34; from account number [\u0026#34; + accountId.toString() + \u0026#34;]\u0026#34;); } }   At this stage, we created the aggregate that receives and handles the Commands and for every Command will dispatch a Query.\nGood ! But no data is inserted in the DB, no boundary is available to emit instructions ü•∫\nNow we will create the JPA Repository for our BankAccount JPA Entity:\n1 2 3  @Repository public interface BankAccountRepository extends JpaRepository\u0026lt;BankAccount, UUID\u0026gt; { }   Now, we can use the BankAccountRepository to made CRUD operations on BankAccount in the DB. Ok, but from where ?\nYou can think in the BankAccountAggregate, but it will not be suitable, as it will be doing many tasks which will cause us to lose the Single Responsibility principle.\nThe common practice is to create a dedicated class that will match the DB operations for every received event. I saw that the Axon team is calling it Projector class.\nWe will call our Projector class BankAccountProjection that looks like:\n1 2 3 4 5 6 7 8 9  @Slf4j @RequiredArgsConstructor @Component public class BankAccountProjection { private final BankAccountRepository repository; ... }   The BankAccountProjection is a Spring Component on which we injected our BankAccountRepository.\nGood ! Now, we need to define the Handler for every emitted Event. For example, the EventHandler for AccountCreatedEvent will look like:\n1 2 3 4 5 6 7 8 9 10  @EventHandler public void on(AccountCreatedEvent event) { log.debug(\u0026#34;Handling a Bank Account creation command {}\u0026#34;, event.getId()); BankAccount bankAccount = new BankAccount( event.getId(), event.getOwner(), event.getInitialBalance() ); this.repository.save(bankAccount); }   Opppaaa ü•≥ ! The Event is serving as a DTO wrapping the needed values to create a BankAccount üòÅ\nThe same thing for the MoneyCreditedEvent and MoneyDebitedEvent:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ... @EventHandler public void on(MoneyCreditedEvent event) throws AccountNotFoundException { log.debug(\u0026#34;Handling an Account Credit command {}\u0026#34;, event.getId()); Optional\u0026lt;BankAccount\u0026gt; optionalBankAccount = this.repository.findById(event.getId()); if (optionalBankAccount.isPresent()) { BankAccount bankAccount = optionalBankAccount.get(); bankAccount.setBalance(bankAccount.getBalance().add(event.getCreditAmount())); this.repository.save(bankAccount); } else { throw new AccountNotFoundException(event.getId()); } } @EventHandler public void on(MoneyDebitedEvent event) throws AccountNotFoundException { log.debug(\u0026#34;Handling an Account Debit command {}\u0026#34;, event.getId()); Optional\u0026lt;BankAccount\u0026gt; optionalBankAccount = this.repository.findById(event.getId()); if (optionalBankAccount.isPresent()) { BankAccount bankAccount = optionalBankAccount.get(); bankAccount.setBalance(bankAccount.getBalance().subtract(event.getDebitAmount())); this.repository.save(bankAccount); } else { throw new AccountNotFoundException(event.getId()); } } ...   Here, I defined an AccountNotFoundException thrown¬†when no account is found:\n1 2 3 4 5  public class AccountNotFoundException extends Throwable { public AccountNotFoundException(UUID id) { super(\u0026#34;Cannot found account number [\u0026#34; + id + \u0026#34;]\u0026#34;); } }   Yoopi ! ü§© Now, we will need the REST API and the Spring Service that will be receiving the HTTP Requests and dispatching the Commands to the Axon Engine.\nLet\u0026rsquo;s start by the REST API for the Commands:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  @RestController @RequestMapping(value = \u0026#34;/accounts\u0026#34;) @Api(value = \u0026#34;Bank Account Commands\u0026#34;, description = \u0026#34;Bank Account Commands API\u0026#34;) @AllArgsConstructor public class AccountCommandController { private final AccountCommandService accountCommandService; @PostMapping @ResponseStatus(value = CREATED) public CompletableFuture\u0026lt;BankAccount\u0026gt; createAccount(@RequestBody AccountCreationDTO creationDTO) { return this.accountCommandService.createAccount(creationDTO); } @PutMapping(value = \u0026#34;/credit/{accountId}\u0026#34;) public CompletableFuture\u0026lt;String\u0026gt; creditMoneyToAccount(@PathVariable(value = \u0026#34;accountId\u0026#34;) String accountId, @RequestBody MoneyAmountDTO moneyCreditDTO) { return this.accountCommandService.creditMoneyToAccount(accountId, moneyCreditDTO); } @PutMapping(value = \u0026#34;/debit/{accountId}\u0026#34;) public CompletableFuture\u0026lt;String\u0026gt; debitMoneyFromAccount(@PathVariable(value = \u0026#34;accountId\u0026#34;) String accountId, @RequestBody MoneyAmountDTO moneyDebitDTO) { return this.accountCommandService.debitMoneyFromAccount(accountId, moneyDebitDTO); } }   The AccountCreationDTO:\n1 2 3 4 5  @Value public class AccountCreationDTO { private final BigDecimal initialBalance; private final String owner; }   The MoneyAmountDTO:\n1 2 3 4 5  @Data @NoArgsConstructor public class MoneyAmountDTO { private BigDecimal amount; }   Now, we will create the Spring Service that will be dispatching the Commands to the Axon Engine. To do this, the framework has a very useful component called CommandGateway, which is a very convenient interface towards the command dispatching mechanism üòÅ\nOur AccountCommandService will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  @Service @AllArgsConstructor public class AccountCommandService { private final CommandGateway commandGateway; public CompletableFuture\u0026lt;BankAccount\u0026gt; createAccount(AccountCreationDTO creationDTO) { return this.commandGateway.send(new CreateAccountCommand( UUID.randomUUID(), creationDTO.getInitialBalance(), creationDTO.getOwner() )); } public CompletableFuture\u0026lt;String\u0026gt; creditMoneyToAccount(String accountId, MoneyAmountDTO moneyCreditDTO) { return this.commandGateway.send(new CreditMoneyCommand( formatUuid(accountId), moneyCreditDTO.getAmount() )); } public CompletableFuture\u0026lt;String\u0026gt; debitMoneyFromAccount(String accountId, MoneyAmountDTO moneyDebitDTO) { return this.commandGateway.send(new DebitMoneyCommand( formatUuid(accountId), moneyDebitDTO.getAmount() )); } }   Cool ! ü•≥ We need to define the Query part. Let\u0026rsquo;s start by defining FindAccountQuery:\n1 2 3 4 5 6  @Data @NoArgsConstructor @AllArgsConstructor public class FindBankAccountQuery { private UUID accountId; }   In the BankAccountProjection, we need to add a QueryHandler method:\n1 2 3 4 5  @QueryHandler public BankAccount handle(FindBankAccountQuery query) { log.debug(\u0026#34;Handling FindBankAccountQuery query: {}\u0026#34;, query); return this.repository.findById(query.getAccountId()).orElse(null); }   We need to create the Query REST API and Service. The AccountQueryController looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  @RestController @RequestMapping(value = \u0026#34;/accounts\u0026#34;) @Api(value = \u0026#34;Bank Account Queries\u0026#34;, description = \u0026#34;Bank Account Query Events API\u0026#34;) @AllArgsConstructor public class AccountQueryController { private final AccountQueryService accountQueryService; @GetMapping(\u0026#34;/{accountId}\u0026#34;) public CompletableFuture\u0026lt;BankAccount\u0026gt; findById(@PathVariable(\u0026#34;accountId\u0026#34;) String accountId) { return this.accountQueryService.findById(accountId); } @GetMapping(\u0026#34;/{accountId}/events\u0026#34;) public List\u0026lt;Object\u0026gt; listEventsForAccount(@PathVariable(value = \u0026#34;accountId\u0026#34;) String accountId) { return this.accountQueryService.listEventsForAccount(accountId); } }   The AccountQueryService looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  @Service @AllArgsConstructor public class AccountQueryService { private final QueryGateway queryGateway; private final EventStore eventStore; public CompletableFuture\u0026lt;BankAccount\u0026gt; findById(String accountId) { return this.queryGateway.query( new FindBankAccountQuery(formatUuid(accountId)), ResponseTypes.instanceOf(BankAccount.class) ); } public List\u0026lt;Object\u0026gt; listEventsForAccount(String accountId) { return this.eventStore .readEvents(formatUuid(accountId).toString()) .asStream() .map(Message::getPayload) .collect(Collectors.toList()); } }   **STOP!!**üëÆüèª‚Äç‚ôÇÔ∏è‚õîÔ∏è What is this strange EventStore ?? EventStore provides access to both the global event stream comprised of all domain and application events. We will be using it to list all the events about a given aggregate.\nWe need to add these properties to the application.properties:\n1 2 3 4 5  # H2 settings spring.h2.console.enabled=true spring.h2.console.path=/h2-console # Axon axon.serializer.general=jackson   We will be using Swagger to have a small UI to test our REST APIs. The SwaggerConfiguration file looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  @Configuration @EnableSwagger2 public class SwaggerConfiguration { @Bean public Docket apiDocket() { return new Docket(DocumentationType.SWAGGER_2) .select() .apis(RequestHandlerSelectors .basePackage(\u0026#34;com.targa.labs.dev.cqrses\u0026#34;)) .paths(PathSelectors.any()) .build() .apiInfo(getApiInfo()); } private ApiInfo getApiInfo() { return new ApiInfo( \u0026#34;CQRS \u0026amp; ES Sample App based on Spring Boot and Axon\u0026#34;, \u0026#34;App to demonstrate CQRS \u0026amp; ES based on Spring Boot and Axon\u0026#34;, \u0026#34;0.0.1-SNAPSHOT\u0026#34;, \u0026#34;Terms of Service\u0026#34;, new Contact(\u0026#34;Nebrass Lamouchi\u0026#34;, \u0026#34;https://blog.nebrass.fr\u0026#34;, \u0026#34;lnibrass@gmail.com\u0026#34;), \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, Collections.emptyList()); } }   Now, let\u0026rsquo;s start our application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  $ mvn spring-boot:run [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.targa.labs.dev:cqrs-es \u0026gt;--------------------- [INFO] Building cqrs-es 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- .... ...AxonServerConnectionManager: Connecting using unencrypted connection... ...AxonServerConnectionManager: Requesting connection details from localhost:8124 ...AxonServerConnectionManager: Connecting to AxonServer node [localhost]:[8124] failed: UNAVAILABLE: io exception ********************************************** * * * !!! UNABLE TO CONNECT TO AXON SERVER !!! * * * * Are you sure it\u0026#39;s running? * * If you haven\u0026#39;t got Axon Server yet, visit * * https://axoniq.io/download * * * ********************************************** To suppress this message, you can - explicitly configure an AxonServer location, - start with -Daxon.axonserver.suppressDownloadMessage=true ...AxonServerQueryBus: Error subscribing query handler org.axonframework.axonserver.connector.AxonServerException: No connection to AxonServer available ... ...DocumentationPluginsBootstrapper: Context refreshed ...TrackingEventProcessor: Fetch Segments for Processor \u0026#39;com.targa.labs.dev.cqrses.projection\u0026#39; failed: No connection to AxonServer available. Preparing for retry in 1s ...   Wooh üò±üò±üò± These errors are due to a missing Axon Server. We can start a new instance it easily using a Docker Container:\n1  $ docker run -d --name axon-server -p 8024:8024 -p 8124:8124 axoniq/axonserver   Now, the Axon Server UI will be reachable on http://localhost:8024/\nAxon Server UI\n If you click on the Overview section:\nAxon Server UI - Overview\n Next, check the Commands¬†section:\nAxon Server UI - Commands\n Next, check the Queries¬†section:\nAxon Server UI - Queries\n Nothing wrong üòÜ Don\u0026rsquo;t be scared, as no application is communicating with the Axon Server, everything is empty.\nLet\u0026rsquo;s start now the application again:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  $ mvn spring-boot:run [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.targa.labs.dev:cqrs-es \u0026gt;--------------------- [INFO] Building cqrs-es 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\  \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.2.RELEASE) ... ..AxonServerConnectionManager : Connecting using unencrypted connection... ..AxonServerConnectionManager : Requesting connection details from localhost:8124 ..AxonServerConnectionManager : Reusing existing channel ..AxonServerConnectionManager : Re-subscribing commands and queries ..AxonServerCommandBus : Creating new command stream subscriber ..AxonServerQueryBus : Creating new query stream subscriber ..DocumentationPluginsBootstrapper : Context refreshed ..DocumentationPluginsBootstrapper : Found 1 custom documentation plugin(s) ..ApiListingReferenceScanner : Scanning for api listing references ..TrackingEventProcessor : Worker assigned to segment Segment[0/0] for processing ..TrackingEventProcessor : Using current Thread for last segment worker: TrackingSegmentWorker{processor=com.targa.labs.dev.cqrses.projection, segment=Segment[0/0]} ..TrackingEventProcessor : Fetched token: null for segment: Segment[0/0] ..AxonServerEventStore : open stream: 0 ..TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; ..CqrsEsApplication : Started CqrsEsApplication in 5.262 seconds (JVM running for 5.532)   Cool ! Let\u0026rsquo;s access now the Swagger UI on http://localhost:8080/swagger-ui.html:\nSwagger UI\n As you see, there is already two REST Controllers one for the Commands and one for the Queries. Let\u0026rsquo;s test the createAccount REST API:\nCreate Account - Swagger UI\n Here we created a new BankAccount that has:\n id: 962f7577-19b6-439b-9368-9742b44d2a20 initialBalance: 5000 owner: Nebrass Lamouchi  Next, I will test some creditMoneyToAccount¬†operation twice the first with an amount of 300 and the second with 480:\nCredit Account - Swagger UI\n Let\u0026rsquo;s debit the amount of 2000 from the sample account using the debitMoneyFromAccount operation:\nDebit Account ‚Äì Swagger UI\n Now, we need to check how much we have in our account, normally the remaining balance will be 5000 + 300 + 480-2000 = 3780.\nLet\u0026rsquo;s check the account using the findById operation on the AccountQueryController:\nFind Account by ID ‚Äì Swagger UI\n As expected ! the balance is 3780 ü§©\nWe can verify the Events list occurred on our BankAccount using the listEventsForAccount operation:\nAccount Events list ‚Äì Swagger UI\n Great ! Everything is working like a charm ! ü•≥\nAfter we executed our application and after we did some operations, let\u0026rsquo;s visit again the Axon Server UI:\nAxon Server UI - Overview after execution\n As you see, our application instance is spotted on the Axon Dashboard. Let\u0026rsquo;s move to the Commands section:\nAxon Server UI - Commands after execution\n You already see that the CreateAccountCommand was fired once, the CreditMoneyCommand twice (300 \u0026amp; 480) and the DebitMoneyCommand once (2000).\nNext, move to the Queries section:\nAxon Server UI - Queries after execution\n You can easily see that the FindBankAccountQuery was executed once.\nYou can see all of the Events in the Search section. Click directly on Search button to grab all the Events:\nAxon Server UI - Search section\n If you want to test our Axon code programmatically, we will start by adding the Axon Test module to the pom.xml:\n1 2 3 4 5 6  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.axonframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;axon-test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;   Next, we will create a BankAccountTest class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  public class BankAccountTest { private static final String customerName = \u0026#34;Nebrass\u0026#34;; private FixtureConfiguration\u0026lt;BankAccountAggregate\u0026gt; fixture; private UUID id; @BeforeEach public void setUp() { fixture = new AggregateTestFixture\u0026lt;\u0026gt;(BankAccountAggregate.class); id = UUID.randomUUID(); } @Test public void should_dispatch_accountcreated_event_when_createaccount_command() { fixture.givenNoPriorActivity() .when(new CreateAccountCommand( id, BigDecimal.valueOf(1000), customerName) ) .expectEvents(new AccountCreatedEvent( id, BigDecimal.valueOf(1000), customerName) ); } @Test public void should_dispatch_moneycredited_event_when_balance_is_lower_than_debit_amount() { fixture.given(new AccountCreatedEvent( id, BigDecimal.valueOf(1000), customerName)) .when(new CreditMoneyCommand( id, BigDecimal.valueOf(100)) ) .expectEvents(new MoneyCreditedEvent( id, BigDecimal.valueOf(100)) ); } @Test public void should_dispatch_moneydebited_event_when_balance_is_upper_than_debit_amount() { fixture.given(new AccountCreatedEvent( id, BigDecimal.valueOf(1000), customerName)) .when(new DebitMoneyCommand( id, BigDecimal.valueOf(100))) .expectEvents(new MoneyDebitedEvent( id, BigDecimal.valueOf(100))); } @Test public void should_not_dispatch_event_when_balance_is_lower_than_debit_amount() { fixture.given(new AccountCreatedEvent( id, BigDecimal.valueOf(1000), customerName)) .when(new DebitMoneyCommand( id, BigDecimal.valueOf(5000))) .expectNoEvents(); } }   In our BankAccountTest class, we are using FixtureConfiguration class which we will use to define a test scenario in terms of events and commands:\n Given certain events in the past When executing this command Expect these events to be published  Our test are designed like:\n In the first test, we are testing an Account Creation operation:  we are supposing that we don\u0026rsquo;t have any account created when we will dispatch a CreateAccountCommand we are expecting to get the a AccountCreatedEvent with the same values   In the second test, we are testing the Credit Money operation:  we are supposing that we have an account when we will dispatch a CreditMoneyCommand with an amount of 100 we are expecting to get the a MoneyCreditedEvent with an amount of 100   In the third test, we are testing the Debit Money operation:  we are supposing that we have an account when we will dispatch a Debit****MoneyCommand with an amount of 100 we are expecting to get the a MoneyDebitedEvent with an amount of 100   In the third test, we are testing the impossibility to execute Debit Money operation when the requested amount is higher than the account balance:  we are supposing that we have an account when we will dispatch a Debit****MoneyCommand with an amount of 5000 we are expecting to have NO events that occurred    The end !\n Conclusion In this tutorial, I have implemented a CQRS and Event Sourcing based Spring Boot application using the Axon Platform. This is a blog post I wanted to write from some time ago, but I couldn\u0026rsquo;t find an opportunity until now. I tried to present the key concepts of the CQRS and Event Sourcing patterns and the main features offered by Axon Platform. I tried to make the demo using a very common example in the DDD world, the Bank Account.\nI really encourage you to dig into the DDD paradigms especially that we have many great platforms like Axon which make our life easier.\nThe sample code of this tutorial can be found on Github.\n","permalink":"https://blog.nebrass.fr/playing-with-cqrs-and-event-sourcing-in-spring-boot-and-axon/","summary":"The Microservices Architecture World, we can meet many concepts and patterns, like the Centralized Configuration, Circuit Breaker, Service Registry and Discovery, etc.. Two of these patterns are the CQRS and the Event Sourcing patterns, coming from the Domain Driven Design planet üåè In the most of the use-cases, these two patterns are sold together üòÅ in this new tutorial, we will discover what does each one ? why they are usually used together ?","title":"Playing with CQRS and Event Sourcing in Spring Boot and Axon"},{"content":"Nowadays, the most of the Java microservices and even many Java application are packaged and deployed as Docker containers. Everyone is enjoying (I hope üòÜ) the Docker experience, compared to the traditional VMs.¬†But, the Docker containerization will not come alone.. nothing is autonomous üòÅ So, there are many concerns to take into consideration while containerizing Java applications, like playing with the JVM.\nThe Docker racing was accelerated by the presence of Kubernetes in the market, which is coming with an amazing set of great features.\nOne of the great features available for handling the containers is having the option to control the resources allocated for a Pod üòÜ for example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  apiVersion:v1kind:Podmetadata:name:frontendspec:containers:- name:dbimage:mysqlenv:- name:MYSQL_ROOT_PASSWORDvalue:\u0026#34;password\u0026#34;resources:requests:memory:\u0026#34;64Mi\u0026#34;cpu:\u0026#34;250m\u0026#34;limits:memory:\u0026#34;128Mi\u0026#34;cpu:\u0026#34;500m\u0026#34;- name:wpimage:wordpressresources:requests:memory:\u0026#34;64Mi\u0026#34;cpu:\u0026#34;250m\u0026#34;limits:memory:\u0026#34;128Mi\u0026#34;cpu:\u0026#34;500m\u0026#34;  This yaml resource descriptor will define CPU and Memory limits for the two containers running in this frontend Pod. These limits was not recognized by the JVM under Java versions until 9 üò≠üò≠With the appearance of the Java 10, the JVM can now be aware of these limits. ü•≥\nThis great support was even ported to the JDK 8 update 191.\nwhat are these great Improvements for Docker Containers ? We can quickly start testing the awareness of the JVM under Java 10 of the Container limits that we can put:\nLet\u0026rsquo;s create a Docker container without any limits. In the created container, we will be running the JShell to test what information do we have in the Runtime?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ docker container run -it openjdk:10-jdk Dec 02, 2019 8:14:54 AM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. | Welcome to JShell -- Version 10.0.2 | For an introduction type: /help intro jshell\u0026gt; Runtime.getRuntime().totalMemory() $1 ==\u0026gt; 33554432 jshell\u0026gt; Runtime.getRuntime().maxMemory() $2 ==\u0026gt; 524288000 jshell\u0026gt; Runtime.getRuntime().availableProcessors() $3 ==\u0026gt; 8   Before digging into the returned values, what are these methods ?\nBased on the Javadoc of the Runtime class:\n totalMemory() Gives the total amount of memory in the Java virtual machine. The value returned by this method may vary over time, depending on the host environment. Note that the amount of memory required to hold an object of any given type may be implementation-dependent. Returns: the total amount of memory currently available for current and future objects, measured in bytes. maxMemory() Gives the maximum amount of memory that the Java virtual machine will attempt to use. If there is no inherent limit then the value Long.MAX_VALUE will be returned. Returns: the maximum amount of memory that the virtual machine will attempt to use, measured in bytes  Results:\n Total Memory: 33554432 bytes = 32 MB üëà total allocated space¬†reserved for¬†the JVM Max Memory: 524288000 bytes = 500 MB üëà the maximum amount of memory that the JVM can use Available CPUs: 8  Let\u0026rsquo;s see my Docker Engine configuration on my laptop:\nDocker Engine configuration\n You can see that there are the 8 CPUs listed by the JShell, but the Memory allocated for the Docker Engine is 2GB and not 500MB ü§î Why it\u0026rsquo;s only 25% of the available memory? üò§\nLet\u0026rsquo;s do the same, with a Docker container with Memory limit of 800M and a CPU limit of 2:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ docker container run -it --memory 800M --cpus 2 openjdk:10-jdk Dec 01, 2019 10:10:29 PM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. | Welcome to JShell -- Version 10.0.2 | For an introduction type: /help intro jshell\u0026gt; Runtime.getRuntime().totalMemory() $1 ==\u0026gt; 14221312 jshell\u0026gt; Runtime.getRuntime().maxMemory() $2 ==\u0026gt; 202768384 jshell\u0026gt; Runtime.getRuntime().availableProcessors() $3 ==\u0026gt; 2   Results:\n Total Memory: 13.5625 MB Max Memory: 193.375 MB Available CPUs: 2  The Max Memory is approximately¬†25% of the 800MB memory that we allocated to our container ü§î\nWe can run an other command to check the maximum memory allocated for the JVM:\n1 2 3 4 5 6  $ docker container run -it --memory 800M --cpus 2 --entrypoint bash openjdk:10 root@ae152619006a:/# java -XX:+PrintFlagsFinal -version | grep MaxHeapSize size_t MaxHeapSize = 209715200 {product} {ergonomic} openjdk version \u0026#34;10.0.2\u0026#34; 2018-07-17 OpenJDK Runtime Environment (build 10.0.2+13-Debian-2) OpenJDK 64-Bit Server VM (build 10.0.2+13-Debian-2, mixed mode)   You can see here, that the MaxHeapSize is equal to 209715200 Bytes = 200 MB üòÅ more precisions ü•≥ But again, why it\u0026rsquo;s only 25% of the available memory? üò§\nThis is a default Java 10+ behavior, the JVM Heap will get 25% of the container‚Äôs memory. This is why we got only 25% of the allocated memory. üò≠\nDon\u0026rsquo;t worry ! There is a way to override the default 25% üßê we can pass a -XX:MaxRAMPercentage parameter with the desired value. Let\u0026rsquo;s test it:\n1 2 3 4 5 6 7  $ docker container run -it --memory 800M --cpus 2 --entrypoint bash openjdk:10 root@6327c67b8d35:/# java -XX:MaxRAMPercentage=50 \\  -XX:+PrintFlagsFinal -version | grep MaxHeapSize size_t MaxHeapSize = 419430400 {product} {ergonomic} openjdk version \u0026#34;10.0.2\u0026#34; 2018-07-17 OpenJDK Runtime Environment (build 10.0.2+13-Debian-2) OpenJDK 64-Bit Server VM (build 10.0.2+13-Debian-2, mixed mode)   When we defined the MaxRAMPercentage to 50%, we got 419430400 Bytes = 400MB, which is 50% of the 800M limit memory defined for the container.\n1 2 3 4 5 6 7  $ docker container run -it --memory 800M --cpus 2 --entrypoint bash openjdk:10 root@6327c67b8d35:/# java -XX:MaxRAMPercentage=100 \\  -XX:+PrintFlagsFinal -version | grep MaxHeapSize size_t MaxHeapSize = 838860800 {product} {ergonomic} openjdk version \u0026#34;10.0.2\u0026#34; 2018-07-17 OpenJDK Runtime Environment (build 10.0.2+13-Debian-2) OpenJDK 64-Bit Server VM (build 10.0.2+13-Debian-2, mixed mode)   Cool ! ü•≥ The -XX:MaxRAMPercentage parameter is also available on Java 8 starting from the update 191. But, it accepts only decimal value:\n1 2 3 4 5 6 7  $ docker container run -it --memory 800M --cpus 2 --entrypoint bash openjdk:8 root@5c441e3f56f3:/# java -XX:MaxRAMPercentage=50.0 \\  -XX:+PrintFlagsFinal -version | grep MaxHeapSize uintx MaxHeapSize := 419430400 {product} openjdk version \u0026#34;1.8.0_232\u0026#34; OpenJDK Runtime Environment (build 1.8.0_232-b09) OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode)   ü•≥ In this Java 8 based-container, we got the same MaxHeapSize value as the Java 10 container üòÅ\n‚ö†Ô∏è Important Note: it\u0026rsquo;s true that we can define the -XX:MaxRAMPercentage parameter to 100% but it\u0026rsquo;s not recommended and it\u0026rsquo;s even dangerous üò± but guess why ?\nImagine a situation: we want to deploy on our Kubernetes cluster a Java 10 based Docker image containing a Java application with a memory limit defined to 1GB. When the Pod will be created, it will have 1GB memory allocated to it. If we will have the -XX:MaxRAMPercentage parameter defined to 100%, the Java application running in the container can be consuming the total amount of the memory, which is the 2GB. In this case, the Pod will not have enough resources to have any Shell session, or even to be able to communicate with the kube api-server, etc.. So, the Kube Master will consider that this Pod is dead or unhealthy when it became unreachable.. so the Master will create a new Pod, and will kill the old one.. and when we will get the memory saturation.. a new Pod will be created and it will discard a the older one.. infinite loop ü•µ\nüí°A golden tip for defining the -XX:MaxRAMPercentage parameter to 75% only.. to let some space for the Pod internal processes üòé\nThere are other two JVM options have been added to allow Docker container users to gain more fine grained control over the amount of system memory that will be used for the Java Heap:\n -XX:InitialRAMPercentage: Set initial JVM Heap size as a percentage of the total memory -XX:MinRAMPercentage: Set the minimal JVM Heap size as a percentage of the total memory  Another recommended JVM option to use is the HeapDumpOnOutOfMemoryError which is used to tell the Java HotSpot VM to generate a heap dump when an allocation from the Java heap or the permanent generation cannot be satisfied. There is no overhead in running with this option, so it can be useful for production systems where the OutOfMemoryError exception takes a long time to surface. When the java.lang.OutOfMemoryError exception is thrown, a heap dump file is created. By default the heap dump is created in a file called java_pidpid.hprof in the working directory of the VM. We can specify an alternative file name or directory with the -XX:HeapDumpPath= option.\nFor example -XX:HeapDumpPath=/disk2/dumps will cause the heap dump to be generated in the /disk2/dumps directory.\nBefore the end The JVM options that we already saw can be integrated to the Dockerfile in the ENTRYPOINT Java command. But, this will require building an other image each time we want to change the values of the JVM options.\nA great solution is to integrate this JVM options in the JAVA_OPTS environment variable in the Kubernetes Deployment , which looks like:\n1 2 3 4 5 6 7 8 9 10 11  ...env:- name:JAVA_OPTSvalue:\u0026#34;-XX:MinRAMPercentage=20.0 -XX:MaxRAMPercentage=75.0 -XX:+HeapDumpOnOutOfMemoryError\u0026#34;...resources:limits:memory:512Mirequests:memory:256Mi...  That\u0026rsquo;s all tale ! üòÅ\nJava support for Docker Containers is a great new feature that comes with Java 10 (even ported to Java8 u191). It\u0026rsquo;s a very useful option in the most of the Java application deployed to Kubernetes ü•≥\n","permalink":"https://blog.nebrass.fr/playing-with-the-jvm-inside-docker-containers/","summary":"Nowadays, the most of the Java microservices and even many Java application are packaged and deployed as Docker containers. Everyone is enjoying (I hope üòÜ) the Docker experience, compared to the traditional VMs.¬†But, the Docker containerization will not come alone.. nothing is autonomous üòÅ So, there are many concerns to take into consideration while containerizing Java applications, like playing with the JVM.\nThe Docker racing was accelerated by the presence of Kubernetes in the market, which is coming with an amazing set of great features.","title":"Playing with the JVM inside Docker Containers"},{"content":"The serverless architecture became one of the most buzzy words nowadays. Almost all the cloud providers have a Serverless platforms in their catalogues:\n Microsoft Azure Functions Amazon Web Services Lambda Google Cloud Functions IBM Cloud Functions Oracle Functions  In an other world, there are many solutions to have a Serverless Runtime into Kubernetes, which is the most popular (and the most wonderful) container orchestrator in the market. These solutions are so helpful especially if you need portability for your functions. For example, you can have the same Serverless Runtime deployed to Azure Kubernetes Service and Google Kubernetes Engine. You can be deploying the same binaries as functions identically to both of the cloud providers.\nIn this tutorial, I will demonstrate how to deploy a Serverless Framework¬†to Minikube and on which we will deploy some Functions based on Spring Boot Framework.\nThere are many serverless runtimes for Kubernetes:\n Kubeless Knative Fission OpenFaas OpenWhisk Fn Project and maybe there is an other framework that appears while I\u0026rsquo;m writing this post üòÅ  For this first tutorial, I will choose Knative üòÑ\nWhat is Knative? Based on the Knative Documentation:\n Knative extends Kubernetes to provide a set of middleware components that are essential to build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a third-party data center.\nEach of the components under the Knative project attempt to identify common patterns and codify the best practices that are shared by successful, real-world, Kubernetes-based frameworks and applications.\nDevelopers on Knative can use familiar idioms, languages, and frameworks to deploy functions, applications, or containers workloads.\n The shortest definition that I like for Knative: a platform to build and run Serverless applications on Kubernetes:\n The Build features are offered via a set of building blocks for creating container images from source code. ‚ö†Ô∏è‚ö†Ô∏è Before the v0.8, Knative had its own Build component. Since the v0.8, Knative Build is deprecated in favor of Tekton üê±¬†‚ö†Ô∏è‚ö†Ô∏è The Run features are provided via two high level components:  Serving provides the possibility to deploy and serving functions and serverless applications. It includes automatic scale-to-zero function. Knative\u0026rsquo;s Serve component offers two important features for container management.:  The first feature is multiple-configuration: offers the ability to create different versions of the same container-based service and run them concurrently, and this is where the other feature of its serving component comes into play. The second feature is¬†service routing: offers the ability to do A/B Testing: route a subset of our users to the new version of service, while keeping the rest of the users routed to the old version. This process is helpful to ensure that the new version has no killers before totally migrating to it üòÅ   Eventing provides blocks for defining, consuming and producing events to bind event sources to services. This component define also the triggering actions based on them within a cloud-native environment.    To ensure the Serving \u0026amp; Eventing, Knative is based on the powerful Istio Service Mesh features to expose, monitor, control the services and to encrypt the transiting data.\nThe Knative high-level ecosystem looks like:\nKnative high-level ecosystem\n What\u0026rsquo;s this kitty looking to the deprecated Build component ? This is actually Tekton.\nTekton does not belong to Knative as internal component, but it\u0026rsquo;s adopted by Knative as its recommended CI/CD pipelines.\nI will be covering Tekton Pipelines¬†in a dedicated tutorial. Stay tuned üìªüé∂üòÅ\nWhy Knative ? Knative is an source platform developed and supported by very big companies like IBM, Google, Pivotal\u0026hellip; Knative is designed and developed based of the needs and the latest standards of the market. It is mainly developer focused, this is why all the features are developer friendly. With Knative, developers will be focusing only on implementing the business logic in the source code and avoid them the waste of effort/time related to building, deploying and managing the environment, which is the core principle of the serverless architecture. The key part is to package the code in Containers that can be deployed to Kubernetes via the Knative pipelines.\nWith the integration with Kubernetes, Knative boosts its position by adopting Istio as core Service Mesh solution.\nNow, let\u0026rsquo;s move to the practical part of this post.\nInstalling the requirements of Knative Knative requires the having a Kubernetes cluster with Istio Service Mesh deployed ü§ì\nI will be using Minikube as local Kubernetes cluster.\nConfiguring Minikube So, we will start by configuring Minikube. I suppose that you already installed the VM Driver and Minikube binaries for your OS ü•≥\nWe will start by creating a new minikube profile:\n1  $ minikube profile knative   Next, we will start the minikube with our custom configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ minikube start --memory=8192 --cpus=6 \\  --kubernetes-version=v1.14.0 \\  --vm-driver=hyperkit \\  --disk-size=30g \\  --extra-config=apiserver.enable-admission-plugins=\u0026#34;LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook\u0026#34; üòÑ [knative] minikube v1.5.2 on Darwin 10.15.1 üî• Creating hyperkit VM (CPUs=6, Memory=8192MB, Disk=30000MB) ... üê≥ Preparing Kubernetes v1.14.0 on Docker \u0026#39;18.09.9\u0026#39; ... ‚ñ™ apiserver.enable-admission-plugins=LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook üöú Pulling images ... üöÄ Launching Kubernetes ... ‚åõ Waiting for: apiserver üèÑ Done! kubectl is now configured to use \u0026#34;knative\u0026#34;   Now, we need to install Istio Service Mesh on our Minikube cluster üòÅ\nInstalling Istio This tutorial will cover the installation of Istio v1.1.7.\nDownloading Istio and installing Custom resources definitions   Download Istio Service Mesh files:\n1 2 3  $ export ISTIO_VERSION=1.1.7 $ curl -L https://git.io/getLatestIstio | sh - $ cd istio-${ISTIO_VERSION}     We will install the Istio CRDs:\n1  $ for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done     Now, we need to create a namespace with a label istio-injection: disabled. To do that:\n1 2  $ kubectl create ns istio-system $ kubectl label ns istio-system istio-injection=disabled     Now we will proceed to install Istio without Sidecar Injection, which is the recommended default installation ü•≥ We need to build the resources file based on the template :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  $ helm template --namespace=istio-system \\  --set prometheus.enabled=false \\  --set mixer.enabled=false \\  --set mixer.policy.enabled=false \\  --set mixer.telemetry.enabled=false \\  `# Pilot doesn\u0026#39;t need a sidecar.` \\ --set pilot.sidecar=false \\  --set pilot.resources.requests.memory=128Mi \\  `# Disable galley (and things requiring galley).` \\ --set galley.enabled=false \\  --set global.useMCP=false \\  `# Disable security / policy.` \\ --set security.enabled=false \\  --set global.disablePolicyChecks=true \\  `# Disable sidecar injection.` \\ --set sidecarInjectorWebhook.enabled=false \\  --set global.proxy.autoInject=disabled \\  --set global.omitSidecarInjectorConfigMap=true \\  --set gateways.istio-ingressgateway.autoscaleMin=1 \\  --set gateways.istio-ingressgateway.autoscaleMax=2 \\  `# Set pilot trace sampling to 100%` \\ --set pilot.traceSampling=100 \\  install/kubernetes/helm/istio \\  \u0026gt; ./istio-minimal.yaml     The final step will be to create the resources based on the generated file:\n1  $ kubectl apply -f istio-minimal.yaml     Now, we need to verify that everything is working fine :\n1  $ kubectl get pods --namespace istio-system   The output will looks like:\n1 2 3  NAME READY STATUS RESTARTS AGE istio-ingressgateway-57dfd8fd67-gf27c 1/1 Running 0 112m istio-pilot-6fb7569c86-rhk5n 1/1 Running 0 112m   As everything is working as expected, now we can proceed to install Knative ü•≥\n  Installing the Knative After installing Istio Service Mesh, we will install Knative:\n  As we did for Istio, we need to install the Knative CRDs:\n1 2 3 4  $ kubectl apply --selector knative.dev/crd-install=true \\  --filename https://github.com/knative/serving/releases/download/v0.10.0/serving.yaml \\  --filename https://github.com/knative/eventing/releases/download/v0.10.0/release.yaml \\  --filename https://github.com/knative/serving/releases/download/v0.10.0/monitoring.yaml   This command will install these CRDs:\n apiserversources.sources.eventing.knative.dev brokers.eventing.knative.dev certificates.networking.internal.knative.dev channels.messaging.knative.dev configurations.serving.knative.dev containersources.sources.eventing.knative.dev cronjobsources.sources.eventing.knative.dev eventtypes.eventing.knative.dev images.caching.internal.knative.dev ingresses.networking.internal.knative.dev inmemorychannels.messaging.knative.dev metrics.autoscaling.internal.knative.dev parallels.messaging.knative.dev podautoscalers.autoscaling.internal.knative.dev revisions.serving.knative.dev routes.serving.knative.dev sequences.messaging.knative.dev serverlessservices.networking.internal.knative.dev services.serving.knative.dev subscriptions.messaging.knative.dev triggers.eventing.knative.dev    We need to install Knative and all its dependencies now:\n1 2 3  $ kubectl apply --filename https://github.com/knative/serving/releases/download/v0.10.0/serving.yaml \\  --filename https://github.com/knative/eventing/releases/download/v0.10.0/release.yaml \\  --filename https://github.com/knative/serving/releases/download/v0.10.0/monitoring.yaml   These commands will create 245 resources ü§™ divised like:\n 3 Namespaces 37 ClusterRole 14 ServiceAccount 17 ClusterRoleBinding 8 RBAC Role 6 RoleBinding 21 CRDs 32 ConfigMaps 15 Deployment 15 ReplicaSet 16 Services 20 Pods 2 StatefulSet 2 DaemonSet 1 HorizontalPodAutoscaler \u0026hellip;    Before moving to the next step, we need to verify that all the pods are running:\n1  $ kubectl get pods --all-namespaces   Now that your cluster has Knative installed, you‚Äôre ready to deploy an app. üòÅ\n  PREPARING OUR SAMPLE APPLICATION We will generate a new Spring Boot application with the Web dependency. We will create a new RestController:\n1 2 3 4 5 6 7 8 9  @RestController @RequestMapping(\u0026#34;/api\u0026#34;) public class HelloController { @GetMapping(\u0026#34;/hello\u0026#34;) public String sayHello() { return \u0026#34;Hello Nebrass ! it\u0026#39;s \u0026#34; + System.currentTimeMillis(); } }   Now, I will package this beautiful application üòú in a Docker üê≥ container. I will be using the great Google Jib Maven plugin. To do that I will add the plugin in the build section of the pom.xml:\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jib-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;to\u0026gt; \u0026lt;image\u0026gt;docker.io/nebrass/${project.artifactId}:${project.version}\u0026lt;/image\u0026gt; \u0026lt;credHelper\u0026gt;osxkeychain\u0026lt;/credHelper\u0026gt; \u0026lt;/to\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   The credHelper section will tell the plugin to get my Docker Hub credentials from the Mac OS Keychain, where I stored them üòÅ for sure I will not be happy to share them üòú\nLet\u0026rsquo;s build the project now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ mvn compile jib:build This command will compile, build and push the container to **Docker Hub**. The **Jib** build log will look like: [INFO] [INFO] --- jib-maven-plugin:1.8.0:build (default-cli) @ spring-knative-application --- [INFO] [INFO] Containerizing application to nebrass/spring-knative-application:0.0.1-SNAPSHOT... [WARNING] Base image \u0026#39;gcr.io/distroless/java:8\u0026#39; does not use a specific image digest - build may not be reproducible [INFO] Using base image with digest: sha256:a13ac1ce516ec5e49ae9dfd3b8183e9e8328180a65757d454e594a9ce6d1e35d [INFO] [INFO] Container entrypoint set to [java, -cp, /app/resources:/app/classes:/app/libs/*, com.targa.labs.dev.knative.SpringKubelessApplication] [INFO] [INFO] Built and pushed image as nebrass/spring-knative-application:0.0.1-SNAPSHOT [INFO] Executing tasks: [INFO] [=========================== ] 90,0% complete [INFO] \u0026gt; launching layer pushers [INFO]   Good ! But this is an over engineered solution for our minikube proof-of-concept. Instead, we can use the minikube local Docker registry. The Jib plugin configuration will look like:\n1 2 3 4 5 6 7 8 9 10  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jib-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;to\u0026gt; \u0026lt;image\u0026gt;nebrass/${project.artifactId}:${project.version}\u0026lt;/image\u0026gt; \u0026lt;/to\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   Before building the Docker container, don\u0026rsquo;t forget to map the environment to the minikube local environment:\n1 2 3  $ eval $(minikube docker-env) $ mvn compile jib:dockerBuild   Now, the container will be built in the local Docker daemon and pushed into the minikube local registry.\nNow, we need to create a Knative Service (aka ksvc). The ksvc definition file looks like:\n1 2 3 4 5 6 7 8 9  apiVersion:serving.knative.dev/v1kind:Servicemetadata:name:spring-knative-applicationspec:template:spec:containers:- image:nebrass/spring-knative-application:0.0.1-SNAPSHOT  Now, let\u0026rsquo;s create the ksvc resource from the this ksvc.yaml file:\n1  $ kubectl apply -f ksvc.yaml   Now that our KService is created, Knative will perform the following steps:\n Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balancer for our app. Automatically scale our pods up and down (including to zero active pods).  To check the listing of the KServices:\n1 2 3 4  $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON spring-knative-application http://spring-knative-application.default.example.com spring-knative-application-h4w27 spring-knative-application-h4w27 True   The URL section is the one that interests us now: http://spring-knative-application.default.example.com\nWe will be using this value for consuming our KService. But before, we need to get the Istio Ingress Gateway IP address and port:\n1 2 3  $ export IP_ADDRESS=$(minikube ip):$(kubectl get svc istio-ingressgateway \\  --namespace istio-system \\  --output \u0026#39;jsonpath={.spec.ports[?(@.port==80)].nodePort}\u0026#39;)   In Knative, KService is reachable on:\nKnative KService access path\n We can for example access the /api/hello REST API on our spring-knative-application ksvc via curl :\n1  $ curl -H \u0026#34;Host: spring-knative-application.default.example.com\u0026#34; $IP_ADDRESS/api/hello   We can use also httpie, my preferred tool:\n1  $ http $IP_ADDRESS/api/hello \u0026#39;Host: spring-knative-application.default.example.com\u0026#39;   The request parameters are:\n spring-knative-application.default.example.com: is the URL of our ksvc $IP_ADDRESS: is the Istio Ingress Gateway that we created in the previous step /api/hello: is the URI of our REST Controller that we created in our sample Spring Boot Application  Magically, we will get a response like :\nHello Nebrass ! it's 1574799175389\nü§©ü•≥ü§©ü•≥ WaaaaaYY ü§©ü•≥ü§©ü•≥\nbehind the scenes - how things work ? Our application is served by the KNative Serving component (wow obviously üòÖ).. so let\u0026rsquo;s go deep in the KNative Serving.\nLet\u0026rsquo;s start with some theories ü§ì This definitionsüëáüëá are grabbed from the official Knative documentation üëáüëá I made some modificatiosn\nKnative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster:\n Service: The¬†service.serving.knative.dev¬†resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision. Route: The¬†route.serving.knative.dev resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named Routes. Configuration: The¬†configuration.serving.knative.dev resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the 12Factor App methodology. Modifying a configuration creates a new revision. This is a very big point in KNative: REVISIONS ARE IMMUTABLE. Revision: The¬†revision.serving.knative.dev resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. AGAIN: Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic.  Knative Serving diagram\n This is the theory, but what\u0026rsquo;s happening in our case ? üòÅ\nThe Knative Serving diagram of our use case his:\nThe Knative Serving Ecosystem for our use case\n What\u0026rsquo;s this Autoscaler and Activator? üò±üò±\nThe Autoscaler \u0026amp; Activator are the two main components used by Knative to:\n handle elasticity of Pods to handle the high load of requests scale down to zero when there is not active request üòÅ Don\u0026rsquo;t forget, the idea behind the Serverless is to have no active resource in the IDLE status ü§ë  The Autoscaler \u0026amp; Activator appear in the cluster as Pods running in the knative-serving namespace:\n The Autoscaler collects information about the number of concurrent requests to a Revision, through a container called the queue-proxy running inside the Revision‚Äôs Pod that already runs an other container called user-provided which is our spring-knative-application image. The Autoscaler, based on the number of requests, will increase or decrease the number of desired replicas in the Revision related Deployment. The Activator is a component that receives all the traffic coming to the IDLE Revisions. When the Activator receives a request, it¬†changes the Revision state to Active, which lets the Revision Pods receive the requests.  Playing with Logs, metrics and traces on Knative Knative logs In the KNative world, we have a (very) common Kubernetes logging pattern offered by the EFK brothers üòÅ no, seriously, no it\u0026rsquo;s the EFK Band ü§©\n Elasticsearch is a distributed, RESTful search and analytics engine. Fluentd is an Logs collector - alternative for Logstash and more suitable for Containers. Kibana most used Elasticsearch data visualization engine.  Cool ! But we need to enable the EFK Logging in our KNative cluster: To do that we need to some configuration and some installations.\n  We need to edit the config-observability ConfigMap stored in the knative-serving Namespace:\n1  $ kubectl edit cm -n knative-serving config-observability     Now, under the data section add this entry:\n1 2 3 4 5 6  logging.request-log-template: \u0026#39;{\u0026#34;httpRequest\u0026#34;: {\u0026#34;requestMethod\u0026#34;: \u0026#34;{{.Request.Method}}\u0026#34;, \u0026#34;requestUrl\u0026#34;: \u0026#34;{{js .Request.RequestURI}}\u0026#34;, \u0026#34;requestSize\u0026#34;: \u0026#34;{{.Request.ContentLength}}\u0026#34;, \u0026#34;status\u0026#34;: {{.Response.Code}}, \u0026#34;responseSize\u0026#34;: \u0026#34;{{.Response.Size}}\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;{{js .Request.UserAgent}}\u0026#34;, \u0026#34;remoteIp\u0026#34;: \u0026#34;{{js .Request.RemoteAddr}}\u0026#34;, \u0026#34;serverIp\u0026#34;: \u0026#34;{{.Revision.PodIP}}\u0026#34;, \u0026#34;referer\u0026#34;: \u0026#34;{{js .Request.Referer}}\u0026#34;, \u0026#34;latency\u0026#34;: \u0026#34;{{.Response.Latency}}s\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;{{.Request.Proto}}\u0026#34;}, \u0026#34;traceId\u0026#34;: \u0026#34;{{index .Request.Header \u0026#34;X-B3-Traceid\u0026#34;}}\u0026#34;}\u0026#39;   Be careful about the spaces, the ConfigMap will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  apiVersion:v1data:...logging.request-log-template:\u0026#39;{\u0026#34;httpRequest\u0026#34;: {\u0026#34;requestMethod\u0026#34;: \u0026#34;{{.Request.Method}}\u0026#34;, \u0026#34;requestUrl\u0026#34;: \u0026#34;{{js .Request.RequestURI}}\u0026#34;, \u0026#34;requestSize\u0026#34;: \u0026#34;{{.Request.ContentLength}}\u0026#34;, \u0026#34;status\u0026#34;: {{.Response.Code}}, \u0026#34;responseSize\u0026#34;: \u0026#34;{{.Response.Size}}\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;{{js .Request.UserAgent}}\u0026#34;, \u0026#34;remoteIp\u0026#34;: \u0026#34;{{js .Request.RemoteAddr}}\u0026#34;, \u0026#34;serverIp\u0026#34;: \u0026#34;{{.Revision.PodIP}}\u0026#34;, \u0026#34;referer\u0026#34;: \u0026#34;{{js .Request.Referer}}\u0026#34;, \u0026#34;latency\u0026#34;: \u0026#34;{{.Response.Latency}}s\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;{{.Request.Proto}}\u0026#34;}, \u0026#34;traceId\u0026#34;: \u0026#34;{{index .Request.Header \u0026#34;X-B3-Traceid\u0026#34;}}\u0026#34;}\u0026#39;kind:ConfigMapmetadata:annotations:kubectl.kubernetes.io/last-applied-configuration:|{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;_example\u0026#34;:\u0026#34;... creationTimestamp: \u0026#34;2019-11-27T13:53:51Z\u0026#34; labels: serving.knative.dev/release: v0.10.0 name: config-observability namespace: knative-serving resourceVersion: \u0026#34;5058\u0026#34;selfLink:/api/v1/namespaces/knative-serving/configmaps/config-observabilityuid:57a86288-111d-11ea-ba80-2659da8acb87    Install the ELK Stack:\n1  $ kubectl apply --filename https://github.com/knative/serving/releases/download/v0.10.0/monitoring-logs-elasticsearch.yaml     Before going to the next steps, verify that all the pods are running:\n1  $ kubectl get pods --namespace knative-monitoring --watch     We need to to ensure that the Fluentd DaemonSet runs on all our nodes via labeling the nodes with the beta.kubernetes.io/fluentd-ds-ready=true¬†label:\n1  $ kubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=\u0026#34;true\u0026#34;     Run the following command to ensure that the fluentd-ds DaemonSet is ready on at least one node:\n1  $ kubectl get daemonset fluentd-ds --namespace knative-monitoring --watch     Now, we will start a local Kubernetes proxy:\n1  $ kubectl proxy     Next, access the Kibana via the URL: http://localhost:8001/api/v1/namespaces/knative-monitoring/services/kibana-logging/proxy/app/kibana\n  Now, the first step is the configure an index pattern:\n Index pattern: logstash-* Time Filter field name: @timestamp  Kibana - Create index pattern\n   Now, click on the Discover menu, and in the search menu, we will enter the name of our KNative Service: spring-knative-application\nKibana - Searching KNative Service logging\n Have fun ! ü•≥üòçü§©\n  Knative metrics On KNative, we access metrics through the Grafana UI, which is the visualization tool for Prometheus.\n‚ö†Ô∏è If you are not used to Grafana and Prometheus, I highly recommend this great free course üòé Nowadays, mastering Grafana and Prometheus is a highly recommended skill for any Kubernetes professional.\nhttps://www.youtube.com/watch?v=bErGEHf6GCc\u0026amp;list=PLpbcUe4chE7-HuslXKj1MB10ncorfzEGa\nTo open the KNative¬†Grafana UI, enter the following command:\n1 2 3 4  $ kubectl port-forward --namespace knative-monitoring \\  $(kubectl get pods --namespace knative-monitoring \\  --selector=app=grafana \\  --output=jsonpath=\u0026#34;{.items..metadata.name}\u0026#34;) 3000   Now, the Grafana UI is reachable via the URL: http://localhost:3000:\nGrafana UI: Knative Serving - Control Plane Efficiency dashboard\n The following dashboards are pre-installed with KNative Serving:\n Revision HTTP Requests:¬†HTTP request count, latency, and size metrics per revision and per configuration Nodes:¬†CPU, memory, network, and disk metrics at node level Pods:¬†CPU, memory, and network metrics at pod level Deployment:¬†CPU, memory, and network metrics aggregated at deployment level Istio, Mixer and Pilot:¬†Detailed Istio mesh, Mixer, and Pilot metrics Kubernetes:¬†Dashboards giving insights into cluster health, deployments, and capacity usage  If you are used to play with Prometheus, keep calm, you will find always your toy. To access Prometheus, run this command:\n1 2 3 4  $ kubectl port-forward -n knative-monitoring \\  $(kubectl get pods -n knative-monitoring \\  --selector=app=prometheus \\  --output=jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) 9090   Next, access the Prometheus UI via the URL: http://localhost:9090\nPrometheus UI: sample http_request_duration_microseconds¬†graph\n Enjoy ! ü•≥\nKnative Request Tracing We need to enable the support of Zipkin - I will be using Zipkin with in-memory database:\n1  $ kubectl apply --filename https://github.com/knative/serving/releases/download/v0.10.0/monitoring-tracing-zipkin-in-mem.yaml   Next, before accessing the Zipkin UI, start local proxy:\n1  $ kubectl proxy   Next, access the Zipkin UI via the URL: http://localhost:8001/api/v1/namespaces/istio-system/services/zipkin:9411/proxy/zipkin/\nNow, click \u0026ldquo;Find Traces\u0026rdquo; to see the latest traces.\nZipkin UI: KNative ksvc requests traces\n To load the requests traces on our KService, just choose:\n istio-ingressgateway as Service Name spring-knative-application-something as Span Name üëâ which is the only instance of our KNative, in case of high load you will find many instances of our ksvc  Click Find Traces to load all the gathered traces.\nIf you check the loaded traces, you can notice that the first request took 4 seconds to be handled, not like all the others that took only some milliseconds only. These traces can demonstrate the heavy cost of the first request. üòÅ\nWhere is The knative eventing ? üòÅ COMING SOON !\n","permalink":"https://blog.nebrass.fr/playing-with-serverless-in-kubernetes-with-knative/","summary":"The serverless architecture became one of the most buzzy words nowadays. Almost all the cloud providers have a Serverless platforms in their catalogues:\n Microsoft Azure Functions Amazon Web Services Lambda Google Cloud Functions IBM Cloud Functions Oracle Functions  In an other world, there are many solutions to have a Serverless Runtime into Kubernetes, which is the most popular (and the most wonderful) container orchestrator in the market. These solutions are so helpful especially if you need portability for your functions.","title":"Playing with Serverless in Kubernetes with Knative"},{"content":"In some previous articles, I was writing about the Azure Functions, which is the Microsoft Serverless solutions. After joining Microsoft, I got the chance to work on this great product for many business cases and I found it was really great ü•≥\nRecently, one of my Facebook friends asked me to write a blog post about what is Serverless architecture ? ü§î what is for ? ü§î and especially, what are the scenarios that are not suitable for Serverless ? ü§î\nMaybe I had to write this post before starting the Azure Functions tutorials series. But, as we say \u0026ldquo;It\u0026rsquo;s never too late to set things right\u0026rdquo; üòÅ\nINTRODUCTION Serverless computing is an architecture that aims to delegate the execution of a piece of code to the cloud, where necessary resources will be dynamically allocated. Dynamic allocation means always the¬†pay-as-you-go¬†model when we are hosting the code on a Public Cloud Provider.\nThe Serverless application is composed by functions triggered by some event like an HTTP Request, a Message (Kafka for example) or even a Scheduled event. That‚Äôs why, we refer to the serverless applications as¬†\u0026ldquo;Functions as a Service\u0026rdquo;¬†or¬†\u0026ldquo;FaaS\u0026rdquo;.\nmy code will be running without a server ? NO ! The naming for Serverless is just to say that you dont need to care about where the code will be running. You have just to write your code, based on the templates/practices guide provided by your cloud/serverless provider.\nGenerally speaking, the most of the cloud providers serverless platforms take as input the source code packaged with some dependencies in a JAR file.\nSo, you just you take care of implementing your business logic in the code and the cloud provider will be taking care of running, scaling and even for monitoring your Serverless Application.\nWhen do I need to use serverless architecture ? It\u0026rsquo;s good to start by asking this question. For sure, the serverless architecture is not the key solution for every project.\nYou should adopt serverless architecture for some (can be all) components of you application, in case:\n high latency application  AND\n having specific heavy frequent solicited components unlike all the rest of the application components  AND\n not permanent running application  AND\n stateless applications  There are other cases that can fit into Serverless, like IoT and Big Data applications and batchs.\nIf you have this use cases, you can start thinking about Serverless Architectures. Making tests and PoCs will give you a better idea if your components can go to the Serverless world.\nWhen I SHOULDN\u0026rsquo;T use serverless architecture ? For sure, you shouldn\u0026rsquo;t adopt Serverless architecture if your application doesn\u0026rsquo;t satisfy the previously listed requirements, especially if:\n you expect to have a response with a somehow defined latency like real-time applications  AND\n you have a permanent running application without IDLE time  OR\n you have a very long executing processes  WHAT IS THE ADVANTAGES OF ADOPTING THE SERVERLESS ARCHITECTURE?  Cost effective: the billing model is only pay-as-you-go üòÅ you don\u0026rsquo;t need to pay for the IDLE time of your application. Highly scalable: serverless application are scalable automatically. When there is a high load, your application is highly available without any need for special clustering or any manual operations. ü§© Fast time to market: just time is consumed for writing code - no administration boilerplates ü•≥ Light responsibilities: you don\u0026rsquo;t care about runtime and maintenance of the environment. You just focus on implementing the business logic in the code. Life is great when we are lazy üòÅ Easy monitoring: everything needed for monitoring and tracing the serverless application is provided by the vendor out of the box üòé  WHAT IS THE DRAWBACKS OF ADOPTING THE SERVERLESS ARCHITECTURE?  THE COST OF THE FIRST CALL üò±ü§Øüò≠I personally consider that as huge drawback for the serverless. Serverless applications go always to the idle state (as by design they are made to run occasionally and not permanently). To bring them up from the idle state, when triggered, the application will take some time to start which cannot be afforded by some client needing low latency. You lose control of your application: you don\u0026rsquo;t administer you application as everything is managed üò§ Complexity for development: developing and testing locally is no so evident - it\u0026rsquo;s very hard ü§Ø Cloud provider lock-in: your code is packaged based on your vendor templates - so no easy or evident portability ü•µ Security risks: your vendor can have security flaws that will directly impact your components üò±  CONCLUSION Serverless architecture is a very innovative paradigm in the software engineering ecosystem, which comes with a very wide variety of advantages, and certainly with drawbacks. The compatibility of this architecture with your application has no magic answer, you need to audit, test and try it to be sure that you can fit on. ü§î\nI suggest that every developer give it a try and to gather its practices and skills. I am sure that the serverless will be a mandatory skill required in the market the same as the microservices architecture became nowadays. üòÅ\n","permalink":"https://blog.nebrass.fr/playing-with-serverless-architecture/","summary":"In some previous articles, I was writing about the Azure Functions, which is the Microsoft Serverless solutions. After joining Microsoft, I got the chance to work on this great product for many business cases and I found it was really great ü•≥\nRecently, one of my Facebook friends asked me to write a blog post about what is Serverless architecture ? ü§î what is for ? ü§î and especially, what are the scenarios that are not suitable for Serverless ?","title":"Playing with Serverless Architecture"},{"content":"In this tutorial, we will be experimenting a new use case of the great Azure Functions service üòÅ one of my favorite products in Azure.\nIn this tutorial, we will bring the powerful features of the Spring Framework to our Azure Functions Java projects. After this tutorial, creating a new Azure Functions Java based on business logic that you already have in your Spring Boot Application will be a very easy game.\nBefore digging into the code, we need to create a new Functions Application in our Azure subscription.\nCreating the Azure Function App The first step is to create the new Functions Application on the Azure Portal: https://portal.azure.com\nNext, go to the Function App and when you click on Add you will get this window:\nCreate new Azure Function\n Fill this form with this details:\n App name: spring-azure-function Resource Group: spring-azure-function-rg OS: Windows Hosting Plan: Consumption plan Location: France Central Runtime Stack: Obviously Java üòÅ Storage: We will create a new one Application Insights: Just click on Application insights and Enable the Collect application monitoring data using Application Insights. Then Apply and next click on Create ü§ì  PREPARING the Functions Application POM.XML We will start by generating a new Spring Boot application using the Spring Initializr:\nGenerate a blank new Spring Boot application using Spring Initializr\n Next; in the generated project, in the pom.xml replace the properties section with these values:\n1 2 3 4 5 6 7 8 9 10 11 12 13  \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;azure.functions.maven.plugin.version\u0026gt;1.3.4\u0026lt;/azure.functions.maven.plugin.version\u0026gt; \u0026lt;azure.functions.java.library.version\u0026gt;1.3.4\u0026lt;/azure.functions.java.library.version\u0026gt; \u0026lt;functionAppName\u0026gt;spring-azure-function\u0026lt;/functionAppName\u0026gt; \u0026lt;functionAppRegion\u0026gt;francecentral\u0026lt;/functionAppRegion\u0026gt; \u0026lt;stagingDirectory\u0026gt;${project.build.directory}/azure-functions/${functionAppName}\u0026lt;/stagingDirectory\u0026gt; \u0026lt;functionResourceGroup\u0026gt;spring-azure-function-rg\u0026lt;/functionResourceGroup\u0026gt; \u0026lt;start-class\u0026gt;com.targa.labs.dev.springfunctionsazure.SpringFunctionsAzureApplication\u0026lt;/start-class\u0026gt; \u0026lt;wrapper.version\u0026gt;1.0.23.RELEASE\u0026lt;/wrapper.version\u0026gt; \u0026lt;/properties\u0026gt;   In this properties, we have:\n functionAppName: the name of the Functions Application that we already created in our Azure Subscription functionAppRegion: the location defined for the Functions Application that we already created in our Azure Subscription start-class: the full path of the main class of the generated Spring Boot application  Next, in the dependencies section, we need to replace the listed ones by these values:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-function-web\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-function-adapter-azure\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.10\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;   In this code we added:\n Spring Cloud Starter Function Web: brings in the necessary dependencies to run the functions locally. Spring Cloud Function Adapter Azure: bootstraps a Spring Cloud Function context and channels function calls from the Azure framework into the user functions, using Spring Boot configuration where necessary. Lombok: Java library that will help us avoid writing the getters, setters, toString, constructors.. Spring Boot Starter Test: contains the majority of dependencies and configurations required for the tests.  Add to that, we need to add this dependencyManagement section:\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-function-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt;   This dependencyManagement section will make our project inherit dependencies information from the Spring Cloud Function Dependencies parent pom.\nNow, we will prepare the build section of our pom.xml. In this section we will need to add and configure many plugins:\n Azure Functions Maven Plugin Maven Resources Plugin Maven Dependency Plugin Maven Clean Plugin Spring Boot Maven Plugin  We will define the plugins that we will have in the pluginManagement section:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026lt;pluginManagement\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;azure-functions-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${azure.functions.maven.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-clean-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/pluginManagement\u0026gt;   Yes üòÜ I didn\u0026rsquo;t mentioned the plugin spring-boot-maven-plugin in the pluginManagement section for a very simple reason: our project is a Spring Boot Application and it will inherit the details of the spring-boot-maven-plugin from the Spring Boot Parent Pom ü§ì\nNow we will configure the build plugins one by one. Let\u0026rsquo;s start by azure-functions-maven-plugin:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;azure-functions-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;resourceGroup\u0026gt;${functionResourceGroup}\u0026lt;/resourceGroup\u0026gt; \u0026lt;appName\u0026gt;${functionAppName}\u0026lt;/appName\u0026gt; \u0026lt;region\u0026gt;${functionAppRegion}\u0026lt;/region\u0026gt; \u0026lt;appSettings\u0026gt; \u0026lt;!-- Run Azure Function from package file by default --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;WEBSITE_RUN_FROM_PACKAGE\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;FUNCTIONS_WORKER_RUNTIME\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;java\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;MAIN_CLASS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.targa.labs.dev.demo.SpringFunctionsAzureApplication\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;FUNCTIONS_EXTENSION_VERSION\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;~2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/appSettings\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;package-functions\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;package\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   We passed the Azure Functions properties that we defined in a previous step. We also specified some Settings for our Azure Functions Application:\n WEBSITE_RUN_FROM_PACKAGE: 1 üëâ True to run Azure Function from package file FUNCTIONS_WORKER_RUNTIME: Java ü§î Obviously MAIN_CLASS: the full path name of the Main class of our Spring Boot Application FUNCTIONS_EXTENSION_VERSION: ~2 üëâ the Azure Functions Extension Version, we will be using the latest stable version but you can use the ~3 to try to the preview of the incoming 3.x  Now, we will configure the¬†maven-resources-plugin:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-resources\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${project.basedir}/src/main/azure \u0026lt;/directory\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;outputDirectory\u0026gt; ${project.build.directory}/azure-functions/${functionAppName} \u0026lt;/outputDirectory\u0026gt; \u0026lt;overwrite\u0026gt;true\u0026lt;/overwrite\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   The Maven Resources Plugin will copy all the resources available under the folder src/main/azure into the folder target/azure-functions/spring-azure-function.\nGood, but we don\u0026rsquo;t have this folder yet, so it\u0026rsquo;s a good opportunity to create it. Inside this folder, we need to create a host.json file that contains:\n1 2 3 4 5 6 7  { \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;extensionBundle\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.Functions.ExtensionBundle\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;[1.*, 2.0.0)\u0026#34; } }   and we need to create local.setting.json file that contains:\n1 2 3 4 5 6 7  { \u0026#34;IsEncrypted\u0026#34;: false, \u0026#34;Values\u0026#34;: { \u0026#34;AzureWebJobsStorage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FUNCTIONS_WORKER_RUNTIME\u0026#34;: \u0026#34;java\u0026#34; } }   Great ! Now let\u0026rsquo;s define the configuration maven-dependency-plugin:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-dependencies\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-dependencies\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;outputDirectory\u0026gt;${stagingDirectory}/lib\u0026lt;/outputDirectory\u0026gt; \u0026lt;overWriteReleases\u0026gt;false\u0026lt;/overWriteReleases\u0026gt; \u0026lt;overWriteSnapshots\u0026gt;false\u0026lt;/overWriteSnapshots\u0026gt; \u0026lt;overWriteIfNewer\u0026gt;true\u0026lt;/overWriteIfNewer\u0026gt; \u0026lt;includeScope\u0026gt;runtime\u0026lt;/includeScope\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   The Maven Dependency Plugin will copy all the dependencies for running the Azure Function Application into to the folder target/azure-functions/spring-azure-function/lib.\nNow we need to configure the Maven Clean Plugin to remove the obj folder generated by the Azure Functions DotNet SDK:\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-clean-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;filesets\u0026gt; \u0026lt;fileset\u0026gt; \u0026lt;directory\u0026gt;obj\u0026lt;/directory\u0026gt; \u0026lt;/fileset\u0026gt; \u0026lt;/filesets\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   Finally, it\u0026rsquo;s the turn of the Spring Boot Maven Plugin:\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot.experimental\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-thin-layout\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${wrapper.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt;   Here as you see, we added a dependency within the Spring Boot Maven Plugin which is the Spring Boot Thin Layout which helps to dry run (download and cache the dependencies) the Spring Boot built JAR ü•≥\nCODING THE FUNCTIONS APPLICATION Let\u0026rsquo;s create a sample Function that gets a User object with Name attribute and makes as output a Greeting object with \u0026ldquo;Hello Name\u0026rdquo; message attribute.\nThe User class:\n1 2 3 4 5 6 7 8  import lombok.AllArgsConstructor; import lombok.Data; @Data @AllArgsConstructor public class User { private String name; }   The Greeting class:\n1 2 3 4 5 6 7 8  import lombok.AllArgsConstructor; import lombok.Data; @Data @AllArgsConstructor public class Greeting { private String message; }   Now, we can create our sample Function class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  public class GreetingHandler extends AzureSpringBootRequestHandler\u0026lt;User, Greeting\u0026gt; { @FunctionName(\u0026#34;greet\u0026#34;) public Greeting execute( @HttpTrigger(name = \u0026#34;request\u0026#34;, methods = {HttpMethod.POST}, authLevel = AuthorizationLevel.ANONYMOUS) HttpRequestMessage\u0026lt;Optional\u0026lt;User\u0026gt;\u0026gt; request, ExecutionContext context) { context.getLogger().info( String.format(\u0026#34;Request for : [%s]\u0026#34;, request.getBody().map(User::getName)) ); return handleRequest(request.getBody().get(), context); } }   Our GreetingHandler class inherits from AzureSpringBootRequestHandler\u0026lt;InputTypeClass, OutputTypeClass\u0026gt;. The AzureSpringBootRequestHandler class has a main method called handleRequest to which we will use to delegate the actual function call that we received in execute() method to the Spring Context, which will forward our call to the suitable configured Spring Bean.\nCool, but where our function call will be forwarded ?\nWe want that our request get computed in a Spring Service component which looks like this:\n1 2 3 4 5 6 7  @Service public class GreetingService { public Greeting sayHello(User name) { return new Greeting(\u0026#34;Hello \u0026#34; + name); } }   Now, we need to make the glue that links our Function Handler to the Spring Service ü§î\nIt\u0026rsquo;s quite easy ! to do that, we just need to define a Bean¬†of type¬†Function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @SpringBootApplication public class SpringFunctionsAzureApplication { @Autowired private GreetingService greetingService; @Bean public Function\u0026lt;User, Greeting\u0026gt; hello() { return user -\u0026gt; greetingService.sayHello(user); } public static void main(String[] args) throws Exception { SpringApplication.run(SpringFunctionsAzureApplication.class, args); } }   Now, our function will forward the call to the sayHello() method of the GreetingService that we injected ü•≥\nLet\u0026rsquo;s build and run our Spring Booted Azure Function ü§™\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  $ mvn clean package azure-functions:run ... [INFO] --- azure-functions-maven-plugin:1.3.4:run (default-cli) @ spring-functions-azure --- .. [INFO] Azure Functions Core Tools found. %%%%%% %%%%%% @ %%%%%% @ @@ %%%%%% @@ @@@ %%%%%%%%%%% @@@ @@ %%%%%%%%%% @@ @@ %%%% @@ @@ %%% @@ @@ %% @@ %% % Azure Functions Core Tools (2.7.1846 Commit hash: 458c671341fda1c52bd46e1aa8943cb26e467830) Function Runtime Version: 2.0.12858.0 .. [19/11/2019 20:33:36] Generating 1 job function(s) [19/11/2019 20:33:36] Found the following functions: [19/11/2019 20:33:36] Host.Functions.greet [19/11/2019 20:33:36] [19/11/2019 20:33:36] Initializing function HTTP routes [19/11/2019 20:33:36] Mapped function route \u0026#39;api/greet\u0026#39; [POST] to \u0026#39;greet\u0026#39; [19/11/2019 20:33:36] [19/11/2019 20:33:36] Host initialized (213ms) [19/11/2019 20:33:36] Host started (218ms) [19/11/2019 20:33:36] Job host started [19/11/2019 20:33:36] Listening for transport dt_socket at address: 5005 Http Functions: greet: [POST] http://localhost:7071/api/greet Hosting environment: Production .. Now listening on: http://0.0.0.0:7071 Application started. Press Ctrl+C to shut down. [19/11/2019 20:33:43] Host lock lease acquired by instance ID \u0026#39;00000000000000000000000014D8753A\u0026#39;.   To consume our Function, just POST a User Payload to the shown URL:\n1  $ curl http://localhost:7071/api/greet -d \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Nebrass\\\u0026#34;}\u0026#34;   The response will look like:\n1 2 3  { \u0026#34;message\u0026#34;: \u0026#34;Hello Nebrass\u0026#34; }   Cool ! Our HTTP Request is now propagated from Azure Functions Host to the Spring Boot Context ü•≥\nHTTP Request propagation from Azure Functions to Spring Context\n We can now go further to do more tasks. We can for example make some advanced operations like inserting data to CosmosDB from the Spring Service and based on the Azure Functions Connection Strings üòÅ\nAccessing Cosmos DB from Spring Service Now, just add this Spring Data CosmosDB dependency to our pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-data-cosmosdb\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1.M1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Now, we need to create the CosmosDbConfiguration class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  @Configuration @EnableCosmosRepositories(basePackageClasses = StudentRepository.class) @Slf4j public class CosmosDBConfiguration extends AbstractCosmosConfiguration { @Value(\u0026#34;${CosmosDbConnection}\u0026#34;) private String key; @Value(\u0026#34;${azure.cosmosdb.database}\u0026#34;) private String dbName; @Bean public CosmosDBConfig getConfig() { CosmosDBConfig cosmosdbConfig = CosmosDBConfig.builder(key, dbName).build(); log.error(\u0026#34;key is {}\u0026#34;, key); log.error(\u0026#34;dbName is {}\u0026#34;, dbName); return cosmosdbConfig; } }   The @Value(\u0026quot;${CosmosDbConnection}\u0026quot;) will grab the value from the System Environment Variables. This value will be injected in the application environment from the Azure Functions Runtime based on our configuration in the Azure Portal or from the local.settings.json file when working locally.\nThe @Value(\u0026quot;${azure.cosmosdb.database}\u0026quot;) will grab this value from the application.properties - so let\u0026rsquo;s add it, it will be our CosmosDB Database Name, which I called university:\n1  azure.cosmosdb.database=university   The @EnableCosmosRepositories(basePackageClasses = StudentRepository.class) annotation will inject the CosmosDbConfiguration that we already created in the StudentRepository class that we need to create now üòÅ\n1 2 3 4  @Repository public interface StudentRepository extends CosmosRepository\u0026lt;Student, String\u0026gt; { List\u0026lt;Student\u0026gt; findByName(String name); }   It\u0026rsquo;s a very classic Spring Data Repository interface üòÜnothing special ! and this is the beauty of Spring Boot !\nNow we can inject our StudentRepository in any Spring Service class and enjoy its powerful features:\n1 2 3 4 5 6 7 8 9 10 11 12 13  @Service @Slf4j public class StudentsService { @Autowired private StudentsRepository studentsRepository; public void countUsers() { List\u0026lt;User\u0026gt; studentsList = new ArrayList\u0026lt;\u0026gt;(); this.studentsRepository.findAll().forEach(studentsList::add); log.info(\u0026#34;We have {} records in the Students DB\u0026#34;, studentsList.size()); } }   So simple !! In this way, you can enjoy all the features and the facilities of the Spring Boot in the Azure Functions Java projects !\nHave fun ! ü•≥\n","permalink":"https://blog.nebrass.fr/playing-with-spring-cloud-in-azure-functions/","summary":"In this tutorial, we will be experimenting a new use case of the great Azure Functions service üòÅ one of my favorite products in Azure.\nIn this tutorial, we will bring the powerful features of the Spring Framework to our Azure Functions Java projects. After this tutorial, creating a new Azure Functions Java based on business logic that you already have in your Spring Boot Application will be a very easy game.","title":"Playing with Spring Cloud in Azure Functions"},{"content":"I was for a week in Seattle \u0026amp; Redmond, WA, to attend the Microsoft CSE OneWeek Event, which is a global hackathon for all CSE Team worldwide, from the 1st to the 9th of November.\nIt\u0026rsquo;s my first visit to Seattle and Redmond. It was an amazing visit in very beautiful cities ü•≥\nThe places I visited:\n Olympic Sculpture Park, Seattle The Space Needle, Seattle The Fish Market, Seattle which is one of the oldest markets in the US The Microsoft Campus, Redmond üòçüòç The most wonderful place I ever visited. A full universe of Microsoft in a dedicated city ü•∞ One of oldest Starbucks stores in the US, which still have the original logo üòÅ Yeah ! with the naked lady üò≥ The International District, Seattle The Volunteer Park, Seattle the WaMu Theater, Seattle where the OneWeek Hackathon event was held ü•∞  It was really an amazing trip in a wonderful place. I feel like I felt in love with Seattle üòç\nLike every trip, here is a small Flickr album:\nhttps://www.flickr.com/photos/nebrass78/albums/72157711861398808\n","permalink":"https://blog.nebrass.fr/trip-report-seattle-trip-november-2019/","summary":"I was for a week in Seattle \u0026amp; Redmond, WA, to attend the Microsoft CSE OneWeek Event, which is a global hackathon for all CSE Team worldwide, from the 1st to the 9th of November.\nIt\u0026rsquo;s my first visit to Seattle and Redmond. It was an amazing visit in very beautiful cities ü•≥\nThe places I visited:\n Olympic Sculpture Park, Seattle The Space Needle, Seattle The Fish Market, Seattle which is one of the oldest markets in the US The Microsoft Campus, Redmond üòçüòç The most wonderful place I ever visited.","title":"Trip Report: Seattle Trip ‚Äì November 2019"},{"content":"In this post, I will share with you my experience starting from a disaster that occurred to my blog ‚ò†Ô∏èüò¢üò≠ until reaching the 100/100 performance score and get all Audits Validation Badges on Google Lighthouse üèÜü•áüéâüéäüéàüíØ/üíØü•≥\nOctober 10th, I was writing a new post in my blog, in one of the previews, the site kept loading without giving any response. üò±üò´üò∞üòìüò≥ü•µ\nMy blog is based on the Wordpress CMS and hosted in an Ubuntu VPS by OVH since August 28th, 2015.\nWhen I tried to connect to the VPS server using SSH but the connection was always refused. I tried to restart the server in the Rescue Mode and to reach it via the KVM Console. But this is was failing and I got a strange behavior from the VPS. It was stuck on the VM startup log:\nVPS Rescue Mode Startup Log\n So, I tried to connect in SSH to my VPS in the Rescue Mode. My last solution was to grab the Wordpress and MySQL files in hope to rescue the maximum amount of data.\nIn that moment, I discovered that I made a huge mistake when I didn\u0026rsquo;t setup a backup of my blog üò∞ I really felt stupid to forget to do that üò´üò¢\nI will spend so much time to tell you how the recovery was painful but fortunately:\n I had a one month backup JSON file of the posts that I generated the Ghost Blogs Migration Process. So I used that to rescue the articles. I could grabbed the Medias from the VPS Disk thru the Rescue Mode. I copy/pasted the ingoing \u0026ldquo;Playing With Java In Azure Functions\u0026rdquo; article in Word when I found that the Wordpress Preview button not working.  So, I started by reinstalling the OS of the VPS. I have chosen the Wordpress Image from OVH, which is a Debian customized image with some Wordpress dedicated stuff. In the previous VPS, it was just Ubuntu 15.04 and it was not dedicated for hosting a blog. I brought it to do some Java EE programming on a Remote Server. When I got the idea of blogging, I thought in hosting my blog on my Programming Server. Which was really a very bad idea, but it was economic solution: less than 10$/month.\nI started configuring the new Wordpress instance. I imported all my Posts and Medias. I moved next to install plugins. I took into consideration to have a backup mechanism for my blog. After some heavy searches about the plugins to have, my final list is:\n UpdraftPlus Backup/Restore üëâ Backup and restore: take backups locally, or backup to Amazon S3, Dropbox, Google Drive, Rackspace, (S)FTP, WebDAV \u0026amp; email, on automatic schedules. Akismet Anti-Spam All In One SEO Pack üëâ Out-of-the-box SEO for WordPress AMP üëâ Enable AMP on your WordPress site, the WordPress way. Auto Image Attributes From Filename With Bulk Updater üëâ Automatically Add Image Title, Image Caption, Description And Alt Text From Image Filename Broken Link Checker üëâ Checks your blog for broken links and missing images and notifies you on the dashboard if any are found. Classic Editor üëâ Enables the WordPress classic editor and the old-style Edit Post screen with TinyMCE, Meta Boxes, etc. Disable Comments üëâ Allows administrators to globally disable comments on their site. Comments can be disabled according to post type. Export Media Library üëâ Allows admins to export media library files as a compressed zip archive. Google Analytics Dashboard for WP üëâ Displays Google Analytics Reports and Real-Time Statistics in your Dashboard. Automatically inserts the tracking code in every page of your website. jQuery Manager for WordPress üëâ Manage jQuery and jQuery Migrate, activate a specific jQuery and/or jQuery Migrate version. The ultimate jQuery debugging tool for WordPress. This plugin is an open source project, made possible by your contribution (code). Development is done on GitHub. Mivhak Syntax Highlighter üëâ A lightweight syntax highlighter for WordPress, fully integrated into the TinyMCE rich editor. Really Simple SSL üëâ Lightweight plugin without any setup to make your site SSL proof Schema üëâ The next generation of Structured Data. Super Progressive Web Apps üëâ Convert your WordPress website into a Progressive Web App Swap Google Fonts Display üëâ Inject font-display: swap to Google Fonts to ensure text remains visible during webfont load TinyMCE Advanced üëâ Enables advanced features and plugins in TinyMCE, the visual editor in WordPress. WordPress Importer üëâ Import posts, pages, comments, custom fields, categories, tags and more from a WordPress export file. WP Mail SMTP üëâ Reconfigures the wp_mail() function to use Gmail/Mailgun/SendGrid/SMTP instead of the default mail() and creates an options page to manage the settings. WP Fastest Cache üëâ The simplest and fastest WP Cache system  Add to that I configured Cloudflare to boost the speed of my Wordpress Blog. After some tweaks and configuration, the scores started growing up üòÅ\nTo measure the efficiency of websites, Google has a great tool called Lighthouse.\nGoogle Lighthouse logo\n Google defines Lighthouse as:\n Lighthouse is an¬†open-source, automated tool for improving the quality of web pages. You can run it against any web page, public or requiring authentication. It has audits for performance, accessibility, progressive web apps, and more.\nYou can run Lighthouse in Chrome DevTools, from the command line, or as a Node module. You give Lighthouse a URL to audit, it runs a series of audits against the page, and then it generates a report on how well the page did. From there, use the failing audits as indicators on how to improve the page. Each audit has a reference doc explaining why the audit is important, as well as how to fix it.\n\u0026ndash; Source: https://developers.google.com/web/tools/lighthouse\n After the hard job, that took 2 or 3 days of hard work in Performance, Accessibility, Speed and other, I successfully attended the 100/100 on the Google Lighthouse Audits Reports.\nLighthouse Audit Results\n Now, I have a real strong automated blog with:\n regular daily automatic backup of all the blog content backed up in the Google Drive ü•≥ automatic broken links detection and notification ü•≥ mailing notification based on GMail instead of the Hosting Provider one ü•≥ accreditation that the blog responds to the most of the best practices ü•≥ accreditation that the blog responds to accessibility standards ü•≥ accreditation that the blog is very performant ! ü•≥ the blog now has its Progressive Web Application version that is accessible offline ! ü•≥  I\u0026rsquo;m very happy with this results ! I\u0026rsquo;m glad that I escaped the crash that had my blog üò£ it\u0026rsquo;s my fault from the beginning that I didn\u0026rsquo;t make an automatic backup process since a moment. But, I took this opportunity to review all the blog design and I deployed all the necessary elements to make it work like a charm ! üòÅ\n","permalink":"https://blog.nebrass.fr/celebrating-the-100-points-in-the-google-lighthouse-reports/","summary":"In this post, I will share with you my experience starting from a disaster that occurred to my blog ‚ò†Ô∏èüò¢üò≠ until reaching the 100/100 performance score and get all Audits Validation Badges on Google Lighthouse üèÜü•áüéâüéäüéàüíØ/üíØü•≥\nOctober 10th, I was writing a new post in my blog, in one of the previews, the site kept loading without giving any response. üò±üò´üò∞üòìüò≥ü•µ\nMy blog is based on the Wordpress CMS and hosted in an Ubuntu VPS by OVH since August 28th, 2015.","title":"Celebrating the 100 points in the Google Lighthouse Reports"},{"content":"In one of the previous posts, I introduced the Azure Functions Java. I felt that I need to write a dedicated tutorial to this great Azure Serverless service üòÅ\nIn this post, I will be covering many concepts in deep:\n Triggers¬†and¬†bindings Events and messaging Deployments \u0026amp; Consumptions Monitoring  Generating the hello-world project We will scaffold a Java-based Azure Function project using Maven Archetypes, using this command:\n1 2 3 4 5 6 7 8 9  $ mvn archetype:generate -DarchetypeGroupId=com.microsoft.azure \\  -DarchetypeArtifactId=azure-functions-archetype \\  -DappName=hello-world-app-example \\  -DappRegion=FranceCentral \\  -DresourceGroup=helloworld-rg \\  -DgroupId=com.helloworld.group \\  -DartifactId=helloworld-functions \\  -Dpackage=com.helloworld \\  -DinteractiveMode=false   The generated project will look like:\n The sample function looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  /** * Azure Functions with HTTP Trigger. */ public class Function { @FunctionName(\u0026#34;HttpTrigger-Java\u0026#34;) public HttpResponseMessage run( @HttpTrigger(name = \u0026#34;req\u0026#34;, methods = {HttpMethod.GET, HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); // Parse query parameter  String query = request.getQueryParameters().get(\u0026#34;name\u0026#34;); String name = request.getBody().orElse(query); if (name == null) { return request .createResponseBuilder(HttpStatus.BAD_REQUEST) .body(\u0026#34;Please pass a name on the query string or in the request body\u0026#34;) .build(); } else { return request .createResponseBuilder(HttpStatus.OK) .body(\u0026#34;Hello, \u0026#34; + name) .build(); } } }   You can grab the final sample project from GitHub.\nWe need to create the Function Application on the Azure portal. I already mentioned how to do that on the previous Azure Functions tutorial ü§ì\nBuilding the Azure Functions Application Before running the project, you need to build it: mvn package This Maven command will do the package-functions operation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  [INFO] ... [INFO] --- azure-functions-maven-plugin:1.3.4:package (package-functions) @ helloworld-functions --- [INFO] [INFO] Step 1 of 7: Searching for Azure Functions entry points [INFO] 1 Azure Functions entry point(s) found. [INFO] [INFO] Step 2 of 7: Generating Azure Functions configurations [INFO] Generation done. [INFO] [INFO] Step 3 of 7: Validating generated configurations [INFO] Validation done. [INFO] [INFO] Step 4 of 7: Saving empty host.json [INFO] Successfully saved to /Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-example/host.json [INFO] [INFO] Step 5 of 7: Saving configurations to function.json [INFO] Starting processing function: HttpTrigger-Java [INFO] Successfully saved to /Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-example/HttpTrigger-Java/function.json [INFO] [INFO] Step 6 of 7: Copying JARs to staging directory/Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-example [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 1 resource to /Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-example [INFO] Copied successfully. [INFO] Step 7 of 7: Installing function extensions if needed [INFO] Extension bundle specified, skip install extension [INFO] Successfully built Azure Functions.   This command will create an azure-functions folder under the Maven target folder:\n In the Function.java class, shown in the sample code above, we have a function called HttpTrigger-Java, this is why we got a folder with the same name, with a function definition file called function.json file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;scriptFile\u0026#34; : \u0026#34;../helloworld-functions-1.0-SNAPSHOT.jar\u0026#34;, \u0026#34;entryPoint\u0026#34; : \u0026#34;com.helloworld.Function.run\u0026#34;, \u0026#34;bindings\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;httpTrigger\u0026#34;, \u0026#34;direction\u0026#34; : \u0026#34;in\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;req\u0026#34;, \u0026#34;methods\u0026#34; : [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34; ] }, { \u0026#34;type\u0026#34; : \u0026#34;http\u0026#34;, \u0026#34;direction\u0026#34; : \u0026#34;out\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;$return\u0026#34; } ] }    The scriptFile is pointing on the packaged application JAR file. the entryPoint is pointing on the run method of the Function class; where the Azure Function is defined. The first element in the¬†bindings array is the HTTP Trigger. The type¬†and¬†direction¬†properties identify the trigger. The¬†name property identifies the function parameter that receives the request content. The methods array will list the HTTP verbs that are identifying the HTTP Request. The second element in the bindings array is the HTTP output binding. The type¬†and¬†direction¬†properties identify the binding. The¬†name property specifies how the function provides the response, in this case by using the function return value. This means, when we do a return \u0026quot;Hello World\u0026quot;; this String will be emitted as an HTTP Response.  Running \u0026amp; deploying the Azure Functions Application To run the project locally just do: mvn azure-functions:run\nTo deploy the project to your Azure Subscription: mvn azure-functions:deploy\n The azure-functions:deploy command needs that your Azure CLI have to be authenticated to your subscription ü§ì\n Triggers and bindings Triggers¬†are what cause a function to run. A¬†trigger¬†defines how a function is invoked and a function must have exactly one¬†trigger.¬†Triggers¬†have associated data, which is often provided as the payload of the function.\nBinding¬†to a function is a way of declaratively connecting another resource to the function;¬†bindings¬†may be connected as¬†input bindings,¬†output bindings, or both. Data from¬†bindings¬†is provided to the function as parameters.\nWe can mix and match different bindings¬†to suit your needs.¬†Bindings¬†are optional and a function might have one or multiple input and/or output¬†bindings.\nWith Triggers¬†and¬†bindings, we can avoid hardcoding access to other services. Our function receives data (for example, the content of a queue message) in function parameters. We send data (for example, to create a queue message) by using the return value of the function.\nHow works the triggers and Bindings ? ü§î I found a great animated GIF in the Azure Functions documentation that describes how Triggers and Bindings work ?\nIn Azure Functions, bindings are available as separate packages from the functions runtime. Extension bundles allow other functions access to all bindings through a configuration setting. HTTP and timer triggers are supported by default and don\u0026rsquo;t require an extension.\nExtension bundles for local development Extension bundles is a local development technology that helps us to add a compatible set of Functions binding extensions to our Azure Functions project.\nWhen used, these extension packages will be included in the deployment package when we deploy it to Azure. To avoid conflicts between packages, the Bundles guarantee that¬†the packaged extension inside are compatible with each other.\n To develop Azure Functions application locally, we need to have the latest version of Azure Functions Core Tools, which provide a local development experience for creating, developing, testing, running, and debugging Azure Functions locally.\n The host.json file included in the generated project skull enables the Extension Bundle:\n1 2 3 4 5 6 7  { \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;extensionBundle\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.Functions.ExtensionBundle\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;[1.*, 2.0.0)\u0026#34; } }   This Extension Bundle includes these extensions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  [ { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.Storage\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.4\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.ServiceBus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.3\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.EventHubs\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.3\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.SendGrid\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.0\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.DurableTask\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.8.3\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.EventGrid\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.CosmosDB\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.3\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.Twilio\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.0.0\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;Microsoft.Azure.WebJobs.Extensions.SignalRService\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; } ]   Great ! Now, we saw how things work and what is the idea behind the Azure Functions?\nNow, we will discuss our demo scenario.\nThe demo scenario In our demo will be dealing with Azure Functions that are dealing with Cosmos DB and Service Bus.\nFor development cycles, we will be based on Azure DevOps as mentioned in this post.\n In the next steps, we will create:\n a Cosmos DB account a Storage account a Service Bus namespace  Create Azure Cosmos DB Account We need to create a CosmosDB account:\n Click on Review + create and confirm the creation.\nAfter creating the CosmosDB account, we need to create our Database and our Container (some thing like the SQL Table üòÅ )\nTo do that, we need to go to the Overview section of the recently Cosmos DB Account, and click Add Container:\n Next, fill the creation form :\n Database id: university Container id: students Partition key: /id   Now, we need to plug the Cosmos DB connection to our application.\nFirst of all, we need to get the Cosmos DB Connection Key and next we will plug it in two locations:\n in the Azure Function Application portal in the local development project  Okey üòÅ let\u0026rsquo;s do that !\ngetting the Cosmos DB Connection key First of all, we need to go to the Keys menu, next choose the Read-write Keys section and copy the Primary Connection String, this property will be used in the Azure Function Application to access the Cosmos DB:\n Next, we will use this connection key in the next steps..\n1) Plug the connection Key to the Azure Function Application portal To plug the Connection Key to the Azure Function Application, we need to go to our Azure Function dashboard, and click on the Configuration menu:\n Next, we will access the Configuration listing:\n Now, click on the New application setting and paste the Connection Key to the form :\n After filling the form and validating the creation, you will be back to the Configuration listing, you need to click on Save to confirm the creation.\n2) Plug the connection Key to the local development project We will go back to the source code project. We will add this Connection Key to the l_ocal.settings.json_ file. The file will look like:\n1 2 3 4 5 6 7 8  { \u0026#34;IsEncrypted\u0026#34;: false, \u0026#34;Values\u0026#34;: { \u0026#34;AzureWebJobsStorage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;CosmosDbConnection\u0026#34;: \u0026#34;AccountEndpoint=https://hello-world-cosmos-db-account.documents.azure.com:443/;AccountKey=GHXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX==;\u0026#34;, \u0026#34;FUNCTIONS_WORKER_RUNTIME\u0026#34;: \u0026#34;java\u0026#34; } }   That\u0026rsquo;s all for the Cosmos DB configuration part ! üòÅ\nAs you see, we have already an AzureWebJobsStorage entry that did not have a value, but in the Configuration listing we have already this entry in the Application Settings; just click on¬†the Show values button to show the values. Next, we need to copy this value and past it into the l_ocal.settings.json_ file:\n1 2 3 4 5 6 7 8  { \u0026#34;IsEncrypted\u0026#34;: false, \u0026#34;Values\u0026#34;: { \u0026#34;AzureWebJobsStorage\u0026#34;: \u0026#34;DefaultEndpointsProtocol=https;AccountName=helloworldexamplestorage;AccountKey=vs3aXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX==;EndpointSuffix=core.windows.net\u0026#34;, \u0026#34;CosmosDbConnection\u0026#34;: \u0026#34;AccountEndpoint=https://hello-world-cosmos-db-account.documents.azure.com:443/;AccountKey=GHXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX==;\u0026#34;, \u0026#34;FUNCTIONS_WORKER_RUNTIME\u0026#34;: \u0026#34;java\u0026#34; } }   Great ! But from where comes this value? Why it did come automatically ?\nIf you go back and look to the Azure Function creation form, you will notice that there already a Storage creation element in the form:\n So the AzureWebJobsStorage entry comes from the create Azure Storage account.\nNow, we need to move to creating the Azure Service Bus namespace üòé\nCreate Azure Service bus namespace We need to create a new Azure Service Bus namespace:\n Click on Add or Create namespace to go to the creation form:\n Click on Create and confirm the creation.\nNext, we will create a Queue on which we will be sending some messages.¬†To do that, we need to click on + Queue:\n The Queue creation form:\n  Name: hello-world-app-queue Max queue size: 1 GB Time To Live: 14 Days We need to enable:  Enable dead lettering on message expiration ‚ÑπÔ∏è Dead lettering messages involves holding messages that cannot be successfully delivered to any receiver to a separate queue after they have expired. Messages do not expire in the dead letter queue, and it supports peek-lock delivery and all transactional operations.     A dedicated tutorial about sessions will come soon üòé ‚ÑπÔ∏è Service bus sessions allow ordered handling of unbounded sequences of related messages. With sessions enabled a queue can guarantee first-in-first-out delivery of messages.\n Now, we need to connect our Azure Functions with the Service Bus. To do that, we will do as we did with the Cosmos DB and the Storage Account: on the Service Bus Namespace dashboard, we need to go to the Shared access policies. Next, double click on the RootManageSharedAccessKey policy to show the keys window.\n In this window, just click on the Copy button of the Primary Connection String.\nNext, go back to the Configuration listing menu of our Azure Functions Application. We will add the Connection String as a setting called ServiceBusConnection:\n Next, we will need to add this same setting to our local.settings.json:\n1 2 3 4 5 6 7 8 9  { \u0026#34;IsEncrypted\u0026#34;: false, \u0026#34;Values\u0026#34;: { \u0026#34;AzureWebJobsStorage\u0026#34;: \u0026#34;DefaultEndpointsProtocol=https;AccountName=helloworldexamplestorage;AccountKey=vsXXXXXXXXXXXXXXXXXXXX==;EndpointSuffix=core.windows.net\u0026#34;, \u0026#34;CosmosDbConnection\u0026#34;: \u0026#34;AccountEndpoint=https://hello-world-cosmos-db-account.documents.azure.com:443/;AccountKey=GHXXXXXXXXXXXXXXXXXXXX==;\u0026#34;, \u0026#34;ServiceBusConnection\u0026#34;: \u0026#34;Endpoint=sb://hello-world-app-example.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=wc0XXXXXXXXXXXXXXXXXXXX=\u0026#34;, \u0026#34;FUNCTIONS_WORKER_RUNTIME\u0026#34;: \u0026#34;java\u0026#34; } }   Perfect ! Let\u0026rsquo;s go back to do some Java üòÅ\nIf we look again to our scenario diagram, we need to write 3 functions:\n HTTP to Cosmos DB function HTTP to Service Bus function Service Bus Message to Cosmos DB function  In our sample project, we will be inserting a Student record into Cosmos DB and we will be pushing a Student message to Service Bus.\nThe Student class will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  public class Student { private String id; private String name; private String email; public Student(String id, String name, String email) { this.id = id; this.name = name; this.email = email; } public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } @Override public String toString() { return \u0026#34;{\\\u0026#34;id\\\u0026#34;:\\\u0026#34;\u0026#34; + id + \u0026#34;\\\u0026#34;, \\\u0026#34;name\\\u0026#34;:\\\u0026#34;\u0026#34; + name + \u0026#34;\\\u0026#34;, \\\u0026#34;email\\\u0026#34;:\\\u0026#34;\u0026#34; + email + \u0026#34;\\\u0026#34;}\u0026#34;; } }   HTTP to Cosmos DB function This function will have as input a Name and Email address, requested using the HTTP Post verb.¬†After parsing the request payload, this function will create a record of a Student in Cosmos DB.\nThe sample HTTP Hello World Function shown in the beginning of this tutorial, will be a good starting point:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  /** * Azure Functions with HTTP Trigger. */ public class Function { @FunctionName(\u0026#34;HttpTrigger-Java\u0026#34;) public HttpResponseMessage run( @HttpTrigger(name = \u0026#34;req\u0026#34;, methods = {HttpMethod.GET, HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); // Parse query parameter  String query = request.getQueryParameters().get(\u0026#34;name\u0026#34;); String name = request.getBody().orElse(query); if (name == null) { return request .createResponseBuilder(HttpStatus.BAD_REQUEST) .body(\u0026#34;Please pass a name on the query string or in the request body\u0026#34;) .build(); } else { return request .createResponseBuilder(HttpStatus.OK) .body(\u0026#34;Hello, \u0026#34; + name) .build(); } } }   In the run() method, we will add an other parameter: the Cosmos DB Binding üòÅ the Azure Functions connector to Cosmos DB ü§ì\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  @FunctionName(\u0026#34;CosmosDBStoreBinding\u0026#34;) public HttpResponseMessage run( @HttpTrigger( name = \u0026#34;req\u0026#34;, methods = {HttpMethod.POST}, 1Ô∏è‚É£ authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, @CosmosDBOutput( name = \u0026#34;database\u0026#34;, databaseName = \u0026#34;university\u0026#34;, collectionName = \u0026#34;students\u0026#34;, connectionStringSetting = \u0026#34;CosmosDbConnection\u0026#34;) 2Ô∏è‚É£ OutputBinding\u0026lt;String\u0026gt; outputItem, final ExecutionContext context) { ... }   1Ô∏è‚É£ I only kept the HTTP Post verb - I don\u0026rsquo;t want that the function get called using the HTTP Get verb.\n2Ô∏è‚É£ Here we are defining an OutputBinding parameter, that will hold the computing result to Cosmos DB. That\u0026rsquo;s why this parameter is annotated using the @CosmosDBOutput.\nThe @CosmosDBOutput annotation has these main attributes:\n name: The variable name used in function.json üëâ Here we will use \u0026ldquo;database\u0026rdquo; databaseName: Defines the database name of the CosmosDB to which to write üëâ Here we will use \u0026ldquo;university\u0026rdquo; collectionName: Defines the collection name of the CosmosDB to which to write üëâ Here we will use \u0026ldquo;students\u0026rdquo; connectionStringSetting: Defines the app setting name that contains the CosmosDB connection string- This is the same attribute that we already defined in the local.settings.json and also in the Configuration section of the Azure Function in the Azure Subscription üëâ Here we will use \u0026ldquo;CosmosDbConnection\u0026rdquo;  Next, in the method body, we will proceed to\n extract the request parameters create a Student record convert the record to JSON set its value to the OutputBinding make an HTTP Response to the caller  The complete method will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  @FunctionName(\u0026#34;CosmosDBStoreBinding\u0026#34;) public HttpResponseMessage run( @HttpTrigger( name = \u0026#34;req\u0026#34;, methods = {HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, @CosmosDBOutput( name = \u0026#34;database\u0026#34;, databaseName = \u0026#34;university\u0026#34;, collectionName = \u0026#34;students\u0026#34;, connectionStringSetting = \u0026#34;CosmosDbConnection\u0026#34;) OutputBinding\u0026lt;String\u0026gt; outputItem, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); String name = \u0026#34;empty\u0026#34;; String email = \u0026#34;empty\u0026#34;; // Parse query parameter  if (request.getBody().isPresent()) { JSONObject jsonObject = new JSONObject(request.getBody().get()); name = jsonObject.getString(\u0026#34;name\u0026#34;); email = jsonObject.getString(\u0026#34;email\u0026#34;); } // Generate random ID  final String id = String.valueOf(Math.abs(new Random().nextInt())); // Generate document  Student student = new Student(id, name, email); final String database = JSONWriter.valueToString(student); context.getLogger().info(String.format(\u0026#34;Document to be saved in DB: %s\u0026#34;, database)); outputItem.setValue(database); // return the document to calling client.  return request.createResponseBuilder(HttpStatus.OK) .body(database) .build(); }   Here, we used the Bindings explicitly, we can do the same thing, just by annotating the Function itself using the @OutputBinding - as we will not have an OutputBinding object to assign the value to it; we will have to return the value that we want to insert into Cosmos DB as a return value of run() method. The function can be written like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  @FunctionName(\u0026#34;CosmosDBStoreAnnotation\u0026#34;) @CosmosDBOutput( name = \u0026#34;database\u0026#34;, databaseName = \u0026#34;university\u0026#34;, collectionName = \u0026#34;students\u0026#34;, connectionStringSetting = \u0026#34;CosmosDbConnection\u0026#34;) public String run( @HttpTrigger( name = \u0026#34;req\u0026#34;, methods = {HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); String name = \u0026#34;empty\u0026#34;; String email = \u0026#34;empty\u0026#34;; // Parse query parameter  if (request.getBody().isPresent()) { JSONObject jsonObject = new JSONObject(request.getBody().get()); name = jsonObject.getString(\u0026#34;name\u0026#34;); email = jsonObject.getString(\u0026#34;email\u0026#34;); } // Generate random ID  final String id = String.valueOf(Math.abs(new Random().nextInt())); // Generate document  Student student = new Student(id, name, email); final String database = new JSONObject(student).toString(); context.getLogger().info(String.format(\u0026#34;Document to be saved in DB: %s\u0026#34;, database)); return database; }   These two possible formats of the function will give the same result - just in the second one, we will not be able to define the HTTP Response to the caller client.\n‚ö†Ô∏è‚ö†Ô∏è At this level, the insertion in the Cosmos DB is not effective yet; at this stage, we don\u0026rsquo;t know if the creation will be done or may fail whatever the reason. This is why the Azure Functions have a verbose logging that will help us understand if things are going in the correct way or not.\nWhen you run the functions locally you can have these logs on your console:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  $ mvn azure-functions:run [INFO] Scanning for projects... [INFO] [INFO] -------------\u0026lt; com.helloworld.group:helloworld-functions \u0026gt;-------------- [INFO] Building Azure Java Functions 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- azure-functions-maven-plugin:1.3.4:run (default-cli) @ helloworld-functions --- [INFO] Azure Function App\u0026#39;s staging directory found at: /Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-app-example [INFO] Azure Functions Core Tools found. %%%%%% %%%%%% @ %%%%%% @ @@ %%%%%% @@ @@@ %%%%%%%%%%% @@@ @@ %%%%%%%%%% @@ @@ %%%% @@ @@ %%% @@ @@ %% @@ %% % Azure Functions Core Tools (2.7.1704 Commit hash: fbab3b9c1de5ab95e3b4b6a471ead62c4f37e89c) Function Runtime Version: 2.0.12742.0 ... [19/10/2019 17:20:13] Host initialized (201ms) [19/10/2019 17:20:13] Host started (236ms) [19/10/2019 17:20:13] Job host started Hosting environment: Production Content root path: /Users/nebrass/azure/helloworld-functions/target/azure-functions/hello-world-app-example Now listening on: http://0.0.0.0:7071 Application started. Press Ctrl+C to shut down. Http Functions: CosmosDBStoreAnnotation: [POST] http://localhost:7071/api/CosmosDBStoreAnnotation CosmosDBStoreBinding: [POST] http://localhost:7071/api/CosmosDBStoreBinding [19/10/2019 17:20:18] Host lock lease acquired by instance ID \u0026#39;0000000000000000000000009DB0E06C\u0026#39;.   Now, if we call one of the two available functions, the log will be displayed on the console.\nWe can call the CosmosDBStoreAnnotation function using the curl tool:\n1  $ curl -i -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://localhost:7071/api/CosmosDBStoreAnnotation   The curl reponse will look like:\n1  {\u0026#34;id\u0026#34;:\u0026#34;1465275753\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;}%   In the console, we will find some thing that looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [19/10/2019...] Executing HTTP request: { [19/10/2019...] \u0026#34;requestId\u0026#34;: \u0026#34;647ca147-3917-4323-951b-b0744c0e2367\u0026#34;, [19/10/2019...] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019...] \u0026#34;uri\u0026#34;: \u0026#34;/api/CosmosDBStoreAnnotation\u0026#34; [19/10/2019...] } [19/10/2019...] Executing \u0026#39;Functions.CosmosDBStoreAnnotation\u0026#39; (Reason=\u0026#39;This function was programmatically called via the host APIs.\u0026#39;, Id=7582d976-d517-4d43-9b12-f645464705e2) [19/10/2019...] Java HTTP trigger processed a request. [19/10/2019...] Document to be saved in DB: {\u0026#34;id\u0026#34;:\u0026#34;1465275753\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;} [19/10/2019...] Function \u0026#34;CosmosDBStoreAnnotation\u0026#34; (Id: 7582d976-d517-4d43-9b12-f645464705e2) invoked by Java Worker [19/10/2019...] Executed \u0026#39;Functions.CosmosDBStoreAnnotation\u0026#39; (Succeeded, Id=7582d976-d517-4d43-9b12-f645464705e2) [19/10/2019...] Executed HTTP request: { [19/10/2019...] \u0026#34;requestId\u0026#34;: \u0026#34;647ca147-3917-4323-951b-b0744c0e2367\u0026#34;, [19/10/2019...] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019...] \u0026#34;uri\u0026#34;: \u0026#34;/api/CosmosDBStoreAnnotation\u0026#34;, [19/10/2019...] \u0026#34;identities\u0026#34;: [ [19/10/2019...] { [19/10/2019...] \u0026#34;type\u0026#34;: \u0026#34;WebJobsAuthLevel\u0026#34;, [19/10/2019...] \u0026#34;level\u0026#34;: \u0026#34;Admin\u0026#34; [19/10/2019...] } [19/10/2019...] ], [19/10/2019...] \u0026#34;status\u0026#34;: 200, [19/10/2019...] \u0026#34;duration\u0026#34;: 1161 [19/10/2019...] }   In this case; we see that everything is going well.\nBut, if there is a problem, we will not get a response for the curl command and in the log we will get something like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  [19/10/2019..] Executing HTTP request: { [19/10/2019..] \u0026#34;requestId\u0026#34;: \u0026#34;28cb88b7-3a7a-4c96-bacc-f70349d30cd9\u0026#34;, [19/10/2019..] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019..] \u0026#34;uri\u0026#34;: \u0026#34;/api/CosmosDBStoreAnnotation\u0026#34; [19/10/2019..] } [19/10/2019..] Executing \u0026#39;Functions.CosmosDBStoreAnnotation\u0026#39; (Reason=\u0026#39;This function was programmatically called via the host APIs.\u0026#39;, Id=627d4d5d-6064-4322-aa2d-5ee1050f9a89) [19/10/2019..] Java HTTP trigger processed a request. [19/10/2019..] Document to be saved in DB: {\u0026#34;id\u0026#34;:\u0026#34;1234769986\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;} [19/10/2019..] Function \u0026#34;CosmosDBStoreAnnotation\u0026#34; (Id: 627d4d5d-6064-4322-aa2d-5ee1050f9a89) invoked by Java Worker [19/10/2019..] Executed \u0026#39;Functions.CosmosDBStoreAnnotation\u0026#39; (Failed, Id=627d4d5d-6064-4322-aa2d-5ee1050f9a89) [19/10/2019..] System.Private.CoreLib: Exception while executing function: Functions.CosmosDBStoreAnnotation. System.Private.CoreLib: The input is not a valid Base-64 string as it contains a non-base 64 character, more than two padding characters, or an illegal character among the padding characters. [19/10/2019..] Executed HTTP request: { [19/10/2019..] \u0026#34;requestId\u0026#34;: \u0026#34;28cb88b7-3a7a-4c96-bacc-f70349d30cd9\u0026#34;, [19/10/2019..] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019..] \u0026#34;uri\u0026#34;: \u0026#34;/api/CosmosDBStoreAnnotation\u0026#34;, [19/10/2019..] \u0026#34;identities\u0026#34;: [ [19/10/2019..] { [19/10/2019..] \u0026#34;type\u0026#34;: \u0026#34;WebJobsAuthLevel\u0026#34;, [19/10/2019..] \u0026#34;level\u0026#34;: \u0026#34;Admin\u0026#34; [19/10/2019..] } [19/10/2019..] ], [19/10/2019..] \u0026#34;status\u0026#34;: 500, [19/10/2019..] \u0026#34;duration\u0026#34;: 14 [19/10/2019..] }   We can easily notice that there is a problem in the execution, and even there is a {status: 500} for the HTTP Request, which is the same headers sent back to the curl command.\nGreat ! Let\u0026rsquo;s continue our trip ! ü§©\nHTTP to Service Bus function Based on the previous section, the HTTP to Service Bus Function will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  @FunctionName(\u0026#34;ServiceBusStoreAnnotation\u0026#34;) @ServiceBusQueueOutput( name = \u0026#34;hello-world-app-queue-output\u0026#34;, queueName = \u0026#34;hello-world-app-queue\u0026#34;, connection = \u0026#34;ServiceBusConnection\u0026#34;) public String run( @HttpTrigger( name = \u0026#34;req\u0026#34;, methods = {HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); String name = EMPTY; String email = EMPTY; // Parse query parameter  if (request.getBody().isPresent()) { JSONObject jsonObject = new JSONObject(request.getBody().get()); name = jsonObject.getString(\u0026#34;name\u0026#34;); email = jsonObject.getString(\u0026#34;email\u0026#34;); } // Generate random ID  final String id = String.valueOf(new Random().nextInt()); // Generate Student  Student student = new Student(id, name, email); final String message = student.toString(); context.getLogger().info(String.format(\u0026#34;Document to be sent to Queue: %s\u0026#34;, message)); return message; }   When called, the log of this function will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [19/10/2019..] Executing HTTP request: { [19/10/2019..] \u0026#34;requestId\u0026#34;: \u0026#34;a36738f3-e1fa-4c78-8600-7b4039382703\u0026#34;, [19/10/2019..] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019..] \u0026#34;uri\u0026#34;: \u0026#34;/api/ServiceBusStoreAnnotation\u0026#34; [19/10/2019..] } [19/10/2019..] Executing \u0026#39;Functions.ServiceBusStoreAnnotation\u0026#39; (Reason=\u0026#39;This function was programmatically called via the host APIs.\u0026#39;, Id=149530e6-d0c9-4c90-b582-726c6cac7cc0) [19/10/2019..] Java HTTP trigger processed a request. [19/10/2019..] Document to be sent to Queue: {\u0026#34;id\u0026#34;:\u0026#34;954440360\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;} [19/10/2019..] Function \u0026#34;ServiceBusStoreAnnotation\u0026#34; (Id: 149530e6-d0c9-4c90-b582-726c6cac7cc0) invoked by Java Worker [19/10/2019..] Executed \u0026#39;Functions.ServiceBusStoreAnnotation\u0026#39; (Succeeded, Id=149530e6-d0c9-4c90-b582-726c6cac7cc0) [19/10/2019..] Executed HTTP request: { [19/10/2019..] \u0026#34;requestId\u0026#34;: \u0026#34;a36738f3-e1fa-4c78-8600-7b4039382703\u0026#34;, [19/10/2019..] \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, [19/10/2019..] \u0026#34;uri\u0026#34;: \u0026#34;/api/ServiceBusStoreAnnotation\u0026#34;, [19/10/2019..] \u0026#34;identities\u0026#34;: [ [19/10/2019..] { [19/10/2019..] \u0026#34;type\u0026#34;: \u0026#34;WebJobsAuthLevel\u0026#34;, [19/10/2019..] \u0026#34;level\u0026#34;: \u0026#34;Admin\u0026#34; [19/10/2019..] } [19/10/2019..] ], [19/10/2019..] \u0026#34;status\u0026#34;: 200, [19/10/2019..] \u0026#34;duration\u0026#34;: 1300 [19/10/2019..] }   Everything is good ! No Bad News Is Good News¬†! As there is no errors means every thing is working like a charm ü•≥\nWe can check on the Azure Portal that there is a Message on the Service Bus Queue:\n Service Bus Message to Cosmos DB function Now, we will create the function that will be triggered by a message in the Service Bus Queue and will insert data to Cosmos DB. The function will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  @FunctionName(\u0026#34;ServiceBusMessageToCosmosDb\u0026#34;) public void run( @ServiceBusQueueTrigger( name = \u0026#34;messageTrigger\u0026#34;, queueName = \u0026#34;hello-world-app-queue\u0026#34;, connection = \u0026#34;ServiceBusConnection\u0026#34; ) String student, @CosmosDBOutput( name = \u0026#34;database\u0026#34;, databaseName = \u0026#34;university\u0026#34;, collectionName = \u0026#34;students\u0026#34;, connectionStringSetting = \u0026#34;CosmosDbConnection\u0026#34;) OutputBinding\u0026lt;String\u0026gt; outputItem, final ExecutionContext context) { context.getLogger().info(String.format(\u0026#34;Service Bus message trigger processed a request: %s\u0026#34;, student)); // This line will be used to validate the received JSON  String jsonValue = new JSONObject(student).toString(); context.getLogger().info(String.format(\u0026#34;Document to be saved in DB: %s\u0026#34;, jsonValue)); outputItem.setValue(jsonValue); }   When we run our Functions Application, the Message that is already in the Service Bus Queue will be consumed - the execution log:\n1 2 3 4 5 6 7 8 9 10  [19/10/2019..] Executing \u0026#39;Functions.ServiceBusMessageToCosmosDb\u0026#39; (Reason=\u0026#39;New ServiceBus message detected on \u0026#39;hello-world-app-queue\u0026#39;.\u0026#39;, Id=dcb8e81d-f09e-48ee-a236-49542ae2e34d) [19/10/2019..] Trigger Details: MessageId: f5a0a1d6e5e94fc790aed052217dda5a, DeliveryCount: 1, EnqueuedTime: 19/10/2019 17:47:39, LockedUntil: 19/10/2019 18:00:44 [19/10/2019..] Service Bus message trigger processed a request: { [19/10/2019..] \u0026#34;id\u0026#34;: \u0026#34;954440360\u0026#34;, [19/10/2019..] \u0026#34;name\u0026#34;: \u0026#34;nebrass\u0026#34;, [19/10/2019..] \u0026#34;email\u0026#34;: \u0026#34;lnibrass@gmail.com\u0026#34; [19/10/2019..] } [19/10/2019..] Document to be saved in DB: {\u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;954440360\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;} [19/10/2019..] Function \u0026#34;ServiceBusMessageToCosmosDb\u0026#34; (Id: dcb8e81d-f09e-48ee-a236-49542ae2e34d) invoked by Java Worker [19/10/2019..] Executed \u0026#39;Functions.ServiceBusMessageToCosmosDb\u0026#39; (Succeeded, Id=dcb8e81d-f09e-48ee-a236-49542ae2e34d)   Yooppi ü•≥ every thing is working like a charm ! ü•∞\nNow, we will see how to debug our Azure Functions Java Application üòÅ\nDEBUGGING the Azure Functions Application Locally To debug the application locally just do: mvn clean package azure-functions:run -DenableDebug to run the Azure Functions application in the Debugging Mode.\nIn you IDE, you need to attach a debugger to the Host: localhost and the Port: 5005.\n  For NetBeans In the Debug menu, click on Attach Debugger:\n Next, in the Attach Debugger window, we need to make localhost as Host and 5005 as Port:\n Next, try to put a breakpoint and call your application, you will see your application pause on your breakpoint:\n   For IntelliJ In the Run menu, select Edit Configurations, next click on the (+) button.. On the Configuration section:\n Host: localhost Port: 5005 Cmd line arguments: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005   Now, you can put a breakpoint in your code, start the application in Debug Mode, run the Remote Debug configuration that you just created in IntelliJ. When you invoke one of the functions, you will see the application pause on the breakpoint.\n   Cool ! Let\u0026rsquo;s move to the deployment part ! ü§©\nDeploying the Azure Functions Java Application Let\u0026rsquo;s deploy our Azure Functions Application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ mvn package azure-functions:deploy ... [INFO] --- azure-functions-maven-plugin:1.3.4:deploy (default-cli) @ helloworld-functions --- [INFO] Authenticate with Azure CLI 2.0 [INFO] Updating the specified function app... [INFO] Java version of function host : 1.8 [INFO] Set function worker runtime to java [INFO] Successfully updated the function app.hello-world-app-example [INFO] Trying to deploy the function app... [INFO] Trying to deploy artifact to hello-world-app-example... [INFO] Successfully deployed the artifact to https://hello-world-app-example.azurewebsites.net [INFO] Successfully deployed the function app at https://hello-world-app-example.azurewebsites.net ...   Great! Our application is deployed ! ü•≥ü•≥\nLet\u0026rsquo;s enjoy our functions ! ü§©\nConsuming the Deployed Azure Functions Application In the deployment log, we see that our Functions App is deployed to https://hello-world-app-example.azurewebsites.net¬†.\nLet\u0026rsquo;s try our curl command on this host:\n1 2  $ curl -i -X POST https://hello-world-app-example.azurewebsites.net/api/ServiceBusStoreAnnotation -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;}\u0026#39;   We will get something like:\n1 2 3 4 5 6  HTTP/1.1 401 Unauthorized Set-Cookie: ARRAffinity=8ba760adb9686d1725150fb81799ab65bc89ea938e47cc3c421bf92dc2df040a;Path=/;HttpOnly;Domain=hello-world-app-example.azurewebsites.net Request-Context: appId=cid-v1:e3232f0f-c61a-48c2-92b8-537739c295b6 Set-Cookie: ARRAffinity=5d57fdaf326a15958b2bfa6786e779beb79860d9cad630e44e1343796e5fac76;Path=/;HttpOnly;Domain=hello-world-app-example.azurewebsites.net Date: Sat, 19 Oct 2019 18:20:29 GMT Content-Length: 0   HTTP 401 Unauthorized üò≥üò±üò≠üò∞üò£üò´ !!!\nBut from where comes the Authorization ? Did we secured our Functions ??\nActually, yes and this comes in the generated function sample. In the method signature, there is a @HttpTrigger¬†annotation with an attribute authLevel=AuthorizationLevel.FUNCTION¬†. In the Javadoc of this annotation, we see that authLevel determines what keys, if any, need to be present on the request in order to invoke the function. The authorization level can be one of the following values:\n anonymous: No API key is required. function: A function-specific API key is required. This is the default value if none is provided. admin: The master key is required.  Good ! So we need to have an API Key to be able to consume our Functions. Great, but from where we can get that? ü§ì\nKEEP CALM ! üò§ Everything is easy to find in the Azure Portal üòÅ\nGo to the Azure Functions Application, then click on Function App Settings:\n Next, in this window, you will find the default Host Keys:\n  The Azure Functions API keys can be either:\n a Host Key : an API Key that can be used with all the functions in the same application a Function Key: an API Key that can be used for only an assigned the function   Based on this definition, if we want to generate a Function Key, we need to go to the Function, next, click on Manage:\n Next, after you copy the Key, you need to include it to the request. We can do that in several ways:\n in the URL: https://APP_URL/api/CosmosDBStoreAnnotation?code=XXXXXXXXXXXXXXXXXXXXXXX in the headers, in a format: Key=x-functions-key and Value=XXXXXXXXXXXXXXXXXXXXXXX  Great ! Let\u0026rsquo;s test the curl again:\n1 2  $ curl -i -X POST https://hello-world-app-example.azurewebsites.net/api/ServiceBusStoreAnnotation?code=XXXXXXXXXXXXXXXXXXX== -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;}\u0026#39;   1 2 3 4 5 6 7 8 9  HTTP/1.1 200 OK Content-Length: 68 Content-Type: text/plain; charset=utf-8 Set-Cookie: ARRAffinity=aaaaaabbbbbbbbcccccccdddddeeeeeefffffff;Path=/;HttpOnly;Domain=hello-world-app-example.azurewebsites.net Request-Context: appId=cid-v1:12345678-abcd-efgh-xywz-537739c295b6 Set-Cookie: ARRAffinity=wwwwwwxxxxxxxxvvvvvvvuuuuuiiiiiihhhhhhh;Path=/;HttpOnly;Domain=hello-world-app-example.azurewebsites.net Date: Sat, 19 Oct 2019 20:03:48 GMT {\u0026#34;id\u0026#34;:\u0026#34;1251989744\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;nebrass\u0026#34;, \u0026#34;email\u0026#34;:\u0026#34;lnibrass@gmail.com\u0026#34;}%   Youuppiiii !!! üòÅ\nMonitoring the Azure Functions Application When we created our application, we created an Application Insights account. To access the Azure Functions Monitoring, you need to go to the Function, next you click on the Monitor menu. Here you will find the latest requests with a small stats about the successful and failing ones:\n To check more, you can click on a request to see more details. Or even you can click on:\n Live app metrics to access the Metrics dashboard for our Functions application.    Run in Application Insights to access the detailed Logs and Log Analytics platform for our Functions application.   Cool ! As you see, we can do many crazy things easily with the Azure Functions, in a very efficient way. No hard configuration, no crazy yaml configuration files..\nThe Azure Functions is one of the most wonderful products that I like in the Azure world ü§© I really will try to go deep more and more in this great service. I will try to write an other tutorial when needed.\nFeel free to try in deep this tutorial and I will be interested to get your feedback. If you have a request or any inquiry, feel free to get in touch with me üòÑ\n","permalink":"https://blog.nebrass.fr/playing-with-java-in-azure-functions-new-release/","summary":"In one of the previous posts, I introduced the Azure Functions Java. I felt that I need to write a dedicated tutorial to this great Azure Serverless service üòÅ\nIn this post, I will be covering many concepts in deep:\n Triggers¬†and¬†bindings Events and messaging Deployments \u0026amp; Consumptions Monitoring  Generating the hello-world project We will scaffold a Java-based Azure Function project using Maven Archetypes, using this command:","title":"Playing with Java in Azure Functions - New Release"},{"content":"I was for a week in Amsterdam, Netherlands, to attend the Microsoft OpenHack about the Migrating Workloads to Azure from the 14th to the 18th of October.\nIt\u0026rsquo;s not my first visit to Amsterdam, and like every time, it was an amazing visit ü•≥\nLike every trip, here is a small Flickr album:\nhttps://www.flickr.com/photos/nebrass78/albums/72157711449636051\n","permalink":"https://blog.nebrass.fr/trip-report-netherlands-trip-october-2019/","summary":"I was for a week in Amsterdam, Netherlands, to attend the Microsoft OpenHack about the Migrating Workloads to Azure from the 14th to the 18th of October.\nIt\u0026rsquo;s not my first visit to Amsterdam, and like every time, it was an amazing visit ü•≥\nLike every trip, here is a small Flickr album:\nhttps://www.flickr.com/photos/nebrass78/albums/72157711449636051","title":"Trip Report: Netherlands Trip - October 2019"},{"content":"I would like to thank JetBrains for offering me the complimentary JetBrains All Products Pack subscription as part of the JetBrains Developer Recognition Program ü§©\nSay hello to my IntelliJ IDEA Ultimate\n ","permalink":"https://blog.nebrass.fr/thank-you-jetbrains-for-the-complimentary-subscription/","summary":"I would like to thank JetBrains for offering me the complimentary JetBrains All Products Pack subscription as part of the JetBrains Developer Recognition Program ü§©\nSay hello to my IntelliJ IDEA Ultimate\n ","title":"Thank you JetBrains for the complimentary subscription ! ü§©"},{"content":"In enterprise application, performance is major requirement of success. Especially for applications where slowness will have a direct detrimental impact on business productivity, profits and even the brand itself, like trading platforms and e-commerces.\nIf a trading platform loads slowly or experiences errors, it will translate into loss of business, and losses can be extremely high, and the customer might end up switching to another competitor.\nIn this context comes the Application Performance Management¬†(APM). APM is the monitoring and management of performance and availability of applications. APM strives to detect and diagnose complex application performance problems to maintain an expected¬†level of service.\nOn this article, I am going to explain how to use¬†Azure Monitor which is the Microsoft Azure\u0026rsquo;s product that helps you maximize performance and availability of your applications and proactively identify problems in seconds.\nWhat\u0026rsquo;s Azure Monitor ? Azure Monitor maximizes the availability and performance of your applications and services by delivering a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. It helps you understand how your applications are performing and proactively identifies issues affecting them and the resources they depend on.\nHow Azure Monitor works Azure Monitor collects monitoring telemetry from a variety of on-premises and Azure sources. Management tools, such as those in Azure Security Center and Azure Automation, also push log data to Azure Monitor. The service aggregates and stores this telemetry in a log data store that‚Äôs optimized for cost and performance. Analyze data, set up alerts, get end-to-end views of your applications, and use machine learning‚Äìdriven insights to quickly identify and resolve problems.\nIn this tutorial we will be covering :\n Azure Application Insights: monitors your running web app. It tells you about failures and performance issues, and helps you analyze how customers use your app. Azure Monitor Log: set of useful tools for performing complex analysis across Log Data from a variety of sources.  PREPARING THE SAMPLE PROJECT The sample project for this tutorial is generated using the Spring Initializr and add these Dependencies :\n Web Actuator DevTools  You can download the sample project that I generated from¬†here.\nTo this project, we need to add this Java class:\n1 2 3 4 5 6 7 8  @RestController @RequestMapping(\u0026#34;/api/hello\u0026#34;) public class HelloWorldRestController { @GetMapping public String sayHello() { return \u0026#34;Hello the time is \u0026#34; + System.nanoTime(); } }   Now, we will need to play with Azure Application Insights üòÅüòÅ\nEnabling the Azure Application Insights We need to start by creating a new Resource Group:\n name: azure-monitor-rg region: (Europe) France Central  Next, go to Monitor and click on the Applications in the Insights section:\n On this screen, click on Create Application Insights apps:\n name: app-insights-demo region: (Europe) France Central  The deployment will start:\n When finished click on Go to resource:\n¬†From this screen we need to grab the Instrumentation Key. This will be used from our application to communicate with Azure.\nNext, in our Spring Boot project, we need to add the Application Insights Starter to the pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;applicationinsights-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Next, we need to add some properties to the application.properties:\n1 2 3 4  # Springboot application logical name spring.application.name=app-insights-demo # Application Insights instrumentation key azure.application-insights.instrumentation-key=de7850bX-XXXX-XXXX-XXXX-XXXXXXXXXXXX   Now, we have two steps of implementing the Application Insights support:\n Enabling the metrics emission Enabling the Logs broadcasting  1. Enabling the metrics emission In our REST Controller, we will use a TelemetryClient instance to send telemetry to Azure Application Insights. Our controller will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  @RestController @RequestMapping(\u0026#34;/api/hello\u0026#34;) public class HelloWorldRestController { @Autowired TelemetryClient telemetryClient; private static final Logger LOGGER = LoggerFactory.getLogger(HelloWorldRestController.class); @GetMapping public String sayHello() { //track a custom event  this.telemetryClient.trackEvent(\u0026#34;Sending a custom event...\u0026#34;); //trace a custom trace  this.telemetryClient.trackTrace(\u0026#34;Sending a custom trace....\u0026#34;); //track a custom metric  this.telemetryClient.trackMetric(\u0026#34;custom metric\u0026#34;, 1.0); //track a custom dependency  this.telemetryClient .trackDependency(\u0026#34;SQL\u0026#34;, \u0026#34;Insert\u0026#34;, new Duration(0, 0, 1, 1, 1), true); String message = \u0026#34;Hello the time is \u0026#34; + System.nanoTime(); LOGGER.info(message); return message; } }   Cool ! ü•≥¬†But is these telemetries ? what is an Event ? a Trace ? a Metric ? a Dependency ?? ü§î\n Event telemetry: represents an event that occurred in your application. Typically it is a user interaction such as button click or order checkout. It can also be an application life cycle event like initialization or configuration update.Semantically, events may or may not be correlated to requests. However, if used properly, event telemetry is more important than requests or traces. Events represent business telemetry and should be a subject to separate, less aggressive¬†sampling. Trace telemetry: represents a textual styled trace. Log file entries are translated into instances of this type. The trace does not have measurements as an extensibility. Metric telemetry: There are two types of metric telemetry : single measurement and pre-aggregated metric.  Single measurement is just a name and value. Pre-aggregated metric specifies minimum and maximum value of the metric in the aggregation interval and standard deviation of it. It assumes that aggregation period was one minute.   Dependency Telemetry: represents an interaction of the monitored component with a remote component such as SQL (like what we simulated in our example) or an HTTP endpoint. Request Telemetry:¬†represents the logical sequence of execution triggered by an external request to your application. Every request execution is identified by unique¬†ID¬†and¬†url¬†containing all the execution parameters. You can group requests by logical¬†name¬†and define the¬†source¬†of this request. Code execution can result in¬†success¬†or¬†fail¬†and has a certain¬†duration. Both success and failure executions may be grouped further by¬†resultCode.  Just start the application and just call the REST Controller on http://localhost:8080/api/hello some times to generate some telemetries:\n Here we go ! After some minutes, we get the results in the same Monitor \u0026gt; Application Insights \u0026gt; Search menu üòÅ\n2. Enabling the Logs broadcasting Spring Boot uses logback as the default logger. Azure does provide its own log-appender which has to be configured in our Spring Boot application. First you have to add the AppInsights log-appender as a new dependency:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.azure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;applicationinsights-logging-logback\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   To configure the newly added log-appender you have to add a new configuration File¬†/src/main/resources/logback-spring.xml to your app:\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;include resource=\u0026#34;org/springframework/boot/logging/logback/base.xml\u0026#34;/\u0026gt; \u0026lt;logger name=\u0026#34;com.targa.dev.labs.appinsights\u0026#34; level=\u0026#34;DEBUG\u0026#34;/\u0026gt; \u0026lt;appender name=\u0026#34;aiAppender\u0026#34; class=\u0026#34;com.microsoft.applicationinsights.logback.ApplicationInsightsAppender\u0026#34;\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;aiAppender\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;   Now, all your logging will be broadcasted automatically to our Application Insights instance.\nIf you go back to the Azure portal, next Azure Monitor, next to Application Insights, click on your application :\n Now, click on Search to be forwarded to the listing of the received logs - if there are no logs; click on the Refresh button:\n Click on a message to see the transaction details:\n Cool ! Now we are sending logs and telemetries to our Azure Monitor ecosystem.\nLog Analytics Log Analytics is a web tool used to write and execute Azure Monitor log queries. Open it by selecting¬†Logs¬†in the Azure Monitor menu:\n Now, you can type an Azure Logs Query in the editor.\nFor example, a query that lists the last 50 requests: requests | limit 50:\n Now that we get the records, we can filter the results, sort them, check the record details to be able to be more productive by understanding what\u0026rsquo;s happening in our application.\n We can even generate some graphs and charts to understand in deep how things are turning into the application. Even if we have some frequently used queries, we can store them. We can go far and export the records to PowerBI or Excel to play more with the data if you like that ü§ì\nLive metrics stream With the Live Metrics Stream service, we can watch metrics and performance counters in real time, without any disturbance to the service. We can easily inspect the stack traces from sample failed requests and exceptions. It provides a powerful and non-invasive diagnostic tool for our live web applications.\n I\u0026rsquo;m sure that you are surprised by the actual values üòÇ It\u0026rsquo;s quite because I didn\u0026rsquo;t generated so much noise üò¥\nWith Live Metrics Stream, we can:\n Validate a fix while it is released, by watching performance and failure counts. Watch the effect of test loads, and diagnose issues live. Focus on particular test sessions or filter out known issues, by selecting and filtering the metrics you want to watch. Get exception traces as they happen. Experiment with filters to find the most relevant KPIs. Monitor any Windows performance counter live. Easily identify a server that is having issues, and filter all the KPI/live feed to just that server.  Performance Dashboard Application Insights collects performance details for the different operations in our application. By identifying those operations with the longest duration, we can diagnose potential problems or best target your ongoing development to improve the overall performance of the application.\n Select¬†Application Insights. To open the¬†Performance¬†panel either select¬†Performance¬†under the¬†Investigate menu OR click the Server Response Time¬†graph.   The¬†Performance panel shows the count and average duration of each operation for the application. You can use this information to identify those operations that most impact users.¬†In our example, the GET /robots.txt is a likely candidate to investigate because of its relatively high duration although it\u0026rsquo;s called only once (COUNT=1).\n You can do more, just take a look on the documentation ü§ì\nApplication Health with Azure Application Insights Azure Application Insights allows us to monitor our application and send us alerts when it is either unavailable, experiencing failures, or suffering from performance issues.\nTo continuously check the availability of our application, we need to create an availability test, used¬†allow us to automatically test our application from various locations around the world.\nTo create an Availability Test, select Availability¬†under the¬†Investigate¬†menu and then click¬†Create test:\n Next, we will fill the Test basic information:\n The Test basic informations:\n Name: Health Check Test Type: URL ping test URL: The actuator health check URI on one of my hosted application: http://app-insights-demo.azurewebsites.net/actuator/health We enabled the retries in case of a test fails - with a frequency of 5 minutes The tests will be run from 16 different locations The criteria of success of the test will be an HTTP 200 response within a 120 seconds. Alerts are enabled and can be customized from an other menu üòÅ  Now our test is created, we can check it and do some modifications like alert conditions or the alert :\n We can see that, by default, the alert will be dispatched only if the test failed from 8 locations or more:\n In the Actions, we need to select an Action Group in matter to define the group that will receive the alert.\nNext steps In this post, we did a small tour in the Azure Monitor services and I hope you could get an idea about how this great product works ? and what you can do with ?\nI used before the ELK/EFK and Dynatrace, and I found that they are so good as a couple to monitor and track what\u0026rsquo;s going on on applications and microservices. In comparaison, I really appreciate the Azure Monitor services and I found them complete and exhaustive and all the features you need are in one place. Give it a try, I will be interested by your feedback üòä\n","permalink":"https://blog.nebrass.fr/playing-with-azure-monitor-services-and-spring-boot/","summary":"In enterprise application, performance is major requirement of success. Especially for applications where slowness will have a direct detrimental impact on business productivity, profits and even the brand itself, like trading platforms and e-commerces.\nIf a trading platform loads slowly or experiences errors, it will translate into loss of business, and losses can be extremely high, and the customer might end up switching to another competitor.\nIn this context comes the Application Performance Management¬†(APM).","title":"Playing with Azure Monitor services and Spring Boot"},{"content":"January 2018, Red Hat acquired CoreOS for 250 million dollars ü§©. CoreOS was one of the leading companies of Linux \u0026amp; Containers market with their wide offer of products:\n CoreOS Tectonic: container application platform based on Kubernetes. CoreOS Container Linux: lightweight Linux distribution designed to run containerized applications. CoreOS Operators Framework: an open source toolkit designed to manage Kubernetes native applications. CoreOS Quay: a container registry for building, storing, and distributing your private containers. CoreOS rkt: an application container engine developed for modern production cloud-native environments. even more and more..  Red HatOpenShift 3.x was facing many big problems especially in installations and upgrades. It was a real nightmare to install an OCP Cluster. I never heard an OCP Admin talking about upgrading OCP without discussing the problems that they faced. Personally, I worked for two customers from 2017 to 2019 that adopted OCP, and when it comes to cluster upgrades they were dedicating time and they were bringing people from Red Hat to help them do that üòÇ¬†Although, the upgrades were guaranteed by Red Hat in all technical and commercial announcements.\nAt that moment, CoreOS\u0026rsquo;s Kubernetes platform, called Tectonic, had already a great feature that made it less disturbing: the one click upgrades ü§© ! Yeah ! I remember the first time I installed a Tectonic cluster (v1.7). I tried to upgrade it to v1.8 with the one click upgrade button in the management console.. and every thing worked like a charm ! ü•≥ I felt that it was a kind of a joke, or that were something wrong ü§™ü§™\nRed Hat OCP was not suffering only from hard installations and upgrades.. it had many issues with the dedicated Docker Registry.. personally, I got it crashed many times, and I remember it also crashed when I was sitting for the OCP Certification that I failed üòÇ At the same moment, CoreOS Tectonic is embedding the Quay Containers Registry üòÅ\nThis led Red Hat to go fast in acquiring CoreOS.. and they started at the same time talking about OpenShift 4.0 üòÅ\nMay 1st, 2019, the OpenShift 4.1 was released. Yeah ! There was no 4.0 version for some reasons üòÜ\nGreat! What did OpenShift 4.1 bring new ?\nOpenShift 4.1 new features OpenShift brings many great features, starting of the version 4.1 :\n  Installation : a wizard will cover all the installation process steps and heavy tasks. No more painful installation experience ! üëà CoreOS Tectonic feature¬†üòé\n  Upgrade: the process will be easily done via a one-click operation from the administration console. No more headaches !¬†üëà CoreOS Tectonic feature¬†üòé NOTE: Upgrading from the 3.x to 4.x is not possible, obviously ü§ì\n  Red Hat Enterprise Linux CoreOS: immutable OS more suitable for containerization and easily manageable ! No more RHEL administration fever ! üëà CoreOS Tectonic feature üòé\n  Operators: An Operator is a method of packaging, deploying and managing a Kubernetes application. A Kubernetes application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl tooling. üëà CoreOS Tectonic feature üòé\nNow, most of the OpenShift\u0026rsquo;s components are now Operators:\n The registry is now managed by an Operator The networking is now configured and managed by an Operator. The Operator upgrades and monitors the cluster network. The cluster autoscaling is now handled by an Operator. The Operator    Enhanced Web console: The new release offers a great revisited Web Console and redesigned Developer Catalog that brings all of the new Operators and existing broker services together, with new ways to discover, sort, and understand how to best use each type of offering. There are also many new management screens that come with the new features.\n    Developer Experience: Red Hat brings a new tool called Code Ready Containers, a local desktop instance of OpenShift Container Platform 4.1 replaces the functions of oc cluster commands, Minishift, and CDK. OpenShift Container Platform 4.1 focuses on ease of access and native experience, with a native installation program on macOS and Microsoft Windows, native hypervisor support, and tray icon integration.  There are many great incoming features, like the Red Hat OpenShift Service Mesh that we have been waiting for long time ü•≥\nPlaying with OpenShift 4.1 Locally\nIn the OpenShift 3.x era, we had the great Minishift tool that enables developers to have a local OpenShift Cluster. But as we listed before, OpenShift 4 come with a new tool called Code Ready Containers (aka CRC), that will bring a minimal OpenShift 4.0 or newer cluster to your local laptop or desktop computer.\nTo install the CRC, you need to go to try.openshift.com and click on Get Started:\n Next, you need to log in your Red Hat account; If you don\u0026rsquo;t have one, you can create it for free in less than 2 minutes.\nNext, you will be redirected to the Red Hat OpenShift Cluster Manager dashboard:\n From this screen, you can create a cluster on many Cloud provider (Aws, Azure, etc..) or even on your dedicated servers; or on your Laptop, which is what we want to do üòÅ\nNext, click on Laptop button to access the download section:\n In this screen, you need to download the CRC binaries and the Pull secret, which is a Red Hat credentials generated for your account to authenticate the downloads that will be made by the CRC tool.\nAfter installing the binaries. You need to place it in your $PATH .. ü§ì\nNext, we will start playing with the CRC.. the first command to do is\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ crc setup INFO Checking if running as non-root INFO Caching oc binary INFO Setting up virtualization with HyperKit INFO Will use root access: change ownership of /Users/developer/.crc/bin/hyperkit INFO Will use root access: set suid for /Users/developer/.crc/bin/hyperkit INFO Installing crc-machine-hyperkit INFO Will use root access: change ownership of /Users/developer/.crc/bin/crc-driver-hyperkit INFO Will use root access: set suid for /Users/developer/.crc/bin/crc-driver-hyperkit INFO Setting file permissions for /etc/resolver/testing INFO Will use root access: create dir /etc/resolver INFO Will use root access: create file /etc/resolver/testing INFO Will use root access: change ownership of /etc/resolver/testing INFO Setting file permissions for /etc/hosts INFO Will use root access: change ownership of /etc/hosts INFO Unpacking bundle from the CRC binary Setup is complete, you can now run \u0026#39;crc start\u0026#39; to start a CodeReady Containers instance   Next, as mentioned in the log, you need just to type crc start¬†to start a CRC instance üòÅ\n1 2 3 4 5 6 7 8 9  $ crc start INFO Checking if running as non-root INFO Checking if oc binary is cached INFO Checking if HyperKit is installed INFO Checking if crc-driver-hyperkit is installed INFO Checking file permissions for /etc/resolver/testing INFO Checking file permissions for /etc/hosts ? Image pull secret [? for help]   Now you need to past the pull secret that you grab from the Red Hat portal in the previous steps.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ? Image pull secret [? for help] ************************ ********************************************************* ********************************************************* INFO Loading bundle: crc_hyperkit_4.1.11.crcbundle ... INFO Extracting bundle: crc_hyperkit_4.1.11.crcbundle ... INFO Creating VM ... INFO Verifying validity of the cluster certificates ... INFO Restarting the host network INFO Check internal and public dns query ... INFO Copying kubeconfig file to instance dir ... INFO Adding user\u0026#39;s pull secret and cluster ID ... INFO Starting OpenShift cluster ... [waiting 3m] INFO To access the cluster using \u0026#39;oc\u0026#39;, run \u0026#39;eval $(crc oc-env) \u0026amp;\u0026amp; oc login -u kubeadmin -p 78UVa-zNj5W-YB62Z-ggxGZ https://api.crc.testing:6443\u0026#39; INFO Access the OpenShift web-console here: https://console-openshift-console.apps-crc.testing INFO Login to the console with user: kubeadmin, password: 78UVa-zNj5W-YB62Z-ggxGZ CodeReady Containers instance is running   Great ! So our CRC instance is running and we get some informations in the log:\n  The OpenShift CLI login command: oc login -u kubeadmin -p 78UVa-zNj5W-YB62Z-ggxGZ https://api.crc.testing:6443\n  The OpenShift web-console URL: https://console-openshift-console.apps-crc.testing\n  The OpenShift web-console credentials:\n login: kubeadmin password: 78UVa-zNj5W-YB62Z-ggxGZ     If you have noticed, the URLs of the web-console or the Master node URL are made based on an *.testing domain name. This is a custom DNS name that is pointing on a local IP address..\nIn the log of the crc setup command, there some lines handling the /etc/hosts file..\nIf you ping a the domain name, you will get a local IP address like mine for example: 192.168.64.3\n Now, go to https://console-openshift-console.apps-crc.testing to access the OpenShift web-console:\n Click on the kube:admin¬†and you need to type the credentials listed in the log of the crc start command:\n Yoppa ! You will be redirected to the Web Console Dashboard:\n Yooopaa !ü•≥ü§©¬†Now every thing is up ! ü•≥ü§© Let\u0026rsquo;s deploy a sample application üòÅ\nFirst of all, click on Create Project:\n Next, click on Browse Catalog:\n Next, choose Apache HTTP Server (httpd) and click on Create Application:\n In the next form, click on the Try Sample, and tick the Create route and click Create:\n Next, you will see the Project Status screen:\n When the deployment is finished. Scroll down to in the menu and click on Routes in the Networking section:\n In this screen, we will get the Route URL created for our Hello World application:\n Now, click on the Location URL: http://httpd-hello-world.apps-crc.testing/ and you will see the application:\n Hakuna matata ! ü•≥ü§©ü•≥\nWe can also enter the OpenShift command: to check if every thing is ok üßê This command will show the Route URL that we got before from the Web Console:\n1 2 3  $ oc get routes -n hello-world httpd -o jsonpath=\u0026#39;{.spec.host}\u0026#39; httpd-hello-world.apps-crc.testing   Cool ! Everything is working like a charm ! Yeah ! I\u0026rsquo;m wonderful üòÇ\nConclusion I\u0026rsquo;m really happy with the OpenShift 4 ! I really appreciate the product revolution ! I have the feeling that I\u0026rsquo;m dealing with a totally different product üòÅ\n","permalink":"https://blog.nebrass.fr/playing-with-openshift-4-locally/","summary":"January 2018, Red Hat acquired CoreOS for 250 million dollars ü§©. CoreOS was one of the leading companies of Linux \u0026amp; Containers market with their wide offer of products:\n CoreOS Tectonic: container application platform based on Kubernetes. CoreOS Container Linux: lightweight Linux distribution designed to run containerized applications. CoreOS Operators Framework: an open source toolkit designed to manage Kubernetes native applications. CoreOS Quay: a container registry for building, storing, and distributing your private containers.","title":"Playing with OpenShift 4 locally"},{"content":"ÿπÿ≥ŸÑÿßŸÖÿ© ŸàŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸäŸÉŸÖ ŸÅŸä ÿ®ÿ±ŸÜÿßŸÖÿ¨ŸÉŸÖ \u0026ldquo;ÿÆÿ∞Ÿàÿß ÿπŸäŸÜŸä ÿ¥ŸàŸÅŸà ÿ®ŸäŸáÿß\u0026rdquo; üòÜ\nÿßŸÑŸäŸàŸÖ ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸäŸÑŸÉŸÖ ÿπŸÑŸâ ŸÖŸáŸÜÿ© ÿßŸÑConsulting ÿßŸÑŸÑŸä ŸàŸÑÿßÿ™ ÿ£ŸÉÿ´ÿ± ÿßÿÆÿ™ÿµÿßÿµ ÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ™Ÿàÿ© ŸÅŸä ÿ≥ŸàŸÇ ÿßŸÑÿ¥ÿ∫ŸÑ ÿÆÿßÿµÿ© ŸÅŸä ÿßŸàÿ±Ÿàÿ®ÿß.. ÿ¥ŸÜŸäŸá ŸÖÿπŸÜÿßŸáÿß Consulting ÿüÿü ŸÖŸÜŸäŸÜ ÿ¨ÿßÿ™ Ÿàÿ¥ŸÜŸäŸá ÿßŸÑÿ≠ŸÉÿßŸäÿ© ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ÿüÿü ŸÅŸä ÿßŸÑpost Ÿáÿ∞ÿß ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸä ŸÖŸÜ ŸÖŸÜÿ∑ŸÑŸÇ ÿ¥ÿÆÿµŸä ÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ© üòÅ\nÿßŸäÿß ÿ≥ŸäÿØŸä ÿ®ŸÜ ÿ≥ŸäÿØŸä.. ŸÜÿ±ÿ¨ÿπ ÿ®ŸäŸÉŸÖ ŸÑÿπÿßŸÖ 2014.. ÿ®ÿπÿØ ÿπÿßŸÖŸäŸÜ ÿÆÿØŸÖÿ© ÿπÿØŸäÿ™ŸáŸÖ ÿ®ÿπÿØ ÿßŸÑLicence ŸÅŸä ÿ¥ÿ±ŸÉÿ© ÿ®ÿ™ÿ±ŸàŸÑŸäÿ© ŸÅŸä ÿ™ŸàŸÜÿ≥.. ÿÆÿ±ÿ¨ÿ™ ÿ®ÿßÿ¥ ŸÜÿπÿØŸä ÿ≥ÿ™ÿßÿ¨ ÿßŸÑŸÖÿßÿ≥ÿ™Ÿäÿ± ŸÅŸä ÿ®ÿßÿ±Ÿäÿ≥ ŸàŸÉŸÜÿ™ ŸÜŸÑŸàÿ¨ ÿπŸÑŸâ ÿÆÿØŸÖÿ© ÿ®ÿßÿ¥ ŸÉŸäŸÅ ŸÜÿ™ÿÆÿ±ÿ¨ ÿßŸÑÿ£ŸÖŸàÿ± ÿ™ÿ®ÿØŸâ ŸÖÿ±ŸäŸÇŸÑÿ© ŸàŸÖÿß ŸÜÿ®ŸÇÿßÿ¥ ÿ®ÿ∑ÿßŸÑ.. ŸÉŸÜÿ™ ŸÜŸÑŸàÿ¨ ÿπŸÑŸâ poste ŸÅŸä ÿ¥ÿ±ŸÉÿ© √©diteur logiciels ŸàÿßŸÑÿß banques ŸàÿßŸÑÿß assurances.. ŸÉŸÜÿ™ ŸÜÿ™ÿµŸàÿ± ÿßŸÑŸÑŸä ÿßŸÑÿÆÿØŸÖÿ© ŸÖÿ™ÿßÿπ ÿßŸÑd√©veloppement ŸÖÿß ÿ™ŸÉŸàŸÜ ŸÉÿßŸÜ ŸÅŸä ÿ¥ÿ±ŸÉÿ© ŸÖŸÜ ŸÜŸàÿπ Ÿáÿ∞ÿß..\nÿ¥ÿßÿ°ÿ™ ÿßŸÑŸÇÿØÿ±ÿ© ÿßŸÜŸä ŸÜÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ŸÖŸáŸÜÿØÿ≥ ÿ™ŸàŸÜÿ≥Ÿä ŸäÿÆÿØŸÖ ŸÅŸä ÿ®ÿßÿ±Ÿäÿ≥ ŸÇŸÑŸä ŸÉÿßŸÜ ÿ™ŸÅŸáŸÖ Java ÿßŸÜÿ¨ŸÖ ÿßŸÜÿµÿ®ŸÑŸÉ CV ŸÖÿ™ÿßÿπŸÉ ŸÅŸä ÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÖÿ™ÿßÿπŸÜÿß ÿ™ÿ¨Ÿä ÿ™ÿÆÿØŸÖ Consultant Java.. ÿßŸÉŸäÿØ ŸÅÿ±ÿ≠ÿ™ ÿ®ÿ±ÿ¥ÿ© ÿ®ÿßŸÑÿπÿ±ÿ∂ ÿ®ÿßŸÑÿ±ÿ∫ŸÖ ÿßŸÜŸä ŸÖÿß ŸÅŸáŸÖÿ™ÿ¥ ÿßÿµŸÑÿß ÿßÿ¥ ŸÖÿπŸÜÿßŸáÿß Consultant üòÇüòÇ\nŸÜŸàÿπŸäÿ© ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ Ÿáÿßÿ∞Ÿä ÿßÿ≥ŸÖŸáÿß ÿßŸÑÿ™ŸÇŸÜŸä ŸáŸà¬†Soci√©t√© de Services en Ing√©nierie Informatique SSII ou SS2I ŸàÿßŸÑŸÑŸä ÿßÿ≥ŸÖŸáŸÖ ÿ™Ÿàÿ© Entreprise de Services du Num√©rique ESN ..\nÿ£Ÿäÿß ŸÖÿ¥Ÿäÿ™ ŸÜŸÑŸàÿ¨ ÿßÿ¥ ŸÖÿπŸÜÿßŸáÿß Consulting.. Ÿàÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ© ŸàŸÇÿ™ ŸÉŸÜÿ™ ŸÜŸÑŸàÿ¨ ÿÆŸäÿßŸÑ ÿßŸÑÿµŸàÿ± ŸÖÿ™ÿ≠ÿ±ŸÉÿ© ŸÉÿßŸÜ ÿÆÿØÿßŸÖ üòÇüòÇüòÇ ŸÉŸäŸÅ ŸÇÿ±Ÿäÿ™ ÿßŸÑŸÑŸä ŸÉŸÑŸÖÿ© Consulting ŸáŸä ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿßÿ™ ŸàÿßŸÑŸÑŸä Consultant ŸáŸà ŸÖÿ≥ÿ™ÿ¥ÿßÿ± ÿ®ÿØŸâ Ÿäÿ≥ÿ±ÿ≠ ÿ®Ÿäÿß ÿßŸÑÿÆŸäÿßŸÑ ÿßŸÑÿÆÿµÿ® ŸÖÿ™ÿßÿπŸä.. ŸÜÿ™ÿÆŸäŸÑ ŸÅŸä ÿ±Ÿàÿ≠Ÿä ÿ±ÿßŸÉÿ¥ ŸÅŸä ÿ¥ÿ±ŸÉÿ© Ÿà 6 ŸàÿßŸÑÿß 7 ŸÖŸÜ ÿßŸÑŸÜÿßÿ≥ ÿØÿßŸäÿ±ŸäŸÜ ÿ®Ÿäÿß ŸàŸäÿ≥ÿ£ŸÑŸàÿß ŸÅŸäÿß \u0026ldquo;ÿ®ÿßŸÑŸÑŸá ŸÉŸäŸÅÿßŸá ŸÜÿπŸÖŸÑŸà ÿßŸÑComposant Ÿáÿ∞ÿßÿüÿü\u0026rdquo; Ÿàÿ™ÿÆŸäŸÑÿ™ ÿ≤ÿßÿØÿ© Ÿàÿ≠ÿØÿ© Blonde ÿ™ÿπŸÖŸÑŸä ŸÅŸä Massage ŸÑÿ∏Ÿáÿ±Ÿä ŸàÿßŸÜÿß ŸÜŸÅÿ≥ÿ± ŸÑŸÑÿ≠ÿ±ŸÅÿßÿ° ŸÖÿ™ÿßÿπŸä ŸÉŸäŸÅÿßŸá ÿßŸÑÿÆÿØŸÖÿ© ÿ™ÿµŸäÿ± üòÇüòÇüòÇ\nÿ£Ÿäÿß ÿ™ÿµÿ® ÿßŸÑCV ŸÖÿ™ÿßÿπŸä ŸàÿπŸäÿ∑ŸàŸÑŸä ŸÅŸä ŸàŸÇÿ™ ŸÇŸäÿßÿ≥Ÿä ŸàÿπÿØŸäÿ™ des entretiens ŸÖŸÜŸáÿß ÿßŸÑÿ™ŸÇŸÜŸä ŸàŸÖŸÜŸáÿß ÿßŸÑRH.. ŸàŸÉŸäŸÅ ÿ∑ŸÑÿ®ÿ™ ŸÖŸÜŸáŸÖ ÿßŸÜŸáŸÖ ŸäÿπŸÖŸÑŸàŸÑŸä ÿßŸÑIntroduction de salari√© √©tranger ŸàÿßŸÅŸÇŸàÿß ÿ®ÿ≥ŸáŸàŸÑÿ© ÿ∫ÿ±Ÿäÿ®ÿ© (ÿ®ÿßŸÑÿ±ÿ∫ŸÖ ÿßŸÜŸá ÿ™ÿ™ŸÉŸÑŸÅ ÿ≠ŸàÿßŸÑŸä 4000 ÿßŸàÿ±Ÿà Ÿàÿ¥Ÿáÿ±ŸäŸÜ ÿ≥ÿ®ÿßÿ≠ÿ© ÿ≠ÿ±ÿ© ŸÅŸä ÿßŸÑÿßÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿßÿØÿßÿ±Ÿäÿ©) Ÿàÿ≠ÿ™Ÿâ ÿßŸÑÿßÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿπŸÖŸÑŸàŸáÿß ÿ®ÿ≥ÿ±ÿπÿ© ŸÇŸäÿßÿ≥Ÿäÿ© üòÖüòÖ\nÿßŸÑsouplesse Ÿáÿßÿ∞Ÿä ÿßŸÑŸÉŸÑ ÿ≤ÿßÿØÿ™ ÿßŸÑÿ¥ŸäÿÆÿ© ŸÖÿ™ÿßÿπ ÿßŸÑÿÆŸäÿßŸÑ ÿßŸÑÿÆÿµÿ® ŸàŸÉŸäŸÖÿß ŸäŸÇŸàŸÑ ÿßŸÑŸÖÿ´ŸÑ \u0026ldquo;ŸÖÿ¥Ÿâ ŸÅŸä ÿ®ÿßŸÑŸä ŸÇŸÖÿ±ÿ© Ÿàÿ±ÿ®Ÿäÿπ\u0026rdquo; üòÇüòÇ ÿ£Ÿäÿß ŸÖŸÜÿ∫Ÿäÿ± ŸÖÿß ÿßŸÜÿ∑ŸàŸÑ ÿπŸÑŸäŸÉŸÖ.. ÿ±Ÿàÿ≠ÿ™ ŸÑÿ™ŸàŸÜÿ≥ Ÿàÿ±ÿ¨ÿπÿ™ ŸÑÿ®ÿßÿ±Ÿäÿ≥ ÿ®ÿßÿ¥ ŸÜÿ®ÿØŸâ ŸÜÿÆÿØŸÖ ŸÖÿ≥ÿ™ÿ¥ÿßÿ± Java ÿπÿ∏ŸäŸÖ üòÇ\nÿßŸàŸÑ ŸÜŸáÿßÿ± ŸÖÿ¥Ÿäÿ™ ŸÑŸÑbureau ÿ®ŸÉÿ±Ÿä Ÿàÿ¥ÿßŸäÿÆ ŸÜÿ™ÿÆŸäŸÑ ŸÅŸä ÿ±Ÿàÿ≠Ÿä ÿ®ÿßÿ¥ ŸÜÿ®ÿØŸâ ŸÜŸÉÿ™ÿ® ŸÅŸä ÿßŸÑCode.. ÿ£Ÿäÿß ÿØÿÆŸÑÿ™ ŸÑŸÑbureau ŸÖÿ¥Ÿäÿ™ ÿ≠ÿ∞Ÿâ ÿßŸÑmanager ŸÖÿ™ÿßÿπŸä ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸà ÿπÿßŸÑÿÆÿØŸÖÿ© ŸàŸÉŸäŸÅÿßŸá ÿ™ÿµŸäÿ± ÿßŸÑÿ£ŸÖŸàÿ±..\n¬†ÿ£Ÿäÿß ÿ®ÿØŸäŸÜÿß ÿßŸÑŸÑŸÇÿßÿ° ŸÖÿ™ÿßÿπŸÜÿß.. ŸàŸáŸà ŸÖÿßŸäÿπÿ±ŸÅ ÿπŸÜŸà ÿßŸÑŸÅÿ±ÿßŸÜÿ≥Ÿäÿ≥ ÿ®ŸÉŸÑŸÖÿ© Point.. ÿ®ÿßÿ¥ ÿ≤ÿπŸÖÿ© ÿ≤ÿπŸÖÿ© ŸÖŸàÿ¥ ÿßÿ¨ÿ™ŸÖÿßÿπ ÿ∑ŸàŸäŸÑ.. ŸÅŸáŸÖÿ™ ŸÖŸÜ ÿßŸÑŸÑŸÇÿßÿ° ÿßŸÑÿπÿ¨Ÿäÿ® ÿßŸÑŸÑŸä ŸÖŸáŸÜÿ© ÿßŸÑConsulting ÿßŸÑÿπÿ¨Ÿäÿ®ÿ© ŸÑÿßÿ≤ŸÖŸáÿß Client Ÿäÿ∑ŸÑÿ® profil ŸÖÿπŸäŸÜ ŸàÿßŸÑÿß ÿ™ÿ¨ŸäŸá ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿ™ŸÇÿ™ÿ±ÿ≠ ÿπŸÑŸäŸá profil ÿ®ÿßÿ¥ ŸäÿÆÿØŸÖŸÑŸà mission ŸÖÿπŸäŸÜÿ©.. ÿßŸÑmission Ÿáÿßÿ∞Ÿä ÿßŸÑÿπŸÇÿØ ŸÖÿ™ÿßÿπŸáÿß ŸÖÿß Ÿäÿ™ÿ¨ÿßŸàÿ≤ÿ¥ 3 ÿ≥ŸÜŸäŸÜ..¬†ÿ£Ÿäÿß ÿ≥ŸäÿØŸä ŸÅŸä ÿßŸÑŸàŸÇÿ™ Ÿáÿßÿ∞ÿßŸÉÿß ŸÅŸÖÿß Appel d\u0026rsquo;offres ŸÖÿ™ÿßÿπ des consultants Java ŸÑŸÑ Airbus Defence \u0026amp; Space ŸàÿßŸÑAlcatel.. ŸÇÿßŸÑŸÑŸä ÿ®ÿßÿ¥ ŸÜÿ≠ÿ∑Ÿà ÿßŸÑCV ŸÖÿ™ÿßÿπŸÉ ŸàŸàŸÉŸäŸÅ ŸäÿπŸäÿ∑ŸàŸÑŸÉ ÿ™ÿπÿØŸä les entretiens¬†ŸàŸÉÿßŸÜ ŸÑÿßÿ®ÿßÿ≥ ÿ™ÿ≥ŸÑŸÉ ÿßŸÖŸàÿ±ŸÉ.. ÿ®ÿπÿØŸáÿß ŸÉÿßŸÜ ÿ™ÿ™ŸÇÿ®ŸÑ.. ÿßŸÑSSII ÿ™ÿ®ÿπÿ´ŸÉ ÿπŸÜÿØ ÿßŸÑClient Ÿàÿ™ÿπŸÖŸÑŸá facturation ÿ®ÿßŸÑŸÜŸáÿßÿ± ÿßŸÑŸÑŸä ÿ™ÿÆÿØŸÖŸá ÿπŸÜÿØŸáŸÖ.. Ÿáÿ∞ÿßŸÉÿß ÿπŸÑÿßŸá ŸÉŸäŸÅ Ÿäÿ®ÿπÿ´Ÿá CV Ÿäÿ≤ŸäÿØŸà ŸÖÿπÿß Taux Journalier Moyen ÿßŸà TJM.. ŸàŸàŸÇÿ™Ÿáÿß ÿπÿ±ŸÅÿ™ ÿßŸÑŸÑŸä ÿπÿ±ŸÅŸä ŸáŸà commercial ŸàŸÖŸàÿ¥ manager.. ÿπŸÑÿß ÿÆÿßÿ∑ÿ± ŸÉÿßŸÜ juste Ÿäÿ®Ÿäÿπ ŸÅŸä prestation.. ŸÑÿß management ŸàŸÑÿßŸáŸÖ Ÿäÿ≠ÿ≤ŸÜŸàŸÜ..\nÿ£Ÿäÿß ŸÉŸÖŸÑŸÜÿß ÿßŸÑŸÜŸÇÿ∑ÿ©.. ŸàŸÇÿßŸÑŸÑŸä ÿ®ÿßÿ¥ ŸÜŸàÿµŸÑŸÉ ŸÑŸÑworkspace ÿßŸÑŸÑŸä ŸáŸà ÿßŸÑÿ®ŸÑÿßÿµÿ© ÿßŸÑŸÑŸä ÿ®ÿßÿ¥ ŸÜÿ®ŸÇŸâ ŸÅŸäŸáÿß ŸàÿßŸÑŸÑŸä ÿ™ÿÆŸäŸÑÿ™Ÿáÿß ŸÉŸäŸÖÿß ŸÖŸÉÿßÿ™ÿ® ÿßŸÑÿßŸÖÿ±ŸäŸÉÿßŸÜ ŸÅŸä ÿßŸÑÿ£ŸÅŸÑÿßŸÖ..\nŸáŸäÿß ŸÖÿ¥ŸäŸÜÿß ŸÑÿµÿßŸÑÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸÅŸäŸáÿß ÿ®ÿ±ÿ¥ÿ© ŸÖŸÜ ŸÜÿßÿ≥ ÿ±ÿßŸÉÿ¥ŸäŸÜ ŸÇÿØÿßŸÖ les PCs ŸÖÿ™ÿßÿπŸáŸÖ.. ÿßŸÑManager ŸÖÿ™ÿßÿπŸä ŸÇÿßŸÑŸä ÿ™ŸÜÿ¨ŸÖ ÿ™ÿ®ŸÇŸâ ŸáŸÜÿß ŸàÿßÿπŸÖŸÑ auto formation ŸÅŸä ÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™ ÿ®ÿßÿ¥ ÿ™ÿ®ÿØŸâ ÿ≠ÿßÿ∏ÿ± ŸÑŸÑentretien ÿßŸÑŸÖŸàÿπŸàÿØ.. ÿ∑ŸÑÿπ ÿßŸÑworkspace ŸáŸà parking ŸÉÿ®Ÿäÿ± ŸÖÿ™ÿßÿπ ŸÖŸáŸÜÿØÿ≥ŸäŸÜ ÿ®ÿ∑ÿßŸÑÿ© Ÿäÿ≥ÿ™ŸÜŸàÿß ŸÅŸä ÿ±ÿ≠ŸÖÿ© ÿ±ÿ®Ÿä ÿ®ÿßÿ¥ ÿ™ŸÜÿ≤ŸÑ..\nÿßŸÑÿ®ŸÑÿßÿµÿ© Ÿáÿßÿ∞Ÿä ÿØŸäŸÖÿß ŸÉŸäŸÅ ŸÜÿ™ÿ∞ŸÉÿ±Ÿáÿß ÿ™ÿ¨Ÿä ŸÅŸä ŸÖÿÆŸä ÿßŸÑÿ±Ÿàÿ∂ÿ©.. ŸÅŸäŸáÿß flipper Ÿàbillard Ÿàpiano ŸàŸÅŸäŸáÿß ŸÜÿßÿ≥ ÿ™ŸÑÿπÿ® ŸàŸÜÿßÿ≥ ÿ™ŸÇÿ±Ÿâ Ÿàÿ™ÿ™ÿπŸÑŸÖ.. ŸàŸÅŸäŸáÿß ÿ≠ÿ™Ÿâ salle de sieste ÿ™ÿπŸÖŸÑ ŸÅŸäŸáÿß ÿ™ÿπÿ≥ŸäŸÑÿ© ŸÅŸä ÿßŸÑŸÇÿßŸäŸÑÿ©.. ÿßŸäÿß ÿ±ŸÉÿ¥ÿ™ ŸÉŸäŸÖÿß ÿπÿßŸÖÿ© ÿßŸÑÿ¥ÿπÿ® Ÿàÿ®ÿØŸäÿ™ ŸÜÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑÿßÿ¥ŸÇÿßÿ° ÿßŸÑÿ®ÿ∑ÿßŸÑÿ©..\nŸàŸÑŸäÿ™ ŸÉŸÑ ŸäŸàŸÖ ŸÜŸÖÿ¥Ÿä ŸÑŸÑopen space Ÿàÿ®ÿØŸäÿ™ ŸÜÿπŸÖŸÑ programme ÿ®ÿßÿ¥ ŸÜŸÇÿ±Ÿâ ÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™.. ÿ®ÿØŸäÿ™ ŸÜŸÇÿ±Ÿâ ŸÅŸä ÿßŸÑÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÑŸÑŸä ÿ∑ÿßŸÑÿ®ŸäŸÜŸáŸÖ ŸÅŸä les appels d\u0026rsquo;offre ŸÖÿ™ÿßÿπ ÿßŸÑAirbus Defence \u0026amp; Space ŸàÿßŸÑAlcatel üòÑ\nÿ£Ÿäÿß ŸÖÿßŸÜÿ∑ŸàŸÑÿ¥ ÿπŸÑŸäŸÉŸÖ.. ÿπŸäÿ∑ŸàŸÑŸä ÿ®ÿßÿ¥ ŸÜÿπŸÖŸÑ des entretiens ŸÅŸäŸáŸÖ ÿßŸÑÿßÿ´ŸÜŸäŸÜ.. les entretiens ŸÉÿßŸÜŸàÿß ÿπÿßÿØŸäŸäŸÜ ÿ¨ÿØÿßÿß ŸàÿßŸÑÿßÿÆÿ™ŸÑÿßŸÅ ÿßŸÑŸàÿ≠ŸäÿØ ŸáŸà ÿßŸÜŸá ÿßŸÑcommercial ŸÉÿßŸÜ ÿ≠ÿßÿ∏ÿ± ŸàŸÉŸÑ ÿ¥Ÿäÿ° ŸäÿµŸäÿ± ŸÇÿØÿßŸÖŸá ÿ®ÿßÿ¥ Ÿäÿπÿ±ŸÅ ÿßÿ∞ÿß ŸÉÿßŸÜ ŸäŸÜÿ¨ŸÖ Ÿäÿ∑ŸÑÿπ ÿßŸÑÿ≥ŸàŸÖ ŸÉÿßŸÜ ÿßŸÑentretien ÿ™ÿπÿØŸâ ÿ®ÿßŸáŸä ŸàÿßŸÑÿß ŸÑÿß.. Ÿáÿßÿ∞ÿßŸÉÿß ÿπŸÑÿßŸá ÿ®ÿπÿØ ŸÖÿß ŸäŸàŸÅŸâ ÿßŸÑentretien ÿßŸÑclient ŸàÿßŸÑcommercial Ÿäÿ∑ŸÑÿ®Ÿàÿß ŸÖŸÜŸÉ ÿ™ÿ≥ÿ™ŸÜŸâ ÿßŸÑÿ®ÿ±ÿ© ÿ®ÿßÿ¥ ŸÖÿß ÿ™ÿ¥ŸàŸÅÿ¥ ÿ®ÿπŸäŸÜŸÉ ŸÉŸäŸÅÿßŸá Ÿàÿ®ŸÇÿØÿßŸá¬†ÿßŸÑfacturation ŸÖÿ™ÿßÿπŸÉ..\nÿ®ÿØŸäÿ™ ŸÜÿÆÿØŸÖ ŸÅŸä ÿßŸÑAirbus Defence \u0026amp; Space.. ÿßŸÑÿßÿÆÿ™Ÿäÿßÿ± ŸÉÿßŸÜ ÿπŸÜÿØŸä.. ŸàÿßŸÑŸÑŸä ÿ®ÿ±ÿ¥ÿ© SSII ŸÖÿßŸäÿπÿ∑ŸàŸÉÿ¥ ÿßŸÑÿßÿÆÿ™Ÿäÿßÿ±.. ŸÑŸÉŸÜ ŸÖŸÜ ÿ≠ÿ≥ŸÜ ÿ≠ÿ∏Ÿä ÿßŸÑSSII ÿßŸÑŸÑŸä ŸÉŸÜÿ™ ŸÅŸäŸáÿß ŸÉÿßŸÜŸàÿß ÿ®ÿßŸáŸäŸÜ ÿ®ÿ±ÿ¥ÿ© ŸÅŸä ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ les consultants ŸÖÿ™ÿßÿπŸáŸÖ..\nÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑSSII ŸÉÿßŸäŸÜŸáŸÖ ÿπŸäŸÑÿ© ÿßŸÑÿπÿ±Ÿàÿ≥ÿ©.. ÿπÿ∑Ÿàÿß ÿ®ŸÜÿ™ŸáŸÖ ŸàŸÖŸäÿ≥Ÿà ÿπŸÑŸâ ÿ±ŸÇÿßÿØŸáÿß.. ŸáÿßŸàŸÉÿß de temps en temps Ÿäÿ¨Ÿä ÿßŸÑcommercial Ÿäÿ∑ŸÑ ÿπŸÑŸäÿß ŸàŸäÿπŸÖŸÑ point ŸÖÿπ ÿßŸÑclient ÿ®ÿßÿ¥ Ÿäÿ™ŸáŸÜŸâ ÿßŸÑmission ŸÖÿßÿ¥Ÿäÿ© ÿ®ÿßŸÑÿ®ÿßŸáŸä ŸàÿßŸÑÿß ŸÑÿß..\nÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑclient ŸÉÿßŸäŸÜŸáŸÖ ÿ¨ÿßŸäŸáŸÖ Ÿàÿßÿ≠ÿØ ÿ®ÿ±ÿßŸÜŸä ÿ∫ÿ±Ÿäÿ® ÿπŸÑŸäŸáŸÖ.. ŸÉÿßŸÜŸàÿß Ÿäÿ™ÿπÿßŸÖŸÑŸàÿß ÿ®ŸÖŸÜÿ∑ŸÑŸÇ ŸÜÿ≠ŸÜ ŸàÿßŸÜÿ™ŸÖ.. ŸÜÿ≠ŸÜ ÿßŸÑŸÑŸä ŸáŸàŸÖÿß les internes ŸÖÿ™ÿßÿπ ÿßŸÑÿ¥ÿ±ŸÉÿ©.. ŸàÿßŸÜÿ™ŸÖ les externes.. Ÿàÿ®ÿßŸÑÿ™ÿ¨ÿ±ÿ®ÿ© ÿπŸÑŸâ ŸÇÿØ ŸÖÿßŸäŸÉŸàŸÜŸà ÿßŸÑinternes ÿßŸÉÿ´ÿ± ÿπŸÑŸâ ŸÇÿØ ŸÖÿß ŸäŸÉŸàŸÜ ÿÆŸäÿ±..\nÿ®ÿπÿØ 6 ÿ¥Ÿáÿ± mission ÿ®ÿØŸâ ÿßŸÑÿ∂ÿ±ÿ® ÿ®ŸäŸÜ ÿßŸÑclient ŸàÿßŸÑssii ÿ®ÿßÿ¥ Ÿäÿ≤ŸäÿØŸà ÿßŸÑtjm ŸÖÿ™ÿßÿπŸáŸÖ.. Ÿàÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ© ÿßŸÑconsultant¬†ŸáŸà ÿØŸäŸÖÿß ŸÜŸÇÿ∑ÿ© ÿßŸÑÿ∂ÿ∫ÿ∑.. ŸàŸáŸÜÿß ÿßŸàŸÑ conflict ŸÖÿπ ÿßŸÑssii ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿÆÿ∞ÿßŸà ÿ≤ŸäÿßÿØÿ™ŸáŸÖ ŸàŸÖÿßÿπÿ∑ŸàŸÜŸäÿ¥ ÿ≤ŸäÿßÿØÿ™Ÿä ŸÖÿπÿßŸáŸÖ.. ŸàŸáŸÜÿß ÿ®ÿØŸâ ÿßŸÑÿ∂ÿ±ÿ® ŸàÿßŸÑÿßÿÆÿ∞ ŸàÿßŸÑÿ±ÿØ Ÿàÿ™ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ ÿ≤ŸäÿßÿØÿ™Ÿä ÿ®ŸÉŸÑ ÿ±Ÿàÿ≠ ÿ±Ÿäÿßÿ∂Ÿäÿ©.. bon ŸÖŸàÿ¥ ÿ±Ÿäÿßÿ∂Ÿäÿ© ÿ®ÿ±ÿ¥ÿ©.. ÿ£ŸÖÿß ÿßŸÑÿßŸÖŸàÿ± ŸÖÿ±ŸäŸÇŸÑÿ©.. üòÅ\nŸàŸÜÿµŸäÿ≠ÿ™Ÿä ŸÑŸäŸÉ.. ŸÖŸáŸÖÿß ŸÉÿßŸÜ Ÿàÿßÿ≠ÿØ ÿ≠ÿ® ŸäÿπŸÖŸÑŸÉ bras de fer ŸÉÿ®ÿ¥ ŸÅŸä ÿßÿÆÿ™Ÿäÿßÿ±ÿßÿ™ŸÉ ŸàŸÖŸáŸÖÿß ŸÉÿßŸÜ ÿßŸÑÿ´ŸÖŸÜ ÿØÿßŸÅÿπ ÿπŸÑŸâ ÿÆÿ∑ ÿßŸÑÿ≥Ÿäÿ± ŸÖÿ™ÿßÿπŸÉ ŸÖŸáŸÖÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ∂ÿ±ŸàŸÅ.. ŸÖŸáŸÖÿß ŸÉÿßŸÜ ÿßŸÑÿ∑ŸÑÿ® ŸÖÿ™ÿßÿπŸÉ.. ÿ≤ŸäÿßÿØÿ© ŸàÿßŸÑÿß mission ŸàÿßŸÑÿß ÿßŸä ÿ≠ÿßÿ¨ÿ©.. ÿØÿßŸÅÿπ Ÿàÿßÿ∂ÿ±ÿ® ÿ®ÿßŸÑÿ®ŸàŸÜŸäÿ©.. ŸàŸÉÿßŸÜ ÿ™ÿ±ÿÆ ŸÖÿ±ÿ© ÿ∑ŸàŸÑ ÿπŸÖÿ±ŸÉ ÿ™ÿ®ŸÇŸâ ÿÆÿßÿ≥ÿ±.. mission ŸÖÿß ÿ™ÿπÿ¨ÿ®ŸÉÿ¥ ÿ®ÿØŸÑŸáÿß ŸàŸÅŸàÿ±ÿß ŸÖÿßÿ™ÿµÿ®ÿ±ÿ¥ ÿπŸÑŸâ ÿßŸÑŸÖÿ±ÿßÿ±ÿ©.. ÿßŸÑsalaire ŸÖŸáŸÖ Ÿäÿßÿ≥ÿ± ŸÖÿßÿ™ŸÜÿ≥ÿßÿ¥ apr√®s tout ÿßŸÜÿ™ salari√© ŸÖÿßŸÉÿ¥ ŸÖÿ™ÿ∑Ÿàÿπ ÿ®ÿßÿ¥ ŸÖÿßÿ™ÿÆŸÖŸÖÿ¥ ŸÅŸä ÿßŸÑŸÅŸÑŸàÿ≥.. ŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© ŸÖŸàÿßŸÇÿπ ÿ™ÿπÿ∑ŸäŸÉ ŸÅŸÉÿ±ÿ© ÿ®ÿßŸáŸäÿ© ÿπŸÑŸâ ŸÖÿπÿØŸÑ les salaires ŸÅŸä ÿßŸÑposte ÿßŸÑŸÑŸä ÿßŸÜÿ™ ŸÅŸäŸá ŸÉŸäŸÖÿß ÿßŸÑGlassdoor ÿßŸÑŸÑŸä ŸÜŸÜÿµÿ≠ŸÉŸÖ ÿ®ŸäŸá.. ÿßŸÑÿÆÿØŸÖÿ© ŸÑŸàŸÉÿßŸÜ ŸÖÿßÿ™ŸÉŸàŸÜÿ¥ ŸÖŸÖÿ™ÿπÿ© Ÿäÿß ÿßŸÖÿß ŸáŸä ŸÖŸàÿ¥ ŸÖÿ™ÿßÿπŸÉ Ÿäÿß ÿßŸÖÿß ÿßŸÜÿ™ ÿÆÿßÿ∑ŸäŸáÿß..\nÿßŸäÿß ŸÜŸÉŸÖŸÑ ÿßŸÑÿ≠ŸÉÿßŸäÿ©.. ŸÉŸÖŸÑÿ™ ÿπÿßŸÖ ŸÅŸä ÿßŸÑAirbus Ÿàÿ®ÿØŸäÿ™ ŸÜÿ≠ÿ® ŸÜÿ®ÿØŸÑ.. ÿ≠ŸÉŸäÿ™ ŸÖÿπ ÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÖÿ™ÿßÿπŸä ÿ®ÿßÿ¥ ŸÜÿ®ÿØŸÑ ŸàŸÉÿßŸÜŸàÿß ŸÖÿ±ÿ≠ÿ®ŸäŸÜ ÿ®ÿßŸÑŸÅŸÉÿ±ÿ©.. ÿßŸÉŸäÿØ client ÿ¨ÿØŸäÿØ ŸÖÿπŸÜÿßŸáÿß visibilit√© ÿßŸÉÿ´ÿ± ŸÑŸÑÿ¥ÿ±ŸÉÿ©.. ŸàŸÅÿ±ÿµ business ÿßŸÉÿ´ÿ±..\nŸàŸÖŸÜ ÿ®ÿπÿØ ŸÖÿ¥Ÿäÿ™ ŸÑŸÑCanal+ ŸàŸÑŸÑSoci√©t√© G√©n√©rale.. ŸàŸÉŸÜÿ™ ŸÜÿ≠ÿ±ÿµ ÿßŸÜŸä ŸÜÿ®ŸÇŸâ ÿ≠ŸàÿßŸÑŸä ÿπÿßŸÖ ŸÅŸä ŸÉŸÑ mission ŸÖÿß ÿßŸÉÿ´ÿ±ÿ¥.. ŸàŸáÿ∞ÿß ŸÑÿ≥ÿ®ÿ® ÿ≥ÿßŸáŸÑ ÿ®ÿ±ÿ¥ÿ©.. ÿ™ŸÇŸÜŸäÿß.. ŸÉŸäŸÅ ÿ™ÿπŸÖŸÑ ÿπÿßŸÖ ŸÅŸä projet ÿ®ÿßÿ¥ ÿ™ŸÉŸàŸÜ ÿ¥ŸÅÿ™ ÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™ Ÿàÿßÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑprojet ÿµÿ∫Ÿäÿ± ŸàÿßŸÑÿß ŸÖÿ™Ÿàÿ≥ÿ∑.. ŸàÿßŸÑŸÑŸä taille ŸÖÿ™ÿßÿπŸá ÿ™ŸÉŸàŸÜ ÿßŸÇŸÑ ŸÖŸÜ 2 ŸÖŸÑŸäŸàŸÜ ÿ≥ÿ∑ÿ±.. ÿ™ŸÜÿ¨ŸÖ ŸÅŸä ÿπÿßŸÖ ÿ®ÿ≥ŸáŸàŸÑÿ© ÿ™ŸÉŸàŸÜ ÿπŸÖŸÑÿ™ ÿßŸÑtour ŸÖÿ™ÿßÿπŸá ŸàÿÆÿ∞Ÿäÿ™ connaissance ÿ®ÿßŸáŸäÿ© ÿπŸÑŸâ ÿßŸÑcode ŸàÿπŸÑŸâ ÿßŸÑarchitecture.. ÿßŸÑŸÉŸÑÿßŸÖ Ÿáÿ∞ÿß ŸÖÿß ŸäŸÜÿ∑ÿ®ŸÇÿ¥ ÿπŸÑŸâ ÿßŸÑpartie m√©tier.. ÿßŸÑŸÑŸä ŸÖÿßŸÉÿßŸÜÿ™ÿ¥ ÿ™ÿπŸÜŸäŸÜŸä ŸÉŸäŸÖÿß ÿßŸÑtechnique.. ŸàÿπŸÖŸÑÿß ÿ®ŸÖŸÇŸàŸÑÿ© \u0026ldquo;ÿÆÿ∞ ŸÖŸÜ ŸÉŸÑ ÿ®ÿ≥ÿ™ÿßŸÜ ÿ≤Ÿáÿ±ÿ©\u0026rdquo; ŸàŸÑŸäÿ™ ŸÉŸÑ ÿπÿßŸÖ ÿπŸÜÿØ client.. ŸÖŸÜ ÿßŸÑdefense ŸÑŸÑm√©dias ŸÑŸÑfinances.. ÿ®ÿ±ÿ¥ÿ© ÿµÿ≠ÿßÿ®Ÿä ŸÉÿßŸÜŸàÿß ŸäŸÇŸàŸÑŸàŸÑŸä ŸÖŸàÿ¥ ÿ®ÿßŸáŸä ÿßŸÑŸÑŸä ÿ™ÿπŸÖŸÑ ŸÅŸäŸá ŸàŸÑÿßÿ≤ŸÖ ÿ™ÿ®ŸÇŸâ stable ŸÅŸä ÿ®ŸÑÿßÿµÿ© Ÿàÿ≠ÿØÿ©.. ŸÑŸÉŸÜŸä ŸÉÿ®ÿ¥ÿ™ ŸÅŸä ŸÖÿ®ÿØÿ£ ÿßŸÑÿπÿßŸÖ ŸÅŸä ŸÉŸÑ ÿ®ŸÑÿßÿµÿ©.. ŸÇŸÖÿß ÿ≥ÿ®ÿ® ÿ´ÿßŸÜŸä ÿÆŸÑÿßŸÜŸä ŸÜŸÉÿ®ÿ¥ ÿßŸÜŸä ŸÖÿßŸÜÿ∑ŸàŸÑÿ¥ ŸÅŸä mission ŸáŸà ÿßŸÜŸá ŸÉŸÜÿ™ ÿ∂ÿØ ŸÖÿ®ÿØÿ£ ŸáÿßŸä ÿ®ŸÑÿßÿµÿ© ÿ®ÿßŸáŸäÿ©.. ŸÖÿß ÿ™ÿ≥ŸÑŸÖÿ¥ ŸÅŸäŸáÿß.. ŸÉÿßŸÜÿ™ ÿπŸÜÿØŸä ŸÇŸÜÿßÿπÿ© ÿßŸÑŸÑŸä ÿπŸÑŸâ ŸÇÿØ ŸÖÿß ŸÜÿ∑ŸÑÿπ techniquement ÿπŸÑŸâ ŸÇÿØ ŸÖÿß ÿ®ÿßÿ¥ ÿ™ÿ¨Ÿä des missions ÿÆŸäÿ±.. ŸàÿßŸÑÿØŸÑŸäŸÑ ÿßŸÜŸà ÿ™Ÿàÿ¨ŸáŸä ŸÖÿßŸÉÿßŸÜÿ¥ ÿ∫ÿßŸÑÿ∑ ÿßŸàŸÑ misson ŸÅŸä ÿßŸÑconsulting ÿπÿßŸÖ 2014 ŸÉÿßŸÜÿ™ client lourd ÿ®ÿßŸÑswing Ÿàbackend ÿ®ÿßŸÑEJB 3 ŸàÿßÿÆÿ± Ÿàÿ≠ÿØÿ© ŸÉÿßŸÜÿ™ microservices Ÿàkubernetes ŸàÿßŸÑcloud native ŸÉÿßŸÜ ÿπŸÑŸâ ÿπÿ¨ŸÑÿ© üòÅ\nŸÅŸä Ÿàÿ≥ÿ∑ ÿßŸÑconsulting.. ŸÉÿßŸÜÿ™ ÿπŸÜÿØŸä ÿ±ÿ∫ÿ®ÿ© ŸÉÿ®Ÿäÿ±ÿ© ÿßŸÜŸà ŸÜŸÅŸáŸÖ ŸáÿßŸÑÿØŸàŸÖÿßŸÜ ÿßŸÑÿ∫ÿ±Ÿäÿ® ÿßŸÑŸÑŸä ÿ≥Ÿäÿ∑ÿ± ŸÅŸä ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑŸÉŸÑ ŸÉŸäŸÅÿßŸá ŸäÿÆÿØŸÖ.. ŸÅŸÉŸÜÿ™ ŸÉŸÑ ŸÖÿßÿ™ÿ¨Ÿä ŸÅÿ±ÿµÿ© ÿ®ÿßÿ¥ ŸÜÿ¥ÿßÿ±ŸÉ ŸÅŸä appel d\u0026rsquo;offre ŸàÿßŸÑÿß √©valuation technique ŸÑcandidat ŸÖÿßŸÜŸÇŸàŸÑÿ¥ ŸÑÿß.. ÿ®ÿπÿØ ÿπÿßŸÖ ÿÆÿØŸÖÿ©.. ÿ¥ÿØŸäÿ™ ÿßŸÑrecrutement ŸàŸàŸÑŸäÿ™ ŸÜÿ≠ÿ® ŸÜŸÅŸÉ ÿ®ŸÑÿßÿµÿ© ÿ£ŸÉÿ®ÿ±.. ŸÅÿ≠ÿßŸàŸÑÿ™ ÿßŸÜŸä ÿßŸÜÿ∏ŸÖ ÿ£ŸàŸÑ Conf√©rence ŸÑŸÑNetBeans ÿ®ŸäŸÜ ÿßŸÑSSII ŸàÿßŸÑOracle.. Ÿàÿ™ŸÖÿ™ ÿßŸÑOrganisation ÿ®ŸÜÿ¨ÿßÿ≠.. Ÿàÿ™ÿπŸÖŸÑ ÿ£ŸàŸÑ NetBeans Day Paris 2015 ŸÅŸä ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿßŸÑŸÑŸä ŸÜÿÆÿØŸÖ ŸÅŸäŸáÿß.. ü•≥ü§©üòç Ÿàÿ™ŸàÿßÿµŸÑÿ™ ÿßŸÑcollaboration ŸÖÿπ ÿßŸÑOracle ŸÑŸäŸÜ ŸàÿµŸÑ ÿ™ÿ≠ÿ∑ Logo ÿßŸÑSSII ÿßŸÑŸÑŸä ŸÜÿÆÿØŸÖ ŸÅŸäŸáÿß ÿπŸÑŸâ ÿßŸÑtshirt officiel ŸÖÿ™ÿßÿπ ÿßŸÑNetBeans ü•≥ü§©üòç\n ŸÅŸä ÿßÿÆÿ± 2015.. ÿπŸÜÿØ ÿßŸÑAirbus Defence \u0026amp; Space Ÿàÿßÿ¨Ÿáÿ™ŸÜŸä ŸÖÿ¥ÿßŸÉŸÑ technique ŸÖÿπ framework Java ÿßÿ≥ŸÖŸà Apache Shiro.. ŸÉŸäŸÅ ŸÖÿ¥Ÿäÿ™ ŸÖŸÜ ÿ®ÿπÿØŸáÿß ŸÑCanal+ ÿ≠ÿ∑Ÿäÿ™Ÿà ŸÅŸä ŸÖÿÆŸä ŸÇŸÑÿ™ ŸÑÿßÿ≤ŸÖ ŸÜŸÅŸáŸÖŸà ŸàŸÜŸÅŸáŸÖ ÿ≤ŸÖŸÑÿßÿ¶Ÿä ŸÅŸäŸá.. ÿ≠ÿ®Ÿäÿ™ ŸÜŸÉÿ™ÿ® tutoriel ÿπŸÑŸäŸá.. Ÿàÿ®ÿØŸäÿ™ ŸÜÿÆÿØŸÖ ÿπŸÑŸâ ÿßŸÑobjectif Ÿáÿ∞ÿßŸÉÿß.. tutoriel ÿ®ÿßŸáŸä ŸäŸÉŸàŸÜ ŸÖÿØÿÆŸÑ ŸÑŸäÿß ŸÅŸä ÿßŸÑcommunity ŸÖÿ™ÿßÿπ ÿßŸÑJava ÿßŸÖ ÿßŸÑÿØŸÜŸäÿß..\nŸÅŸä ÿßŸÑ2016.. ÿ≤ÿØŸÜÿß ÿπŸÖŸÑŸÜÿß NetBeans Day France Ÿàÿ≤ÿßÿØÿ™ ÿ¨ÿßÿ™ ÿßŸÑŸÅÿ±ÿµÿ© ŸÅŸä NetBeans Day London.. Ÿàÿ®ÿØŸâ ŸÜÿ≥ŸÇ ÿßŸÑŸÜÿ¥ÿßÿ∑ ÿßŸÑÿ™ŸÇŸÜŸä Ÿäÿ™ÿ≤ÿßŸäÿØ..\nŸàÿßŸÑtutoriel ÿ®ÿØŸâ ŸäŸÉÿ®ÿ± ŸàŸÑÿß ŸÉÿ™ÿßÿ®.. ŸàŸÉÿßŸÜ ŸáŸà ÿßŸàŸÑ ŸÉÿ™ÿßÿ® ŸÑŸäÿß.. ŸàÿßŸÑŸÑŸä ÿßÿ≥ŸÖŸá Pairing Apache Shiro and Java EE 7..\nŸÅŸä ÿ¨ŸàÿßŸÜ 2016.. ÿÆÿ±ÿ¨ÿ™ ŸÖŸÜ ÿßŸÑCanal+ Ÿàÿ¥ÿØŸäÿ™ ÿßŸÑÿ±ÿ≠ÿßŸÑ ŸÑŸÑSoci√©t√© G√©n√©rale ÿ®ÿßÿ¥ ŸÜÿÆŸàÿ∂ ÿßÿµÿπÿ® ÿ™ÿ¨ÿ±ÿ®ÿ© ŸÅŸä ÿßŸÑcarri√®re ŸÖÿ™ÿßÿπŸä.. ŸàŸáŸä ÿßŸÜŸä ŸÜÿ™ÿπÿØŸâ Technical Leader ÿπŸÑŸâ Projet ŸÉÿ®Ÿäÿ± ŸÅŸä ÿ¥ÿ±ŸÉÿ© ŸÉÿ®Ÿäÿ±ÿ©..\nŸÖŸÜ ŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑConsulting ÿßŸÜŸÉ ŸÅŸä ŸÖÿπÿ∏ŸÖ ÿßŸÑÿ£ÿ≠ŸäÿßŸÜ ÿ™ŸÉŸàŸÜ Ÿàÿ≠ÿØŸÉ ÿ¨ÿßŸä ŸÖŸÜ ÿßŸÑSSII ÿπŸÑŸâ Projet ŸÖÿπŸäŸÜ.. ÿßŸÑÿ≠ÿßŸÑÿ© Ÿáÿßÿ∞Ÿä ÿ™ÿÆŸÑŸä ÿßŸÑÿÆÿØŸÖÿ© ÿ£ÿµÿπÿ® ÿ¥ŸàŸäÿ©.. ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ŸÖÿßŸÅŸÖÿßÿ¥ ŸàŸÑÿØ ÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÖÿ™ÿßÿπŸÉ ÿßŸÑŸÑŸä ÿ®ÿßÿ¥ ŸäÿπÿßŸàŸÜŸÉ.. ÿ≠ÿ™Ÿâ ÿ≤ŸÖŸÑÿßÿ¶ŸÉ ŸÅŸä ÿßŸÑprojet ÿßŸÑŸÑŸä ÿ¨ÿßŸäŸäŸÜ ŸÖŸÜ ÿ¥ÿ±ŸÉÿßÿ™ ÿßÿÆÿ±Ÿâ ÿØŸäŸÖÿß ÿ®ÿßÿ¥ ŸäŸÉŸàŸÜ ÿ®ŸäŸÜÿßÿ™ŸÜÿß esprit comp√©titif.. Ÿàÿ≠ÿ™Ÿâ ŸÉŸäŸÅ ÿ™ÿ≥ÿ™ÿ≠ŸÇ ŸàŸÖÿßŸÅŸÖÿßÿ¥ ÿ¥ŸÉŸàŸÜ ŸÅŸä ÿßŸÑProjet ŸäŸÜÿ¨ŸÖ ŸäÿπÿßŸàŸÜ les SSII ŸÖÿßÿπŸÜÿØŸáŸÖÿ¥ des centres de comp√©tences ŸÅŸäŸáŸÖ ÿπÿ®ÿßÿØ ÿ™ÿπÿßŸàŸÜŸÉ.. apr√®s tout ŸáŸàŸÖÿß Ÿäÿ≠ÿ∑Ÿàÿß¬†des ing√©nieurs ŸÅŸä ÿ¥ÿ±ŸÉÿßÿ™ clientes.. ŸàŸáÿ∞ÿß ÿßŸÑŸÑŸä ŸäŸÖÿ´ŸÑ ÿ£ŸÉÿ®ÿ± ÿßÿ¥ŸÉÿßŸÑ ŸÅŸä ÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑSSII ÿ®ÿµŸÅÿ© ŸÉÿ®Ÿäÿ±ÿ©..\nŸÉÿßŸÜÿ™ ÿßŸÑexp√©rience ŸÅŸä ÿßŸÑsoci√©t√© g√©n√©rale ŸÖŸÜ ÿ£ÿ±Ÿàÿπ les missions ÿßŸÑŸÑŸä ÿπŸÖŸÑÿ™Ÿáÿß.. ŸÅŸäŸáÿß ÿ®ÿ±ÿ¥ÿ© ŸÖÿ¥ÿßŸÉŸÑ technique.. ÿ®ÿ±ÿ¥ÿ© ŸÖÿ¥ÿßŸÉŸÑ gestion de projet ŸàÿÆÿßÿµÿ© ŸÖÿ¥ÿßŸÉŸÑ ŸÖÿπ ÿßŸÑÿßÿ¥ÿÆÿßÿµ ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ŸÉŸÜÿ™ ÿßÿµÿ∫ÿ± Ÿàÿßÿ≠ÿØ ŸÅŸä ÿßŸÑÿπŸÖÿ± ŸàŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™ ŸÉŸÜÿ™ ÿßŸÑtechnical leader ŸÖÿ™ÿßÿπ ÿßŸÑprojet.. ŸàÿßŸÉŸäÿØ ŸÖŸàÿ¥ ÿ≥ÿßŸáŸÑ ÿßŸÜŸá ÿ™ŸÅÿ±ÿ∂ des choix ÿπŸÑŸâ ŸÜÿßÿ≥ ÿßŸÜÿ™ ÿßÿµÿ∫ÿ± ŸÖŸÜŸáŸÖ ÿ®15 ÿ≥ŸÜÿ© üòÅ\nŸÅŸä ÿ¨ŸàÿßŸÜ 2017.. ÿßŸäŸá ŸÜÿπŸÖ.. ÿ®ÿπÿØ ÿπÿßŸÖ ŸÅŸä ÿßŸÑsoci√©t√© g√©n√©rale.. ÿ≠ÿ≥Ÿäÿ™ ÿßŸÑŸÑŸä ŸÖÿßÿπÿßÿØÿ¥ ÿßŸÜÿ¨ŸÖ ŸÜŸÉŸÖŸÑ ŸÅŸä ÿßŸÑSSII ÿßŸÑŸÑŸä ŸÜÿÆÿØŸÖ ŸÅŸäŸáÿß.. ÿ¨Ÿäÿ™ ŸÜÿ≠ÿ≥ÿ® ŸÅŸäŸáÿß ŸÜŸÑŸÇŸâ ÿßŸÑŸÑŸä ÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÖÿßŸáŸäÿ¥ ŸÖÿßÿ¥Ÿäÿ© ŸÅŸä ÿ∑ÿ±ŸäŸÇ ÿ™ÿ≠ÿ≥ŸäŸÜ les activit√©s ŸÖÿ™ÿßÿπŸáÿß.. ÿπŸÑŸâ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ 2000 ŸÖŸáŸÜÿØÿ≥.. ŸÅŸÖÿß ŸÉÿßŸÜ 10 ŸÖŸÜ ŸÜÿßÿ≥ actifs ŸàÿßŸÑÿ®ÿßŸÇŸä ÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÖÿßÿ≠ÿ®ÿ™ ÿ™ÿπŸÖŸÑ ÿ¥Ÿäÿ° ÿ®ÿßÿ¥ ÿ™ÿ≠ÿ±ŸÉŸáŸÖ ŸàÿßŸÑÿß ÿ™ÿ≠ŸÖÿ≥ŸáŸÖ..\nŸàŸáŸÜÿß.. ÿ¥ÿØŸäÿ™ ÿßŸÑÿ±ÿ≠ÿßŸÑ ŸÑÿ´ÿßŸÜŸä ÿ¥ÿ±ŸÉÿ© SSII ÿÆÿØŸÖÿ™ ŸÅŸäŸáÿß.. ŸàŸáŸÜÿß ŸàŸÑÿß ÿ™ÿµŸÜŸäŸÅ ÿßŸÑSSII Ÿäÿ™ŸÇÿßŸÑ ÿπŸÑŸäŸáÿß ÿßŸÑÿßŸÉÿ´ÿ±Ÿäÿ© ESN ÿ®ÿπÿØ ŸÖÿ¨ŸÖŸàÿπÿ© ŸÇŸàÿßŸÜŸäŸÜ ÿ¨ÿØŸäÿØÿ© ÿÆÿ±ÿ¨ÿ™ ŸÅŸä ÿ¨ŸàŸäŸÑŸäÿ© 2017.. ÿ™Ÿàÿ© ÿ®ÿßÿ¥ ŸÜŸÉŸÖŸÑ ÿßŸÑArticle ŸÜÿ≠ŸÉŸä ŸÅŸäŸá ESN ÿπŸÑŸâ ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ©..\nŸÇÿ®ŸÑ ŸÖÿß ŸÜŸÖÿ¥Ÿä ŸÑŸÑESN ÿßŸÑÿ¨ÿØŸäÿØÿ©.. ŸÉŸäŸÅ ÿπŸÖŸÑÿ™ ŸÖÿπÿßŸáŸÖ les entretiens.. ŸÇŸÑÿ™ŸÑŸáŸÖ ÿ±ÿßŸÜŸä ŸÜÿ≠ÿ® ŸÜŸÖÿ¥Ÿä ŸÑÿ®ŸÑÿßÿµÿ© ŸÜÿ™ÿ≠ÿ±ŸÉ ŸÅŸäŸáÿß Ÿàÿ™ÿ™ÿ≠ÿ±ŸÉŸàÿß ŸÖÿπÿßŸäÿß.. ŸàŸÉÿßŸÜ Ÿáÿ∞ÿß ÿ£ŸáŸÖ ÿ∑ŸÑÿ® ŸÖŸÜ ÿ∑ŸÑÿ®ÿßÿ™Ÿä.. ŸÖÿπ ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑŸÖÿßÿØŸäÿ© ÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ©.. üòÅ ÿ≠ŸÉŸäÿ™ŸÑŸáŸÖ ÿ≤ÿßÿØÿ© ÿπŸÑŸâ ÿßŸÑcycle ŸÖÿ™ÿßÿπ ÿπÿßŸÖ max ÿßŸÑŸÑŸä ŸÜÿ≠ÿ® ÿØŸäŸÖÿß ŸÜÿπŸÖŸÑŸà..\nŸàŸáŸÜÿß ŸÅÿßÿ¨ÿ¶ŸàŸÜŸä ÿ®ÿßŸÑproposition ŸÖÿ™ÿßÿπŸáŸÖ.. ÿßŸÑŸÑŸä ŸáŸä ÿ™ÿπŸÖŸÑ consulting multi-clients.. ÿßŸäŸá ŸÜÿπŸÖ.. ÿ≠Ÿäÿßÿ™Ÿä ÿ™ÿ®ÿØŸÑÿ™ ŸÖŸÜ ÿ¥Ÿáÿ± ÿ£Ÿàÿ™ 2017.. ÿ®ÿØŸäÿ™ ŸÜÿπŸÖŸÑ ŸÅŸä 3 missions ŸÅŸä ÿßŸÑÿ¨ŸÖÿπÿ©.. ŸÖÿπŸÜÿßŸáÿß 3 ŸÖÿ¥ÿßÿ±Ÿäÿπ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÖÿπŸÜÿßŸáÿß 3 ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÅÿ±ÿØ ŸàŸÇÿ™.. ŸàŸáŸÜÿß ŸÉÿßŸÜ ÿßŸÑÿ¨Ÿà ŸàÿßŸÑÿ±ÿ®ŸàÿÆ ÿ®ÿßŸÑÿ±ÿ≥ŸÖŸä.. ŸÉŸÜÿ™ ÿ¥ÿßŸäÿÆ Ÿàÿ®ÿßŸÑÿ¨Ÿà ÿØÿßŸäÿÆ.. ŸàŸáŸÜÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÅÿ±ÿµÿ© ÿßŸÜŸä ŸÜÿ±ŸÉÿ≤ ÿπŸÑŸâ probl√©matique ŸÖÿπŸäŸÜÿ© ÿßŸÑŸÑŸä ŸáŸä ÿßŸÑCloud Native Developement ŸàŸÅÿπŸÑÿß ÿ®ÿØŸäÿ™ ŸÜÿÆÿØŸÖ ÿπŸÑŸäŸáŸÖ ÿπŸÜÿØ ÿ®ÿ±ÿ¥ÿ© clients.. ŸàŸáÿ∞ÿß ÿßŸÑŸÑŸä ÿπÿ∑ÿßŸÜŸä ŸÅŸÉÿ±ÿ© ŸÉÿ™ÿßÿ®Ÿä ÿßŸÑÿ´ÿßŸÜŸä ÿßŸÑŸÑŸä ŸÜÿ¥ÿ±ÿ™Ÿá ÿ®ÿπÿØ 15 ÿ¥Ÿáÿ± ÿÆÿØŸÖÿ© ŸÅŸä ÿßŸÑrythme ÿßŸÑÿ¨ÿØŸäÿØ.. ÿßŸÑŸÉÿ™ÿßÿ® ÿßŸÉŸäÿØ ÿ™ÿπÿ±ŸÅŸàŸá Ÿàÿßÿ≥ŸÖŸá Playing with Java Microservices on Kubernetes and OpenShift ..\nÿπÿßŸÖ 2019.. ŸÉÿßŸÜ ÿÆÿßŸÖÿ≥ ŸÑŸäÿß ŸÅŸä ÿßŸÑConsulting.. ÿ®ÿØŸäÿ™ ŸÜŸÇŸÑŸÇ ŸÖŸÜ Ÿáÿ≤ ÿØÿ®Ÿäÿ¥ÿßÿ™ŸÉ Ÿàÿ¥ŸÑŸÉ ÿ¥ŸÑŸÉ ŸÖŸÜ Client ŸÑClient.. ÿ®ÿØŸäÿ™ ŸÜÿ™ÿ≥ÿßÿ°ŸÑ ÿπÿßŸÑfinalit√© ŸÖÿ™ÿßÿπ ÿßŸÑÿ™ÿ®ÿØŸäŸÑ ŸÖŸÜ ÿ®ŸÑÿßÿµÿ© ŸÑÿ®ŸÑÿßÿµÿ©.. ŸàÿÆÿµŸàÿµÿß ÿßŸÜŸá ÿπŸÖŸÑÿ™ ÿßŸÑtour ŸÖÿ™ÿßÿπ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ®ÿßŸáŸäÿ© ŸÖŸÜ ÿ£ŸÉÿ®ÿ± les clients ŸÅŸä ÿßŸÑmarch√© Parisien.. ŸÅŸä ÿßŸÑŸàŸÇÿ™ Ÿáÿ∞ÿß ÿ®ÿØŸäÿ™ ŸÖÿÆŸÖŸÖ ŸÅŸä ÿßŸÜŸä ŸÜÿ™ÿπÿØŸâ ŸÖŸÜ ÿßŸÑConsulting ŸàŸÜŸÖÿ¥Ÿä ÿπŸÜÿØ √âditeur ŸàÿßŸÑÿß Cloud Provider.. ŸàŸáŸÜÿß ŸÜÿµÿ≠ŸÜŸä Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿßÿµÿØŸÇÿßÿ¶Ÿä ÿßŸÜŸä ŸÜÿµÿ® Candidature ÿπŸÜÿØ ÿßŸÑMicrosoft ü§©\nÿßŸÑBagage ÿßŸÑŸÑŸä ŸÑŸÖŸäÿ™Ÿá ŸÅŸä ÿ≥ŸÜŸàÿßÿ™ ÿßŸÑConsulting ŸáŸà ÿßŸÑŸÑŸä ŸÉÿßŸÜ Catalyseur ŸÑŸäÿß ŸÅŸä ÿßŸÑÿ™ÿ≥ÿπÿ© entretienet ÿßŸÑŸÑŸä ÿπÿØŸäÿ™ŸáŸÖ ÿ®ÿßÿ¥ ÿ™ŸÇÿ®ŸÑÿ™¬†üòÅ ÿ≥Ÿàÿßÿ° ÿßŸÑTechnique ŸàÿßŸÑÿß ÿßŸÑexp√©rience ÿßŸÑŸÑŸä ÿπŸÖŸÑÿ™Ÿáÿß ŸÅŸä ÿßŸÑrecrutement ŸÖÿπ ÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑESN ÿßŸÑŸÑŸä ÿ™ÿπÿØŸäÿ™ ÿ®ŸäŸáŸÖ.. ŸàÿßŸÑBagage Ÿáÿ∞ÿßŸÉÿß ŸÜŸÅÿ≥Ÿá ÿπÿßŸàŸÜŸä ÿ®ÿ±ÿ¥ÿ© ŸÅŸä ÿßŸÑOracle Groundbreaker Award ..\nŸàÿ®ÿØÿ£ ÿßŸÑŸÖÿ¥Ÿàÿßÿ± ŸÖÿπ ÿßŸÑMicrosoft ü§© ŸàŸáŸÜÿß ÿ™ŸàŸÅŸâ ŸÇÿµÿ© ÿÆŸÖÿ≥ÿ© ÿ≥ŸÜŸäŸÜ Consulting ŸÉÿßŸÜÿ™ ŸÖÿπÿ®Ÿäÿ© ÿ®ÿßŸÑÿ™ÿπÿ® ŸàÿßŸÑÿ¨Ÿà ŸàÿßŸÑŸÅÿ¥ŸÑ ŸàÿßŸÑŸÜÿ¨ÿßÿ≠..\nŸÅŸä ÿßŸÑÿÆŸÖÿ≥ÿ© ÿ≥ŸÜŸäŸÜ ÿßŸÑŸÑŸä ÿπÿØŸäÿ™ŸáŸÖ ŸÅŸä ÿßŸÑConsulting ÿßŸÜÿ¨ŸÖ ŸÜÿπŸÖŸÑ statistiques ÿµÿ∫Ÿäÿ±ÿ©..\n- 2 ÿ¥ÿ±ŸÉÿßÿ™ SSII\n- 9 ÿ¥ÿ±ŸÉÿßÿ™ Clients\n- ÿ£ŸÉÿ´ÿ± ŸÖŸÜ 20 conf√©rences Ÿà workshops\n- ÿ™ŸÉŸàŸäŸÜ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ 500 ÿ∑ÿßŸÑÿ® ŸàŸÖŸáŸÜÿØÿ≥\nŸÅŸä Ÿàÿ≥ÿ∑ Ÿáÿ∞ÿß ÿßŸÑŸÉŸÑ ÿßŸÜÿ¨ŸÖ ŸÜÿπÿ∑ŸäŸÉŸÖ ÿßÿ¥ ÿßÿ≥ÿ™ÿÆŸÑÿµÿ™:\n- ÿ®ÿ±ÿ¥ÿ© des consultants ÿ∑Ÿäÿßÿ±ÿ© techniquement ŸÖÿßŸäÿπÿ±ŸÅŸàÿ¥ ŸäÿπŸÖŸÑŸàÿß CV ŸàŸÖÿßŸäÿπÿ±ŸÅŸàÿ¥ Ÿäÿ®ŸäŸÜŸàÿß ŸÇŸàÿ™ŸáŸÖ ŸÅŸä entretien d\u0026rsquo;embauche..\n- ÿ®ÿ±ÿ¥ÿ© des consultants ÿ∑Ÿäÿßÿ±ÿ© ÿ®ÿ±ÿ¥ÿ© ŸàŸÖÿßŸäÿπÿ±ŸÅŸàÿ¥ ŸäŸÇŸäŸÖŸàÿß ÿßŸÑpr√©tentions salariales ŸÖÿ™ÿßÿπŸáŸÖ.. ŸÅŸä ÿ≠ŸäŸÜ ÿßŸÜŸá ÿ®ÿ±ÿ¥ÿ© des sites web ŸÉŸäŸÅ ÿßŸÑGlassdoor¬†ŸäÿπÿßŸàŸÜŸàŸÉ ÿ®ÿßÿ¥ ÿ™ÿπÿ±ŸÅ ÿßŸÑposte ÿßŸÑŸÑŸä ÿ™ÿÆÿØŸÖ ŸÅŸäŸá ŸÇÿØÿßÿ¥ ÿßŸÑsalaire moyen ŸÖÿ™ÿßÿπŸá..\n- ÿ®ÿ±ÿ¥ÿ© des consultants Ÿäÿ±ŸÉÿ¥Ÿàÿß ŸÑÿ≥ŸÜŸàÿßÿ™ ÿ∑ŸàŸäŸÑÿ© ŸÅŸä des projets ÿßŸÑŸÑŸä les technologies ŸÖÿ™ÿßÿπŸá obsol√®tes ŸàŸàŸÑŸâ ÿπŸÑŸäŸáÿß ÿßŸÑÿØŸáÿ±.. ŸÅŸä ÿ≠ŸäŸÜ ÿßŸÜŸá ŸÜŸáÿßÿ±ÿßŸÑŸÑŸä ÿ™ŸàŸÅŸâ ÿßŸÑmission ŸäŸÑŸÇŸâ ÿßŸÑConsultant ÿ±Ÿàÿ≠Ÿá ŸÅŸä ÿßŸÑÿ™ÿ≥ŸÑŸÑ.. Ÿàÿ™ÿ±ÿµŸäŸÑŸá Ÿäÿ™ŸÇÿ∑ÿπ ŸÅŸä ÿßŸÑr√©cup√©ration ÿ®ÿßÿ¥ ŸäŸÉŸàŸÜ √† jour ŸÖÿπ ÿßŸÑmarch√©..\n- ÿ®ÿ±ÿ¥ÿ© des consultants ŸÖÿß Ÿäÿ≠ÿ®Ÿàÿ¥ Ÿäÿ™ÿ®ÿπŸàÿß ÿßŸÑupstream ŸÖÿ™ÿßÿπ les technologies ŸàŸÖÿßŸäÿ≠ÿ®Ÿàÿ¥ ŸäŸÜÿ¥ÿ∑Ÿàÿß ÿ≠ÿ™Ÿâ ÿ®ÿßŸÑÿ≠ÿ∂Ÿàÿ± ŸÅŸä des √©v√©nements ŸÅŸä ÿ≠ŸäŸÜ ÿßŸÑElixir ÿßŸÑŸàÿ≠ŸäÿØ ŸÑŸÑConsultant ŸáŸàŸÖÿß les conf√©rences Ÿàles workshops..\n- ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸáŸä clients ŸÖÿ™ÿßÿπ ŸÖÿπÿ∏ŸÖ ÿßŸÑESN ÿßŸÑŸÑŸä ŸÖŸàÿ¨ŸàÿØŸäŸÜ ŸÅŸä ÿßŸÑMarch√©.. ŸàŸÖŸàÿ¥ ŸÖÿπŸÜÿßŸáÿß Ÿàÿßÿ≠ÿØ ŸäÿÆÿØŸÖ ÿπŸÜÿØ client ŸÉÿ®Ÿäÿ± ŸÖÿπŸÜÿßŸáÿß ÿßŸÑstack technique ÿ∑Ÿäÿßÿ±ÿ©.. ŸÑÿß ŸÖŸàÿ¥ ÿ®ÿßŸÑŸÑÿßÿ≤ŸÖÿ©.. ŸÅŸÖÿß ŸÑÿ™Ÿàÿ© Struts ŸÅŸä ÿßŸÑSoci√©t√© G√©n√©rale.. üòÅ\nŸÖŸÑÿÆÿµ ÿßŸÑŸÇŸàŸÑ.. ÿßŸÑConsulting ŸáŸà ŸÖŸÜ ÿ£ŸÉÿ®ÿ± les domaines d\u0026rsquo;activit√©s ÿßŸÑŸÖŸàÿ¨ŸàÿØŸäŸÜ ÿ™Ÿàÿ© ŸÅŸä ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑŸÉŸÑ.. √† part ÿßŸÑcomp√©tence technique.. ŸÑÿßÿ≤ŸÖ ÿ®ÿ±ÿ¥ÿ© ÿµÿ®ÿ± Ÿàÿ®ÿ±ÿ¥ÿ© ÿßÿµÿ±ÿßÿ±..\nŸàsurtout ŸÑÿßÿ≤ŸÖŸÉ ÿ™ÿ≥ÿ∑ÿ± ÿßŸÑtrajectoire ŸÖÿ™ÿßÿπŸÉ ŸÉŸÑ ÿØŸÇŸäŸÇÿ© ŸàŸÉŸÑ ÿØÿ±ÿ¨..\nŸÑÿßÿ≤ŸÖ ŸÇÿ®ŸÑ ŸÖÿß ÿ™ÿ®ÿØŸâ mission ÿ¨ÿØŸäÿØÿ© ÿ≠ÿ∑ ŸÖŸÜ ÿßŸÑÿ£ŸàŸÑ ÿßŸÑÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÑŸÑŸä ÿ™ÿ≠ÿ® ÿ™ÿ™ÿπŸÑŸÖŸáŸÖ ŸÖŸÜŸáÿß..\nÿÆŸÑŸäŸÉ √† jour ŸÖÿπ ÿßŸÑupstream ŸÖÿ™ÿßÿπ ÿßŸÑmarch√© IT Ÿàÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸÜ ÿßŸÑobsolescence..\nŸÖŸÜ ÿ£ŸÇŸàŸâ ŸÅŸàÿßÿ¶ÿØ ÿßŸÑconsulting ÿßŸÜŸÉ ÿ®ÿßÿ¥ ÿßÿØŸàÿ± ÿ®ÿ±ÿ¥ÿ© ÿ¥ÿ±ŸÉÿßÿ™ Ÿàÿ®ÿßÿ¥ ÿ™ŸÇÿßÿ®ŸÑ ÿ®ÿ±ÿ¥ÿ© ÿπÿ®ÿßÿØ.. ÿßÿ∫ÿ™ŸÜŸÖ ÿßŸÑŸÅÿ±ÿµÿ© Ÿàÿßÿ®ŸÜŸä r√©seau ŸÉÿ®Ÿäÿ± ÿßŸÉÿ®ÿ± ŸÖÿß ŸäŸÖŸÉŸÜ.. ÿßŸÑmarch√© IT ÿπŸÑŸâ ŸÇÿØ ŸÖÿß Ÿäÿ®ÿßŸÜ ŸÉÿ®Ÿäÿ± ÿπŸÑŸâ ŸÇÿØ ŸÖÿßŸáŸà ÿµÿ∫Ÿäÿ±.. ÿ≤ŸÖŸäŸÑŸÉ ÿßŸÑŸÑŸä ÿ™ÿÆÿØŸÖ ŸÖÿπÿßŸá ÿßŸÑŸäŸàŸÖ ÿ∫ÿØŸàÿ© ŸäŸÜÿ¨ŸÖ ŸäŸÉŸàŸÜ ÿßŸÑclient ŸÖÿ™ÿßÿπŸÉ..\nÿßŸÑConsulting ŸäÿÆŸÑŸäŸÉ ÿ™ŸÜÿ¨ŸÖ ÿ™ÿÆÿØŸÖ ÿπŸÑŸâ des projets ŸÖÿ™ÿßÿπ ÿ¥ÿ±ŸÉÿßÿ™ ŸÉÿ®ÿßÿ±ŸÅŸä ŸàŸÇÿ™ Ÿàÿ¨ŸáÿØ ÿ£ŸÇŸÑ.. ÿßŸÑŸÑŸä ŸÖŸÖŸÉŸÜ ŸÉÿßŸÜ ÿ™ÿ≠ÿ® ÿ™ÿÆÿØŸÖ ÿπŸÜÿØŸáŸÖ directement ŸÑÿßÿ≤ŸÖŸÉ ÿ®ÿ±ÿ¥ÿ© ŸàŸÇÿ™..\nÿ¥ÿÆÿµŸäÿß.. ŸÜÿ¥ŸàŸÅ ÿßŸÑŸÑŸä ÿßŸÑConsulting ŸáŸà ÿßŸáŸÖ ŸÖÿ±ÿ≠ŸÑÿ© ŸÅŸä ÿßŸÑcarri√®re ŸÖÿ™ÿßÿπŸä ŸÑŸÉŸÜ ŸÖÿß ÿ™ÿµŸàÿ±ÿ™ÿ¥ ŸàŸÖÿßŸÜÿ™ÿµŸàÿ±ÿ¥ ÿßŸÑŸÑŸä ŸäŸÜÿ¨ŸÖ ÿ™ŸÉŸàŸÜ ÿÆÿØŸÖÿ© ŸÖÿ™ÿßÿπ long terme.. ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿ≠ÿ®Ÿäÿ™ Ÿàÿ™ŸÖŸÜŸäÿ™ ÿßŸÑŸÑŸä ŸÜÿØÿÆŸÑ ŸÜÿÆÿØŸÖ ŸÅŸä √©diteur ŸÉÿ®Ÿäÿ±..\nŸÅŸä ÿßŸÑÿßÿÆÿ±.. ŸÜÿ≠ÿ® ŸÜÿ±ŸÉÿ≤ ÿßŸÑŸÑŸä ŸÉŸÑÿßŸÖŸä ÿßŸÑŸÉŸÑŸá ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿ™ÿ¨ÿ±ÿ®ÿ© ÿ¥ÿÆÿµŸäÿ©.. Ÿàÿ£ÿ±ÿßÿ¶Ÿä ŸÉŸÑŸáÿß ÿ¥ÿÆÿµŸäÿ© Ÿàÿ™ŸÑÿ≤ŸÖŸÜŸä Ÿàÿ≠ÿØŸä..\nÿ®ÿßŸÑÿ™ŸàŸÅŸäŸÇ ŸÑŸÑŸÜÿßÿ≥ ÿßŸÑŸÉŸÑ..\n","permalink":"https://blog.nebrass.fr/comment-jai-per%C3%A7u-le-consulting-bel-tounsi/","summary":"ÿπÿ≥ŸÑÿßŸÖÿ© ŸàŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸäŸÉŸÖ ŸÅŸä ÿ®ÿ±ŸÜÿßŸÖÿ¨ŸÉŸÖ \u0026ldquo;ÿÆÿ∞Ÿàÿß ÿπŸäŸÜŸä ÿ¥ŸàŸÅŸà ÿ®ŸäŸáÿß\u0026rdquo; üòÜ\nÿßŸÑŸäŸàŸÖ ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸäŸÑŸÉŸÖ ÿπŸÑŸâ ŸÖŸáŸÜÿ© ÿßŸÑConsulting ÿßŸÑŸÑŸä ŸàŸÑÿßÿ™ ÿ£ŸÉÿ´ÿ± ÿßÿÆÿ™ÿµÿßÿµ ÿ™ŸÇÿ±Ÿäÿ®ÿß ÿ™Ÿàÿ© ŸÅŸä ÿ≥ŸàŸÇ ÿßŸÑÿ¥ÿ∫ŸÑ ÿÆÿßÿµÿ© ŸÅŸä ÿßŸàÿ±Ÿàÿ®ÿß.. ÿ¥ŸÜŸäŸá ŸÖÿπŸÜÿßŸáÿß Consulting ÿüÿü ŸÖŸÜŸäŸÜ ÿ¨ÿßÿ™ Ÿàÿ¥ŸÜŸäŸá ÿßŸÑÿ≠ŸÉÿßŸäÿ© ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ÿüÿü ŸÅŸä ÿßŸÑpost Ÿáÿ∞ÿß ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸä ŸÖŸÜ ŸÖŸÜÿ∑ŸÑŸÇ ÿ¥ÿÆÿµŸä ÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ© üòÅ\nÿßŸäÿß ÿ≥ŸäÿØŸä ÿ®ŸÜ ÿ≥ŸäÿØŸä.. ŸÜÿ±ÿ¨ÿπ ÿ®ŸäŸÉŸÖ ŸÑÿπÿßŸÖ 2014.. ÿ®ÿπÿØ ÿπÿßŸÖŸäŸÜ ÿÆÿØŸÖÿ© ÿπÿØŸäÿ™ŸáŸÖ ÿ®ÿπÿØ ÿßŸÑLicence ŸÅŸä ÿ¥ÿ±ŸÉÿ© ÿ®ÿ™ÿ±ŸàŸÑŸäÿ© ŸÅŸä ÿ™ŸàŸÜÿ≥.. ÿÆÿ±ÿ¨ÿ™ ÿ®ÿßÿ¥ ŸÜÿπÿØŸä ÿ≥ÿ™ÿßÿ¨ ÿßŸÑŸÖÿßÿ≥ÿ™Ÿäÿ± ŸÅŸä ÿ®ÿßÿ±Ÿäÿ≥ ŸàŸÉŸÜÿ™ ŸÜŸÑŸàÿ¨ ÿπŸÑŸâ ÿÆÿØŸÖÿ© ÿ®ÿßÿ¥ ŸÉŸäŸÅ ŸÜÿ™ÿÆÿ±ÿ¨ ÿßŸÑÿ£ŸÖŸàÿ± ÿ™ÿ®ÿØŸâ ŸÖÿ±ŸäŸÇŸÑÿ© ŸàŸÖÿß ŸÜÿ®ŸÇÿßÿ¥ ÿ®ÿ∑ÿßŸÑ.","title":"Comment j'ai per√ßu le Consulting bel Tounsi"},{"content":"I got married on August 16th 2019 ü•≥ü•≥üòçüòçüòòüòò We decided to be in the Maldives üá≤üáªüèù‚õ±ü•• for our honeymoon¬†ü§©ü§©ü§© For the back trip, we spent a day in Doha, Qatar üá∂üá¶üê™üå¥‚õ∫Ô∏è\nIt was an incredible trip üòç so I wanted to share some photos üì∑\nAs usual, I attached the pictures in Flickr:\nhttps://www.flickr.com/photos/nebrass78/albums/72157711450245191\n","permalink":"https://blog.nebrass.fr/trip-report-maldives-qatar-august-2019/","summary":"I got married on August 16th 2019 ü•≥ü•≥üòçüòçüòòüòò We decided to be in the Maldives üá≤üáªüèù‚õ±ü•• for our honeymoon¬†ü§©ü§©ü§© For the back trip, we spent a day in Doha, Qatar üá∂üá¶üê™üå¥‚õ∫Ô∏è\nIt was an incredible trip üòç so I wanted to share some photos üì∑\nAs usual, I attached the pictures in Flickr:\nhttps://www.flickr.com/photos/nebrass78/albums/72157711450245191","title":"Trip Report - Maldives \u0026 Qatar - August 2019"},{"content":"Introduction We.. developers.. we spend all our time writing code (and drinking coffee).. but we rarely care about the code quality.. until we get some pressure from a reviewer or from some quality scanner tool.. and as every field in the Java ecosystem, we have many choices when we come to choose a Quality Scanner tool: FindBugs, CheckStyle, PMD and of course the GREATEST SonarQube üòç\nI fully worked with SonarQube since January 2014, during my M.Sc. internship to analyze my code and even to check if there are some Security Flaws using the OWASP and SANS _Quality Gate_s.\nIn this tutorial, I will show you how to get started with SonarQube and how to use it lightly (without even installing it) üòÅ\nGetting Started In this tutorial, I suppose and I require that you are a Docker addict. ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è If it\u0026rsquo;s not the case , please please please get started to Docker ! It\u0026rsquo;s a MUST nowadays !!!! ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\nPreparing THE sample project The sample project for this tutorial is generated using the Spring Initializr. You can apply this tutorial to ANY MAVEN BASED PROJECT.\nYou can download the sample project that I generated from here.\nPreparing the SonarQUBE Server All the good stuff is here ! We will just run the official Docker image of SonarQube:\n1  $ docker run -d --name sonarqube -p 9000:9000 -p 9092:9092 sonarqube:latest   This command will:\n Pull the sonarqube:latest image from the Docker Hub Run as daemon the pulled image and binds the ports 9000 \u0026amp; 9002 to their corresponding ones in the created Docker container  To test your local SonarQube instance, go to the http://localhost:9000:\n When the loading finishes, you will get redirected to the SonarQube dashboard screen:\n Now, everything is ready, let\u0026rsquo;s go to the code side üòÅ\nRunning the Code analysis from Maven The operation is extremely easy, it\u0026rsquo;s just a regular Maven goal to add to your IDE or in your shell ü§ì The fabulous Maven command is:\n1  $ mvn sonar:sonar   Just this ! No plugin to add into your pom.xml¬†üòÅ Things will happen:\n Maven will download the Sonarqube Maven Plugin Sonarqube Maven Plugin will get the rules and configuration from the default SonarQube URL which is localhost:9000 which matches our local SonarQube instance. Sonarqube Code Scanner will analyze the source code Sonarqube Maven Plugin will generate some local reports (surefire reports) Push the report to the default SonarQube URL  The Maven log will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84  [INFO] --- sonar-maven-plugin:3.6.0.1398:sonar (default-cli) @ demo --- [INFO] User cache: /Users/nebrass/.sonar/cache [INFO] SonarQube version: 7.9.1 [INFO] Default locale: \u0026#34;fr_FR\u0026#34;, source code encoding: \u0026#34;UTF-8\u0026#34; [INFO] Load global settings [INFO] Load global settings (done) | time=117ms [INFO] Server id: BF41A1F2-AWzIIuiyloMw5HVAqvFU [INFO] User cache: /Users/nebrass/.sonar/cache [INFO] Load/download plugins [INFO] Load plugins index [INFO] Load plugins index (done) | time=70ms [INFO] Load/download plugins (done) | time=168ms [INFO] Process project properties [INFO] Execute project builders [INFO] Execute project builders (done) | time=3ms [INFO] Project key: com.example:demo [INFO] Base dir: /Users/nebrass/Downloads/demo [INFO] Working dir: /Users/nebrass/Downloads/demo/target/sonar [INFO] Load project settings for component key: \u0026#39;com.example:demo\u0026#39; [INFO] Load quality profiles [INFO] Load quality profiles (done) | time=103ms [INFO] Load active rules [INFO] Load active rules (done) | time=3772ms [WARNING] SCM provider autodetection failed. Please use \u0026#34;sonar.scm.provider\u0026#34; to define SCM of your project, or disable the SCM Sensor in the project settings. [INFO] Indexing files... [INFO] Project configuration: [INFO] 3 files indexed [INFO] Quality profile for java: Sonar way [INFO] Quality profile for xml: Sonar way [INFO] ------------- Run sensors on module demo [INFO] Load metrics repository [INFO] Load metrics repository (done) | time=85ms ‚Ä¶ [INFO] Sensor JavaSquidSensor [java] [INFO] Configured Java source version (sonar.java.source): 8 [INFO] JavaClasspath initialization [INFO] JavaClasspath initialization (done) | time=15ms [INFO] JavaTestClasspath initialization [INFO] JavaTestClasspath initialization (done) | time=2ms [INFO] Java Main Files AST scan [INFO] 1 source files to be analyzed [INFO] Load project repositories [INFO] Load project repositories (done) | time=11ms [INFO] 1/1 source files have been analyzed [WARNING] Classes not found during the analysis : [javax.annotation.meta.When] [INFO] Java Main Files AST scan (done) | time=406ms [INFO] Java Test Files AST scan [INFO] 1 source files to be analyzed [INFO] 1/1 source files have been analyzed [INFO] Java Test Files AST scan (done) | time=60ms [INFO] Sensor JavaSquidSensor [java] (done) | time=1210ms [INFO] Sensor JaCoCo XML Report Importer [jacoco] [INFO] Sensor JaCoCo XML Report Importer [jacoco] (done) | time=3ms [INFO] Sensor SurefireSensor [java] [INFO] parsing [/Users/nebrass/Downloads/demo/target/surefire-reports] [INFO] Sensor SurefireSensor [java] (done) | time=2ms [INFO] Sensor JaCoCoSensor [java] [INFO] Sensor JaCoCoSensor [java] (done) | time=1ms [INFO] Sensor JavaXmlSensor [java] [INFO] 1 source files to be analyzed [INFO] Sensor JavaXmlSensor [java] (done) | time=200ms [INFO] 1/1 source files have been analyzed [INFO] Sensor HTML [web] [INFO] Sensor HTML [web] (done) | time=14ms [INFO] Sensor XML Sensor [xml] [INFO] 1 source files to be analyzed [INFO] Sensor XML Sensor [xml] (done) | time=138ms [INFO] 1/1 source files have been analyzed [INFO] ------------- Run sensors on project [INFO] Sensor Zero Coverage Sensor [INFO] Sensor Zero Coverage Sensor (done) | time=8ms [INFO] Sensor Java CPD Block Indexer [INFO] Sensor Java CPD Block Indexer (done) | time=13ms [INFO] No SCM system was detected. You can use the \u0026#39;sonar.scm.provider\u0026#39; property to explicitly specify it. [INFO] 1 file had no CPD blocks [INFO] Calculating CPD for 0 files [INFO] CPD calculation finished [INFO] Analysis report generated in 82ms, dir size=81 KB [INFO] Analysis report compressed in 24ms, zip size=14 KB [INFO] Analysis report uploaded in 299ms [INFO] ANALYSIS SUCCESSFUL, you can browse http://localhost:9000/dashboard?id=com.example%3Ademo [INFO] Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report [INFO] More about the report processing at http://localhost:9000/api/ce/task?id=AWzILjRsloMw5HVAqxNt [INFO] Analysis total time: 8.298 s   Great ! Let\u0026rsquo;s go and look what this analysis has generated in the SonarQube Server:\n The first thing, on the dashboard, we notice that already the number of projects analyzed has been incrementing. Let\u0026rsquo;s click on that and see our analyzed project:\n Oppaaaaa ü•≥! Here is our project\u0026rsquo;s dashboard ! But üò≥ Look ! üò≥ There is already a code smell in the freshly generated project?? How come??? ü§î Let\u0026rsquo;s click on that and see what\u0026rsquo;s wrong?\n Yeap ! SonarQube is right üòú Spring Initializr is generating an empty Test method for us to populate it with our unit tests when we start coding (I hope you do ü§ûü§ì)\nGood ! So good !\nLet\u0026rsquo;s add some code to our Spring Boot application, and we will see how things will go with SonarQube. Let\u0026rsquo;s add this Rest Controller class to our project:\n1 2 3 4 5 6 7 8 9 10  @RestController @RequestMapping(\u0026#34;/api/hello\u0026#34;) public class HelloWorldRestController { public String message = \u0026#34;hello world !\u0026#34;; @GetMapping public String sayHello() { return message; } }   Next, run the Maven Clean/Install with the SonarQube tests command: mvn clean install sonar:sonar and check again the projects dashboard:\n And here¬†üò±üò±üò±üò±üò±üò± !! The project FAILED !!! WTF? The command that we did already compiled successfully the project?? What means the project Failed in SonarQube?\nThis alert means simply that the project failed to pass the Quality Gate üò≥ Ok what is a Quality Gate in SonarQube ? Let\u0026rsquo;s take the definition from the Sonar documentation:\n A quality gate is the best way to enforce a quality policy in your organization. It\u0026rsquo;s there to answer ONE question: can I deliver my project to production today or not?\nIn order to answer this question, you define a set of Boolean conditions based on measure thresholds against which projects are measured. For example:\n No new blocker issues Code coverage on new code greater than 80% Etc.   What is interesting here, your company or team can create its own Quality Gate depending of its needs and constraints.\nIn the section under the project name, there are some elements: Bugs, Vulnerabilites, Code Smells, Coverage and Duplications.\nThe Code Smells was already 1 (the empty test method body) - So the Quality Gate failure comes from the Vulnerabilites üò±\nTo see what\u0026rsquo;s going on, just click on the project name üëâ in the Security section, click on the number one vulnerabilities (or the New Vulnerabilites) to list them:\n Our vulnerability is:\n Oppaaa ! SonarQube dont like the public String message¬†ü§î It asks to make it a static final or to make it non-public (private for example) or to add getters and setters if we want to keep it as like it is üòÅ\nIf you want more explanation; just click on See Rule button located in the alert red box of the screen:\n If you dont find this great üëç¬†please leave my blog now üòÇüòÇüòÇ\nThis detailed guidance for coding is really wonderful! You dont need to fear from making errors while you are coding ! neither be surprised by code review üòù¬†comments üöß\nThis guidance will be your powerful complimentary tool while coding.\nI will make modifications and rescan the project:\n Cool ! But, it\u0026rsquo;s a little heavy to run manually the scanning command every time üò´ Yeah ! I\u0026rsquo;m lazy ! And many developers are lazy like me ! üòÅüòÅ That\u0026rsquo;s why the Sonar team thought on us and made for us a great tool called SonarLint !! ü•≥ü•≥ü•≥\nPlaying with SONARLINT SonarLint is an IDE extension that helps you detect and fix quality issues as you write code.¬†Like a spell checker, SonarLint¬†squiggles¬†flaws so that they can be fixed before committing code.\nSonarLint is available on many IDEs. Unfortunately, it\u0026rsquo;s not yet available on NetBeans üò´\n After installing the plugin to your favourite IDE, when you are coding, you can see immediately if you are doing something messy. If we take the example of our Rest Controller:\n Here, in IntelliJ, we see in the SonarLint window, there is the famous \u0026ldquo;Make message a static final constant or non-public and provide accessors if needed.\u0026quot; that we got before with the Maven scanner on the SonarQube Server dashboard.\nThere is more to do with SonarLint:\nWhen we talked about the Quality Gates, we said that a company can create its own Quality Gate based on its own needs. There is a very great option in SonarLint that enables it to connect to a specific SonarQube Server to grab specific Quality Gate to use it locally. On this way, you can code locally and be compliant with your team\u0026rsquo;s quality constraints üòÅ\n Cool ! üòÅ Yeah I\u0026rsquo;m in love with SonarQube ! Please try it ! You will for sure be addicted like me !\nConclusion SonarQube is for sure one of the most powerful and most used Quality metrics tool in the Java World. We find it in many many many CI/CD pipelines standing on the same side with the Tests runners. Even more, SonarQube metrics and analysis are now a very important criteria to validate the Build \u0026amp; Test step to proceed to staging the project in further steps.\n Do SonarQube ! It\u0026rsquo;s cool \u0026amp; great ! It\u0026rsquo;s nice and painless üòÅ Do it before you will be forced to do it üòú\n","permalink":"https://blog.nebrass.fr/playing-with-java-under-the-shield-of-sonarqube/","summary":"Introduction We.. developers.. we spend all our time writing code (and drinking coffee).. but we rarely care about the code quality.. until we get some pressure from a reviewer or from some quality scanner tool.. and as every field in the Java ecosystem, we have many choices when we come to choose a Quality Scanner tool: FindBugs, CheckStyle, PMD and of course the GREATEST SonarQube üòç\nI fully worked with SonarQube since January 2014, during my M.","title":"Playing with Java under the shield of SonarQube"},{"content":"Before the introduction This is my first tutorial about one of the Microsoft\u0026rsquo;s tools and platforms.. I started working for Microsoft on May 6th, I started to look to this huge and mysterious world by the eyes of a Java Developer. I will try to make some tutorials based on the great tools that I use or discover in the Microsoft ecosystem üòä\n‚ö†Ô∏è Disclaimer : This is not a marketing content üòÜ I just want to share with you a feedback about tooling to boost your productivity and to make the Java journey wonderful üòç\nIntroduction Serverless computing is an architecture that aims to delegate the execution of a piece of code to the cloud, where necessary resources will be dynamically allocated. Dynamic allocation means always the pay-as-you-go model when we are hosting the code on a Public Cloud Provider.\nThe serverless application is composed by functions triggered by some event like an HTTP Request, a Message (Kafka for example) or even a Scheduled event. That\u0026rsquo;s why, we refer to the serverless applications as \u0026ldquo;Functions as a Service\u0026rdquo;¬†or¬†\u0026ldquo;FaaS\u0026rdquo;.\nToday, I will make my first tutorial with Microsoft Azure FaaS which is called Azure Functions.\nWhat is Azure Functions We will be based on the Microsoft\u0026rsquo;s¬†presentation of Azure Functions:\n Azure Functions is a solution for easily running small pieces of code, or \u0026ldquo;functions,\u0026rdquo; in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it. Functions can make development even more productive, and you can use your development language of choice, such as C#, Java, JavaScript, Python, or PHP. Pay only for the time your code runs and trust Azure to scale as needed. Azure Functions lets you develop¬†serverless applications on Microsoft Azure.\n Requirements for this tutorial To do this tutorial, you will need an Azure Subscription. If you don\u0026rsquo;t already have one, you can get one with free 200$ credit from this link:\n In the Free Tier account, we can have up to 1 Million requests per month for FREE !!\nCreating the Azure Function App The first step is to create the new Function on the Azure Portal: https://portal.azure.com\nNext, go to the Function App and when you click on Add you will get this window:\n Fill this form with this details:\n App name: the Function App name and will be used in the application\u0026rsquo;s URL: https://APP_NAME.azurewebsites.net üëâ I will call it: hello-world-example -the application will be reachable on the URL https://hello-world-example.azurewebsites.net/ Resource Group: a logical container that will be holding Azure Resources, like the Function App here üëâ in this tutorial we are creating a new one called hello-world-example-rg OS: We will choose Windows as it has the Java Runtime Stack. Hosting Plan: The Consumption plan lets you pay-per-execution and dynamically allocates resources based on your app\u0026rsquo;s load. App Service Plans let you use a predefined capacity allocation with predictable costs and scale üëâ we need to choose Consumption plan. Location: The nearest one for me is France Central. Runtime Stack: Obviously Java üòÅ Storage: We will create a new one üòÉ Maybe we will need it later ü§™ Application Insights: an Azure monitoring platform that we will enable to monitor the Function App¬†üëâ Just click on Application insights and Enable the Collect application monitoring data using Application Insights: Then Apply next click on Create  Create your first function code After the application is created, it\u0026rsquo;s time to add code. To do that, click on your new application on the Function Apps list, to access to the application content:\n Next, click on the New function button:\n We will chose Any editor + Maven and proceed to next step:\n For the deployment method, we will use Use Deployment Center¬†and proceed to next step:\n In this section of the wizard, there are many important sections: like installing dependencies and how to create an Azure Functions project via Maven, which is the important part.\nClick Finish and go to Deployment Center to finish the wizard.\nCreate the maven project The Maven command showed in the quickstart wizard looks like:\n1 2 3 4 5 6 7 8 9  $ mvn archetype:generate -DarchetypeGroupId=com.microsoft.azure -DarchetypeArtifactId=azure-functions-archetype -DappName=hello-world-example -DappRegion={region} -DresourceGroup={resourceGroup} -DgroupId=com.{functionAppName}.group -DartifactId={functionAppName}-functions -Dpackage=com.{functionAppName} -DinteractiveMode=false   With in my case:\n region: FranceCentral resourceGroup: hello-world-example functionAppName: helloworldexample  After generating the Maven Project, just enable the Git version control by typing:\n1  $ git init   Add all the generated files to Git:\n1  $ git add .   Commit the changes to Git:\n1  $ git commit -am \u0026#34;First Commit\u0026#34;   import the project in the Azure DevOps First of all, we need to create a project in Azure DevOps. Ok but what is Azure DevOps ? üòÜ\nAzure DevOps is a Software as a Service (SaaS) from Microsoft that provides many great features for Software Teams - which cover all the lifecycle of a typical application:\n Azure Pipelines: CI/CD that works with any language, platform, and cloud (not only Azure üòÅ) Azure Boards: Powerful work tracking with Kanban boards, backlogs, team dashboards, and custom reporting. Azure Artifacts: Maven, npm, and NuGet package feeds from public and private sources. Azure Repos: Unlimited cloud-hosted private Git repos for your project. Collaborative pull requests, advanced file management, and more. Azure Test Plans: All in one planned and exploratory testing solution.  For the used Java developers; Yes, Azure DevOps is a great alternative of Jenkins/Hudson.\nFirst of all, go to the Azure DevOps portal:\n Click Start free to create an account..\nNext.. authenticate to your Oulook/Hotmail/Live account..\nNext.. create your first project:\n Next, when the project is created, you will be redirected to the Dashboard:\n Go to the Repos section:\n Next.. we need to authenticate the Git CLI to be able to push code and do operation from the CLI on this repository.. to do that.. Azure DevOps offers a great one-time generated passwords that you can directly get when you click on Generate Git credentials..\nNext.. we will go to our generated Maven project and add our Azure DevOps Git repository as remote repository:\n1  $ git remote add origin https://lnibrass@dev.azure.com/lnibrass/hello-azure-functions/_git/hello-azure-functions   Next.. we will push all our code to the Azure DevOps Git repository:\n1  $ git push -u origin --all   When files are pushed to the repository:\n Create the Azure DevOps CI/CD Pipelines Continuous integration To create our first Build pipeline, just go to Pipelines üëâ Builds and click on New pipeline..\n Next.. click on Use the classic editor:\n Next.. choose the Source of the project:\n Next.. the wizard will be listing some templates; obviously, we will choose the Maven template:\n Next.. you will be redirected to the job content..\nIn Azure DevOps, every job (Integration or Delivery) is composed of some tasks .. a Task is a step that can be doing some specific logic in the job.. for example..\nAs we will have steps to execute, before diving into them, we need to know the Agent pool that will be running them: Why we need to define an agent? They\u0026rsquo;re not similar?\nWe define an agent because they are some tasks that can be done exclusively by a specific kind of agents, for example, building DotNet framework üí© requires to have a Windows based agent.. in our example will be using an Ubuntu based agent:\n In our Maven, we can have some separate tasks, for example:\n mvn package üëâ for building the project mvn sonar:sonar üëâ for executing the Sonar quality gates  We can have other needs or constraints that require to have a specific task üò∏ for example; if you need to do some Bash command, or copy some files from somewhere..\nLet\u0026rsquo;s see how looks a job ?\n In the previous screenshot, we can enter the Maven goal that we want to run.. the same way as like your OS CLI.. and there are more you can enable the SonarQube reporting just by some clicks in this same task, on the Code Coverage section:\n In my tutorial, I will just be building the Maven project without running the tests ‚ò†Ô∏è Yes ! I know it\u0026rsquo;s a bull üí©üí©üí©üí© but for our case, I need to show how it works ! ü§°\nSo my Maven Goals:\nclean install -DskipTests\nAfter that we run our Maven task, the pipeline will¬†:\n1Ô∏è‚É£ Copy Files to: $(build.artifactstagingdirectory) üëâ¬†In this task, we will be copying the packaged project to a dedicated folder, so we can use the output of the previous task in the incoming tasks.\nOur Azure Functions Java application is a Maven based project, so when we build our project, we will get the packaged and built files in the target¬†subfolder of our project folder.\nSo we will be copying the files that are available in the target subfolder. So the Copy task will look like:\n  The $(something) are predefined variables of Azure DevOps pipelines. More info here.\n 2Ô∏è‚É£ Publish Artifact: drop üëâ In this task we will be publishing the files that we grabbed from the previous task to a some network reachable location so it can be used later in the Continuous Deployment pipeline.\nNow.. we need to enable the Continuous integration to start a new Build when new code is pushed to the master branch of our Git Repository:\n Now, everything is ready, we can go to prepare our CD pipeline.. ü§ì\nContinuous Deployment To create our first Release pipeline, just go to Pipelines üëâ Releases and click on New pipeline..\nIn the wizard, we will choose the Deploy a function app to Azure Functions..\n Next.. in the Artifacts source, we will define the Source (which is the Build pipeline) to the one that we created in the previous step:\n Next.. we need to enable the Continuous deployment trigger to create a release every time a new build is available:\n Now, we will go to configure the Stage 1 of the Release Pipeline:\nIn this step, we need to give the access of the Azure DevOps Pipeline to the Azure Subscription:\n Choose your Azure Subscription, and next click on Authorize¬†to configure an Azure service connection. A new Azure service principal will be created and added to the Contributor role, having access to all resources in the selected subscription.\nIn the :\n App type, choose the Function App on Windows App Service name, enter the application name that we created in the beginning: hello-world-example   Next.. click on the Deploy Azure Function App step to configure it:\nIn the Package or Folder choose: $(System.DefaultWorkingDirectory)/_hello-azure-functions-Maven-CI/drop/target/azure-functions/hello-world-example\n Next.. click on Create Release to create a new Release:\n Click Create and click next on the Release name that is shown, to follow the build process:\n The Release build process will look like:\n We need to wait until the Build succeeds:\n Now.. go back to the Azure Portal üëâ Resource Groups called hello-world-example-rgüëâ Function App called hello-world-example:\n Under the Function Apps there are our hello-world-example application. In this tree there is a Functions (Read Only) section, which lists all the Functions that are included in our application.\nIf you click on the HttpTrigger-Java you will get something that looks like this:\n Click on \u0026lt;/\u0026gt; Get function URL to get and test the function URL:\n When you click Copy and you past it to a browser you can see the function response which looks like\nPlease pass a name on the query string or in the request body\nWTF!! üò±üò±üò±üò±\nThis is normal, the sample function needs some specific input to provide some response. So, now we will dive into the Java ecosystem ü•∞\nLet\u0026rsquo;s look to the sample project:\n In the sample project, we have only one Java class called Function. It\u0026rsquo;s content looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  public class Function { ... @FunctionName(\u0026#34;HttpTrigger-Java\u0026#34;) public HttpResponseMessage run( @HttpTrigger(name = \u0026#34;req\u0026#34;, methods = {HttpMethod.GET, HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); // Parse query parameter  String query = request.getQueryParameters().get(\u0026#34;name\u0026#34;); String name = request.getBody().orElse(query); if (name == null) { return request .createResponseBuilder(HttpStatus.BAD_REQUEST) .body(\u0026#34;Please pass a name on the query string or in the request body\u0026#34;) .build(); } else { return request .createResponseBuilder(HttpStatus.OK).body(\u0026#34;Hello, \u0026#34; + name) .build(); } } }   In the line (5) we have the @FunctionName¬†annotation that contains the HttpTrigger-Java which is the Function name. This indicates that this function will be available at the endpoint /api/HttpTrigger-Java.\nThere is only one method called run(...)¬†which is the Function body; we pass to this method some arguments:\n HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request : This indicates that the body of the request will potentially contain a String value - this argument is annotated with the @HttpTrigger annotation. In this annotation you\u0026rsquo;ll note that it has been given a name req, as well as told what type of requests it supports (in this case, only HTTP GET and POST requests), and that the AuthorizationLevel¬†is anonymous, allowing access to anyone who can call the endpoint. final ExecutionContext context¬†: The execution context enables interaction with the Azure Functions execution environment. Used for accessing the Java Logger that will see output directed to Azure Portal, as well as any other configured output¬†locations.  So based on this code, if I add a name parameter to the Function URL, I will get a response üòÅ\nThe old URL is :\nhttps://hello-world-example.azurewebsites.net/api/HttpTrigger-Java?code=phKRPlOlAsjlZqiYbOnMaZtu/z66RAY91MqKQ1lwbofXGvlZcggArg==\nThe new URL with the name parameter will look like:\nhttps://hello-world-example.azurewebsites.net/api/HttpTrigger-Java?name=nebrass\u0026amp;code=phKRPlOlAsjlZqiYbOnMaZtu/z66RAY91MqKQ1lwbofXGvlZcggArg==\nWe will get a response like Hello, nebrass¬†ü•≥\nNow, we can change a little bit the code of our HttpTrigger-Java function to be like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  public class Function { @FunctionName(\u0026#34;HttpTrigger-Java\u0026#34;) public HttpResponseMessage run( @HttpTrigger( name = \u0026#34;req\u0026#34;, methods = {HttpMethod.GET, HttpMethod.POST}, authLevel = AuthorizationLevel.FUNCTION ) HttpRequestMessage\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; request, final ExecutionContext context) { context.getLogger().info(\u0026#34;Java HTTP trigger processed a request.\u0026#34;); return request.createResponseBuilder(HttpStatus.OK) .body(\u0026#34;Hello World at \u0026#34; + LocalDateTime.now().toString()) .build(); } }   After you commit, the Build pipeline will be executed and when it succeeds, the Release pipeline will be started to deploy the new code.\nIf you go now to the function URL:\nhttps://hello-world-example.azurewebsites.net/api/HttpTrigger-Java?code=phKRPlOlAsjlZqiYbOnMaZtu/z66RAY91MqKQ1lwbofXGvlZcggArg==\nYou will get the new output message: Hello World at 2019-08-09T12:25:24.035\nTriggers and bindings I will be covering more the Triggers and Bindings in my next post about the fabulous Azure Java Functions:\nhttps://blog.nebrass.fr/playing-with-java-in-azure-functions-new-release/\nDo more üòé You can do more with Azure Functions:\n Azure Functions Java developer guide Spring Cloud Function 2.0 and Azure Functions  Not final words The Azure Functions is a very powerful product and it can be very useful in many use-cases when we are working with events-driven logic such as doing some business tasks when new file is uploaded in Azure Storage or when a new message is dispatched to Azure Event Hub.\nActually, Azure Functions support Java 8. To follow updates about the Java 11 and 12 support check this ticket and this one also.\n","permalink":"https://blog.nebrass.fr/playing-with-java-in-azure-functions-and-azure-devops/","summary":"Before the introduction This is my first tutorial about one of the Microsoft\u0026rsquo;s tools and platforms.. I started working for Microsoft on May 6th, I started to look to this huge and mysterious world by the eyes of a Java Developer. I will try to make some tutorials based on the great tools that I use or discover in the Microsoft ecosystem üòä\n‚ö†Ô∏è Disclaimer : This is not a marketing content üòÜ I just want to share with you a feedback about tooling to boost your productivity and to make the Java journey wonderful üòç","title":"Playing with Java in Azure Functions and Azure DevOps"},{"content":"ŸÉŸäŸÅÿßÿ¥ ŸÜŸàŸÑŸä D√©veloppeur Java ÿ®ÿßŸáŸä ÿü ü§î\nÿßŸÑJava ÿßŸÖ ÿßŸÑÿØŸÜŸäÿß ÿπŸÑŸâ ÿπŸÉÿ≥ ŸÖÿß Ÿäÿ™ÿÆŸäŸÑŸá ÿ®ÿ±ÿ¥ÿ© ŸàÿÆŸäÿßŸÜ ŸáŸä ŸÑÿ∫ÿ© ÿ®ÿ±ŸÖÿ¨ÿ© ÿ≥ÿßŸáŸÑÿ© ÿ®ÿ±ÿ¥ÿ©.. Ÿàÿßÿ∂ÿ≠ÿ© Ÿàÿ≠ÿ™Ÿâ les notions ÿßŸÑŸÖÿπŸÇÿØÿ© ŸÅŸäŸáÿß ŸÉŸäŸÅ ÿ™ÿ±ŸÉÿ≤ ŸÅŸäŸáÿß ÿ™ŸÑŸÇÿßŸáÿß ÿ≠ŸÑŸàÿ©..\nŸÇÿ®ŸÑ ŸÖÿß ÿ™ÿ®ÿØŸâ ÿßÿØ⁄®ÿØ⁄® ŸÅŸä ÿßŸÑŸÉŸàÿØ.. ŸÑÿßÿ≤ŸÖŸÉ bagage ÿ®ÿßŸáŸä ŸÅŸä ÿßŸÑalgorithmique.. ÿßŸÑÿ≠ÿßÿ¨ÿ© ÿßŸÑŸÑŸä ÿ™ŸÅÿ±ŸÇ ÿ®ŸäŸÜ D√©veloppeur ÿ®ÿßŸáŸä ŸàÿÆÿßŸäÿ® ŸáŸä ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ™ŸÅŸÉŸäÿ± ŸÖÿ™ÿßÿπŸà ŸÅŸä ÿ≠ŸÑ ÿßŸä ŸÖÿ¥ŸÉŸÑ.. ÿßŸÑÿ™ŸÖÿ¥Ÿä ÿ®ÿßÿ¥ ÿ™ŸÅŸÉŸÉ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© Ÿàÿ∑ÿ±ŸäŸÇÿ© ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ≠ŸÑ ŸÑÿßÿ≤ŸÖŸáÿß ÿ¥ŸàŸäÿ© ÿÆÿØŸÖÿ©.. ÿ®ÿ±ÿ¥ÿ© ŸÉÿ™ÿ® ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ÿ™Ÿàÿ¨ÿØÿ™ ÿ®ÿßÿ¥ ÿ™ÿπÿ∑Ÿä ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸÑŸàŸÑ ŸÑÿ®ÿ±ÿ¥ÿ© ŸÖÿ≥ÿßÿ¶ŸÑ ŸÖÿπÿ±ŸàŸÅÿ©.. ŸÉŸäŸÖÿß probl√®me de voyageur de commerce Ÿàÿ≠ÿ™Ÿâ ÿßÿ®ÿ≥ÿ∑ ŸÖÿ´ŸÑÿß ŸÉŸäŸÅÿßŸá ÿ™ŸÑŸàÿ¨ ÿπŸÑŸâ √©l√©ment ŸÅŸä Ÿàÿ≥ÿ∑ Tableau.. ŸÜÿπÿ±ŸÅ ÿßŸÑŸÑŸä ÿ®ÿ±ÿ¥ÿ© ŸÖŸÜŸÉŸÖ ÿ™Ÿàÿ© Ÿäÿ≠ÿ®Ÿàÿß Ÿäÿ™ÿπÿØŸàÿß direct ŸÑŸÑŸÉŸàÿØ ŸàŸÖÿßÿ¥Ÿä ŸÅŸä ÿ®ÿßŸÑŸáŸÖ ÿ™ÿ∂ŸäŸäÿπ ŸàŸÇÿ™ ÿßŸÑalgorithme.. Ÿàÿ®ÿ±ÿ¥ÿ© ÿ≤ÿßÿØÿ© ŸäŸÇŸÑŸÉ ÿßŸÑŸÑŸä ŸÜÿπÿ±ŸÅŸàÿß Ÿäÿ≤Ÿä ÿßŸÑŸÖŸáŸÖ ÿÆŸÑŸäŸÜŸä ŸÜÿ®ÿØŸâ ŸÜŸÉŸàÿØŸä Ÿàÿ™Ÿà ÿ®ÿπÿØ ŸÜÿ±ÿ¨ÿπŸÑŸàÿß.. ÿπŸäÿ® Ÿäÿß ŸÑŸàŸÑŸà.. ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿ≠ÿ≥ŸÜ ÿ™ÿπŸÖŸÑ ÿ™ÿπŸÖŸÇ ÿ®ÿßŸáŸä ŸÅŸä ÿßŸÑalgorithmique Ÿàÿ®ÿπÿØ ÿßÿ±ŸÖŸä ÿ±Ÿàÿ≠ŸÉ ŸÅŸä ÿßÿ≠ÿ∂ÿßŸÜ ÿßŸÑŸÉŸàÿØ ŸÖŸàÿ¥ Ÿáÿßÿ±ÿ®.. ÿπŸÑÿßÿ¥ Ÿáÿ∞ÿß ÿßŸÑŸÉŸÑÿü ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ÿßŸÑŸÜÿßÿ≥ ÿßŸÑŸÑŸä ŸÉÿ™ÿ®ÿ™ ÿßŸÑÿ¨ÿßŸÅÿß ÿßŸÖ ÿßŸÑÿØŸÜŸäÿß ÿ≠ÿ∂ÿ±Ÿàÿß ÿ®ÿ±ÿ¥ÿ© des fonctions ÿßŸÑŸÑŸä Ÿäÿ∑ÿ®ŸÇŸàÿß ŸÅŸäŸáÿß ÿ®ÿ±ÿ¥ÿ© des algorithmes ŸÖÿπÿ±ŸàŸÅŸäŸÜ.. ŸÉŸäŸÅ ÿ™ÿ®ÿØŸâ ÿßŸÜÿ™ ÿ™ÿπÿ±ŸÅŸáŸÖ ÿßŸÉŸäÿØ ÿ®ÿßÿ¥ ÿ™ÿπÿ±ŸÅ ŸÉŸäŸÅÿßŸá ŸàŸàŸäŸÜ ÿ®ÿßÿ¥ ÿ™ÿ≥ÿ™ÿπŸÖŸÑŸáŸÖ ŸàŸáÿßÿ∞Ÿä ÿ™ÿ™ÿ≥ŸÖŸâ efficacit√© ŸàŸáŸä ŸÖŸÜ ÿßŸÉÿ®ÿ± ŸÜŸÇÿßÿ∑ ÿßŸÑŸÇŸàÿ© ŸÖÿ™ÿßÿπ ŸÖÿ®ÿ±ŸÖÿ¨ ŸÉÿßÿ≥ÿ≠.. ŸÉŸäŸÖÿß ŸäŸÇŸàŸÑ J√©r√¥me \u0026ldquo;ÿ≤ÿßŸäÿØ ÿ®ÿßÿ¥ ŸÜÿπÿßŸàÿØŸàÿß ŸÜÿÆÿ™ÿ±ÿπŸàÿß ÿßŸÑÿπÿ¨ŸÑÿ©\u0026rdquo;.. ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ÿßŸÑŸÖŸàÿ¨ŸàÿØ ŸáŸà ÿßŸÉÿ®ÿ± ÿ±ŸÉÿßÿ¶ÿ≤ ÿßŸÑopen source ŸàÿßŸÑd√©veloppement logiciel.. ŸàŸáŸÜÿß Ÿäÿ¨Ÿä concept ÿßÿÆÿ± ÿ®ŸäŸÜ ÿßŸÑÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ŸàÿßŸÑŸÉŸàÿØ ŸàŸáŸà ÿßŸÑdesign pattern.. ŸàŸáŸà ÿπÿ®ÿßÿ±ÿ© ÿπŸÑŸâ recette ŸÖÿ™ÿßÿπ ŸÖŸÖÿßÿ±ÿ≥ÿ© ŸÖÿπŸäŸÜÿ©.. ŸÖÿ´ŸÑÿß ŸÅŸä ÿßŸÑÿ∑ÿ®ÿÆ ŸäŸÇŸÑŸÉ ŸÉŸä ÿ™ÿ≠ŸÑ ÿÆŸÖŸäÿ±ÿ© ŸÅŸä ŸÖÿßÿ° ÿ≥ÿÆŸàŸÜ ÿ≤ŸäÿØŸáÿß ÿ≥ŸÉÿ± ÿ®ÿßÿ¥ ÿ™ÿ™ÿ≠ŸÑ ÿßŸÉÿ´ÿ± ŸàÿÆŸäÿ±.. de m√™me ŸÅŸä ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ© ŸäŸÇŸÑŸÉ ŸÉÿßŸÜ ÿ™ÿ≠ÿ® ÿ™ÿπŸÖŸÑ code g√©n√©rique ÿ™ŸÜÿ¨ŸÖ ÿ™ÿ≥ÿ™ÿπŸÖŸÑ ÿßŸÑŸÇÿßŸÑÿ® D√©corateur.. ÿßŸÑŸÇŸàÿßŸÑÿ® Ÿáÿßÿ∞ŸÖ ŸÖŸàÿ¥ ÿ®ÿ±ÿ¥ÿ© ŸáŸÖÿß 23 Ÿàÿ®ÿØÿßŸà Ÿäÿ≤ÿØŸàÿß ŸÖŸÜ ŸàŸÇÿ™ ÿßŸÑŸÑŸä ÿ¨ÿ™ŸÜÿß ÿßŸÑCloud.. ÿ®ÿßŸÑÿ±ÿ∫ŸÖ ÿßŸÑŸÑŸä ŸáŸÖÿß ŸÖŸàÿ¥ ÿ®ÿ±ÿ¥ÿ©ÿå ŸÑŸÑÿ£ÿ≥ŸÅ ÿ®ÿ±ÿ¥ÿ© ŸÖÿ®ÿ±ŸÖÿ¨ŸäŸÜ ŸÖÿß Ÿäÿπÿ±ŸÅŸáŸÖÿ¥ Ÿàÿ≠ŸÜŸâ ŸÉÿßŸÜ Ÿäÿπÿ±ŸÅŸáŸÖ ŸÖÿß Ÿäÿ≥ÿ™ÿπŸÖŸÑŸáŸÖÿ¥.. ŸàÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿ≠ÿ™ŸÖŸäÿ©.. ŸÖÿß ÿßÿ™ÿ®ÿπÿ¥ ÿßŸÑrecette ÿßŸÑŸÖÿßŸÉŸÑÿ© ŸÅŸÖÿß risque ÿ™ÿ¨Ÿä ŸÖŸàÿ¥ ÿ®ÿßŸáŸäÿ©.. ÿßŸÑŸÉŸàÿØ ÿßÿµÿπÿ® ŸÖŸÜ ÿßŸÑŸÖÿßŸÉŸÑÿ© ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿßŸÑfinalit√© ŸÖÿ™ÿßÿπ ÿßŸÑŸÖÿßŸÉŸÑÿ© ŸáŸä ÿßŸÑtoilette üòÅ ÿßŸÖÿß ÿßÿßŸÉŸàÿØ ŸÖÿßÿ¥Ÿä ŸÑŸÑproduction.. Ÿàÿ≠ÿ™Ÿâ ŸÉÿßŸÜ ŸÜŸáÿßÿ± ŸÑŸÇŸäÿ™Ÿàÿß production ÿ™ÿ¥ÿ®Ÿá ŸÑŸÑcuvette ÿ±ÿßŸà 99 ÿ®ÿßŸÑŸÖÿßÿ¶ÿ© ŸÖŸÜ ÿßŸÑD√©veloppeuret ÿßŸÑŸÑŸä ŸäÿÆÿ±ÿ¨Ÿàÿß ŸÅŸä ŸÉŸàÿØ ÿ≤ÿ®ÿßŸÑÿ©.. üòÇ\nÿ™Ÿàÿ© ŸÜÿ™ÿπÿØŸàÿß ŸÑÿ£ŸÖ ÿßŸÑÿØŸÜŸäÿß.. ÿ®ÿßÿ¥ ÿ™ÿ™ÿπŸÑŸÖ ÿßŸÑÿ¨ÿßŸÅÿß ŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© ŸÖÿ±ÿßÿ≠ŸÑ ŸÑŸÉŸÜŸáŸÖ ÿ≠ŸÑŸàŸäŸÜ.. ŸÖŸÜ ÿßŸÑÿßŸàŸÑ ÿ™ÿ™ÿπŸÑŸÖ ÿßŸÑsyntaxe ŸàÿßŸÜŸàÿßÿπ les objets ÿßŸÑŸÖŸàÿ¨ŸàÿØŸäŸÜ Ÿàÿ®ÿπÿØ les boucles Ÿà les iterations ŸÖÿ±Ÿàÿ±ÿß ÿ®ÿßŸÑles conditions.. ÿßŸÑÿßÿ¥ŸÉÿßŸÑ ŸÖŸàÿ¥ ÿ¥ŸÜŸäÿ© ÿ®ÿßÿ¥ ŸÜÿ™ÿπŸÑŸÖ ŸÅŸä ÿßŸÑÿ¨ÿßŸÅÿß.. ŸÑŸÉŸÜ ŸÉŸäŸÅÿßŸá ŸÜÿ™ÿπŸÑŸÖÿü ÿßŸÑÿßÿ¨ÿßÿ®ÿ© Ÿáÿßÿ∞Ÿä ÿ™ÿ¨ÿßŸàÿ® ÿπŸÑŸäŸáÿß ÿ®ÿ≥ÿ§ÿßŸÑ ÿ≥ÿßŸáŸÑ.. ÿ™ÿ≠ÿ® ÿ™ŸÇÿ±Ÿâ ŸÉÿ™ÿßÿ® ŸÅŸäŸá 1000 ÿµŸÅÿ≠ÿ© ÿü ŸàÿßŸÑÿß ÿ™ÿ™ŸÅÿ±ÿ¨ ŸÅŸä ŸÅŸäÿØŸäŸà ÿ∑ŸàŸÑŸà 37 ÿ≥ÿßÿπÿ©ÿü ÿ™ŸÜÿ¨ŸÖ ÿ™ŸÇÿ±Ÿâ Ÿàÿ≠ÿØŸÉ ŸàÿßŸÑÿß ŸÑÿßÿ≤ŸÖŸÉ ÿ≠ÿØ ŸÖÿπÿßŸÉ ÿü üòÅ ÿßŸäŸá ÿü ÿßÿ¥ ŸÇŸÑÿ™ÿü ŸÅŸä ŸÉŸÑ ÿßŸÑÿ≠ÿßŸÑÿßÿ™ ÿßŸÜÿ™ ÿßŸÑŸÑŸä ŸäŸÑÿ≤ŸÖŸÉ ÿ™ŸÅŸáŸÖ ŸÉŸäŸÅÿßŸá ÿ™ŸÇÿ®ŸÑ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© Ÿàÿ®ŸÜÿßÿ° ÿπŸÑŸäŸáÿß ÿ™ÿ®ÿØŸâ.. ÿßŸÑÿßÿ≠ÿ≥ŸÜ ŸàÿßŸÑŸÖŸÅŸäÿØ ŸáŸà ÿßŸÑŸÉÿ™ÿßÿ®.. ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿØŸäŸÖÿß ÿßŸÑcontenu ÿßŸÉÿ´ÿ± ŸàÿØŸäŸÖÿß ÿßŸÑÿ¨ŸÖŸÑÿ© ŸÇÿØÿßŸÖ ÿπŸäŸÜŸäŸÉ ÿ™ŸÜÿ¨ŸÖ ÿ™ÿπÿßŸàÿØ ÿ™ŸÇÿ±ÿßŸáÿß ÿßŸÑŸÅ ŸÖÿ±ÿ©.. ÿßŸÑŸÅŸäÿØŸäŸàÿßÿ™ ÿßŸÉŸäÿØ ÿ®ÿßŸáŸäÿ© ÿÆÿßÿµÿ© ŸÑŸÑŸÜÿßÿ≥ ÿßŸÑŸÑŸä ŸÖÿßÿÆŸäÿ¥ ÿµÿ≠ÿ®ÿ© ŸÖÿπ ÿßŸÑŸÉÿ™ÿ® Ÿàÿ™ŸÇŸÑŸÇ ŸÅŸäÿ≥ÿπ ŸÖŸÜ ÿ®ÿ±ÿ¥ÿ© ŸÉÿ™Ÿäÿ®ÿ© ŸÅŸä √©cran ŸÖŸàÿ¥ ÿ≠ÿ™Ÿâ Ÿàÿ±ŸÇÿ©.. ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸÉÿ™ÿßÿ® ÿßŸÑŸÖŸÜÿßÿ≥ÿ® ŸàÿßŸÑŸÑŸä ÿßŸÑŸÅŸäÿØŸäŸà ÿßŸÑÿ™ÿ≠ŸÅŸàŸÜ ÿ®ÿßÿ¥ ŸÜÿ≠ÿßŸàŸÑ ŸÜÿ≠ÿ∑ ÿ¥ŸàŸäÿ© ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ŸÅŸä ÿßÿÆÿ± ÿßŸÑpost..\nÿ®ÿπÿØ ŸÖÿß ÿ™ÿ™ÿπŸÑŸÖ ÿßŸÑŸÑÿ∫ÿ© ŸÑÿßÿ≤ŸÖ ÿ™ÿ®ÿØŸâ ÿ™ÿ™ÿπŸÑŸÖ ÿ≠ÿßÿ¨ÿßÿ™ avanc√© ÿ¥ŸàŸäÿ© ÿπÿßŸÑÿßÿ≥ÿßÿ≥Ÿäÿßÿ™ ŸÉŸäŸÖÿß Java Enterprise ÿßŸÑŸÑŸä ŸàŸÑÿß ÿßÿ≥ŸÖŸáÿß Jakarta EE.. ŸÖÿßŸà ŸÇŸÑŸÜÿß ŸÑÿßÿ≤ŸÖ ŸÜÿ≥ÿ™ÿπŸÖŸÑŸàÿß ÿßŸÑŸÖŸàÿ¨ŸàÿØ.. ŸÑÿßÿ≤ŸÖ ÿ¥ŸàŸäÿ© les frameworks ÿÆÿßÿµÿ© ÿßŸÑSpring ŸàŸÖÿ¥ÿ™ŸÇÿßÿ™Ÿá..\nÿßŸÉŸäÿØ ÿ±ÿßŸÉŸÖ ÿ≥ŸÖÿπÿ™Ÿàÿß ÿ®ÿßŸÑDevOps.. ÿßŸÑbase ŸÖÿ™ÿßÿπŸáÿß ÿßŸÜŸà ÿßŸÑd√©veloppeur ŸäÿπŸÖŸÑ ÿ∂ÿ±ÿ®ÿßÿ™ infrastructure.. Ÿáÿ∞ÿßŸÉÿß ÿπŸÑÿßÿ¥ ÿßŸÑD√©veloppeur ÿ™Ÿàÿ© ÿßŸÑŸÑŸä ŸÖÿßŸäŸÅŸáŸÖÿ¥ Docker Ÿà Kubernetes ŸÖÿßŸäŸÜÿ¨ŸÖÿ¥ ŸäÿπŸäÿ¥ belgd√© ŸÅŸä ÿßŸÑmarch√©..\nÿßŸá ÿßŸä.. ÿ™ÿ∞ŸÉÿ±ÿ™ ÿ≠ÿßÿ¨ÿ© ŸÖŸáŸÖÿ© ŸÇÿ®ŸÑ ŸÖÿß ŸÜÿÆÿ™ŸÖ.. ÿßŸÑcertification ŸÑÿßÿ≤ŸÖÿ© ŸàÿßŸÑÿß ŸÑÿßÿü ŸÖÿ´ŸÑÿß certification basique ÿ™ŸÇŸÑŸÉ ÿßŸÑŸÑŸä ÿßŸÜÿ™ ÿ™ÿπÿ±ŸÅ les fondamentaux ŸÖÿ™ÿßÿπ ŸÑÿ∫ÿ© ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©.. ÿßŸÖÿß ŸÖÿ≥ÿ™ÿ≠ŸäŸÑ ÿ™ŸÇŸÑŸÉ ÿßÿ∞ÿß ÿ™ÿπÿ±ŸÅ ÿ™ÿÆŸÖŸÖ ŸàÿßŸÑÿß ŸÑÿß.. ÿßŸÜÿß ÿ¥ÿÆÿµŸäÿß ŸÜŸÜÿµÿ≠ ÿßŸÜŸÉ ÿ™ÿÆÿØŸÖ ÿ¥ŸàŸäÿ© des projets Ÿàÿ≠ÿØŸÉ Ÿàÿ™ÿ≠ÿ∑ŸáŸÖ ŸÅŸä Github Ÿàÿ™ÿÆŸÑŸäŸá ŸÉreference ŸÖÿ™ÿßÿπŸÉ.. ÿÆŸäÿ± Ÿàÿ£ŸÇŸÑ ŸÅŸÑŸàÿ≥.. ÿ®ÿßŸÑÿπŸÉÿ≥ ŸÉŸäŸÅ ÿ™ÿπŸÖŸÑ Github ÿ™ŸÜÿ¨ŸÖ ÿ™ÿ≠ÿ∑ ŸÅŸäŸá des mini projets ÿ™ÿ≥ÿ™ÿπŸÖŸÑdes patrerns Ÿàÿ™ÿ≤ŸäÿØ framework.. ÿ™ŸàŸÑŸä r√©f√©rence ÿÆŸäÿ± ŸÖŸÜ certification..\nÿ¥ŸàŸäÿ© ŸÉÿ™ÿ® ŸÜŸÜÿµÿ≠ŸÉŸÖ ÿ®ŸäŸáÿß\n == Algorithmes et Patterns ==\n Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne Head First Design Patterns  == Java SE ==\n Sun Certified Programmer for Java 6 Study Guide Head First Java, 2nd Edition Java The Complete Reference, 11th Edition Effective Java 3rd Edition  == Spring ==\n Hibernate Quickly Spring Boot in Action  == Java EE ==\n Java EE 6 and GlassFish 3 Java EE 8 Application Development  == Microservices ==\n Building Microservices by Sam Newman Playing with Java Microservices on Kubernetes and OpenShift  == Docker \u0026amp; Kubernetes ==\n Docker in Practice, 2nd Edition Kubernetes in action  == Testing \u0026amp; Agilit√© \u0026amp; More ==\n Clean Code: A Handbook of Agile Software Craftsmanship Effective Unit Testing: A guide for Java developers Java Concurrency in Practice Domain-Driven Design: Tackling Complexity in the Heart of Software  ÿ®ÿßŸÑÿ™ŸàŸÅŸäŸÇ ŸÑŸÑŸÜÿßÿ≥ ÿßŸÑŸÉŸÑ..\n","permalink":"https://blog.nebrass.fr/le-petit-guide-pour-d%C3%A9buter-en-java-bel-tounsi/","summary":"ŸÉŸäŸÅÿßÿ¥ ŸÜŸàŸÑŸä D√©veloppeur Java ÿ®ÿßŸáŸä ÿü ü§î\nÿßŸÑJava ÿßŸÖ ÿßŸÑÿØŸÜŸäÿß ÿπŸÑŸâ ÿπŸÉÿ≥ ŸÖÿß Ÿäÿ™ÿÆŸäŸÑŸá ÿ®ÿ±ÿ¥ÿ© ŸàÿÆŸäÿßŸÜ ŸáŸä ŸÑÿ∫ÿ© ÿ®ÿ±ŸÖÿ¨ÿ© ÿ≥ÿßŸáŸÑÿ© ÿ®ÿ±ÿ¥ÿ©.. Ÿàÿßÿ∂ÿ≠ÿ© Ÿàÿ≠ÿ™Ÿâ les notions ÿßŸÑŸÖÿπŸÇÿØÿ© ŸÅŸäŸáÿß ŸÉŸäŸÅ ÿ™ÿ±ŸÉÿ≤ ŸÅŸäŸáÿß ÿ™ŸÑŸÇÿßŸáÿß ÿ≠ŸÑŸàÿ©..\nŸÇÿ®ŸÑ ŸÖÿß ÿ™ÿ®ÿØŸâ ÿßÿØ⁄®ÿØ⁄® ŸÅŸä ÿßŸÑŸÉŸàÿØ.. ŸÑÿßÿ≤ŸÖŸÉ bagage ÿ®ÿßŸáŸä ŸÅŸä ÿßŸÑalgorithmique.. ÿßŸÑÿ≠ÿßÿ¨ÿ© ÿßŸÑŸÑŸä ÿ™ŸÅÿ±ŸÇ ÿ®ŸäŸÜ D√©veloppeur ÿ®ÿßŸáŸä ŸàÿÆÿßŸäÿ® ŸáŸä ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ™ŸÅŸÉŸäÿ± ŸÖÿ™ÿßÿπŸà ŸÅŸä ÿ≠ŸÑ ÿßŸä ŸÖÿ¥ŸÉŸÑ.. ÿßŸÑÿ™ŸÖÿ¥Ÿä ÿ®ÿßÿ¥ ÿ™ŸÅŸÉŸÉ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© Ÿàÿ∑ÿ±ŸäŸÇÿ© ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ≠ŸÑ ŸÑÿßÿ≤ŸÖŸáÿß ÿ¥ŸàŸäÿ© ÿÆÿØŸÖÿ©.. ÿ®ÿ±ÿ¥ÿ© ŸÉÿ™ÿ® ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ÿ™Ÿàÿ¨ÿØÿ™ ÿ®ÿßÿ¥ ÿ™ÿπÿ∑Ÿä ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸÑŸàŸÑ ŸÑÿ®ÿ±ÿ¥ÿ© ŸÖÿ≥ÿßÿ¶ŸÑ ŸÖÿπÿ±ŸàŸÅÿ©.","title":"Le petit guide pour d√©buter en Java bel Tounsi"},{"content":"Oracle Groundbreakers Podcast #368 features a conversation with the winners of the Oracle Groundbreaker Awards 2019. The discussion covers their career journeys, their thoughts on where software development is today, and on where it‚Äôs going tomorrow.\nBut before we meet the award winners, a little background on the Oracle Groundbreaker Awards is in order.\nLaunched in 2018, the Oracle Groundbreaker Awards recognize developers who exhibit not only extraordinary technical skill, but also the ability to inspire others, and the vision to bring the future a little closer. Nominations are submitted by the Oracle developer community, and a short list of developers is chosen from among those nominees. From that short list the community then selects the four award winners.\nThe winners for 2019 were announced and introduced at the Oracle Code event in New York City on May 21, 2019. And now you get to meet them in this podcast.\nOracle Groundbreaker Podcast number #368 was recorded on June 19, 2019.\n  Original post link in the Oracle Groundbreakers Podcast website.\n","permalink":"https://blog.nebrass.fr/podcast-meet-the-2019-oracle-groundbreaker-award-winners/","summary":"Oracle Groundbreakers Podcast #368 features a conversation with the winners of the Oracle Groundbreaker Awards 2019. The discussion covers their career journeys, their thoughts on where software development is today, and on where it‚Äôs going tomorrow.\nBut before we meet the award winners, a little background on the Oracle Groundbreaker Awards is in order.\nLaunched in 2018, the Oracle Groundbreaker Awards recognize developers who exhibit not only extraordinary technical skill, but also the ability to inspire others, and the vision to bring the future a little closer.","title":"Podcast: Meet the 2019 Oracle Groundbreaker Award Winners"},{"content":"During the Oracle Code New York, on May 2019, I got interviewed by the Oracle Developers Team to discuss the Javabilities üòÑ\n ","permalink":"https://blog.nebrass.fr/my-interview-with-the-oracle-developers-team/","summary":"During the Oracle Code New York, on May 2019, I got interviewed by the Oracle Developers Team to discuss the Javabilities üòÑ\n ","title":"My interview with the Oracle Developers Team"},{"content":"I was in Istanbul for nearly a week in a business trip. It was a very great trip : very rich cultural city, delicious food and very nice views ü§©\nI had the chance to visit some attractions in the city:\n Hagia Sophia Suleymaniye Mosque Grand Bazaar Spice Bazaar Blue Mosque Taksim Square  https://www.flickr.com/photos/nebrass78/albums/72157709473573782\n","permalink":"https://blog.nebrass.fr/istanbul-trip-june-2019/","summary":"I was in Istanbul for nearly a week in a business trip. It was a very great trip : very rich cultural city, delicious food and very nice views ü§©\nI had the chance to visit some attractions in the city:\n Hagia Sophia Suleymaniye Mosque Grand Bazaar Spice Bazaar Blue Mosque Taksim Square  https://www.flickr.com/photos/nebrass78/albums/72157709473573782","title":"Istanbul Trip - June 2019"},{"content":"ÿßŸÑŸäŸàŸÖ ÿ®ÿßÿ¥ ŸÜŸÉÿ™ÿ® post ŸÑÿßŸàŸÑ ŸÖÿ±ÿ© ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ© ÿ®ÿßÿ¥ ÿßŸÑŸÉŸÑÿßŸÖ ÿßŸÑŸÉŸÑ ŸäŸÉŸàŸÜ amical ŸàŸÖŸàÿ¥ formel.. üòÅ\nÿßŸÑstage ŸàŸÖÿß ÿ£ÿØÿ±ÿßŸÉ ŸÖŸÜ ÿßŸÑstage ŸÖŸÜ ÿ£ŸÉÿ®ÿ± ÿßŸÑŸÖÿπÿ∂ŸÑÿßÿ™ ÿßŸÑŸÑŸä ÿ™ŸÇÿßÿ®ŸÑ ÿ£Ÿä ÿ∑ÿßŸÑÿ® ŸÖŸáŸÖÿß ŸÉÿßŸÜ ÿßÿÆÿ™ÿµÿßÿµŸà ŸàÿßŸÑÿß diplome ŸÖÿ™ÿßÿπŸà.. ŸÇÿ®ŸÑ ŸÉÿßŸÜ ŸÖÿßÿ¥Ÿä ŸÅŸä ÿ®ÿßŸÑŸä ÿßŸÑŸÑŸä ŸÉÿßŸÜ ÿßŸÑLMD ÿ™ÿßÿπÿ®ŸäŸÜ ÿ®ÿßÿ¥ ŸäŸÑŸÇŸà stage.. ÿ£ŸÖÿß ŸÉŸäŸÅ ÿØÿÆŸÑÿ™ ŸÑŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸÖŸáŸÜŸäÿ©ÿå ÿπÿ±ŸÅÿ™ ÿßŸÑŸÑŸä ÿ≠ÿ™Ÿâ ÿ∑ŸÑÿ®ÿ© ÿßŸÑŸáŸÜÿØÿ≥ÿ© ÿ®ÿßÿ¥ ŸäÿØÿ®ÿ±Ÿà stage ÿ≠ÿßŸÑÿ™ŸáŸÖ ŸÖÿ™ÿπÿ®ÿ© ÿ£ÿµŸÑ..\nÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ™ÿπŸäÿ≥ÿ© Ÿáÿßÿ∞Ÿä ÿßŸÑÿ≥ÿ®ÿ® ŸÅŸäŸáÿß ŸáŸà ŸÜŸÅÿ≥ ÿ≥ÿ®ÿ® ÿ™ÿØŸÜŸä ÿßŸÑŸÖŸáŸÜ ŸÅŸä ÿ™ŸàŸÜÿ≥ ŸàŸáŸà ÿ≥ÿ®ÿ® ŸÖÿπÿ±ŸàŸÅ: ÿßŸÑŸÖÿ≠ÿßÿ®ÿßÿ© ŸàŸáŸà ŸÖÿßŸäÿπÿ±ŸÅ ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© \u0026ldquo;ÿ®ÿßŸÑÿ£ŸÉÿ™ÿßŸÅ\u0026rdquo;.. ÿßŸÑŸÑŸä ÿπŸÜÿØŸà ÿ¥ŸÉŸàŸÜ ŸÅŸä ÿ¥ÿ±ŸÉÿ© ŸàÿÆÿßÿµÿ© ŸÅŸä op√©rateur t√©l√©phonique ŸÖÿß ŸäÿÆŸÖŸÖÿ¥ ÿ®ÿßÿ¥ ŸäŸÑŸàÿ¨ ÿπŸÑŸâ stage.. ÿ¨ŸàŸà ÿ®ÿßŸáŸä ŸÖŸÜ ÿßŸÑŸÑŸàŸÑ.. ŸÑÿß ÿ™ÿÆŸÖŸÖ ŸàŸÑÿß ÿ™ÿ¥ŸÇŸâ tonton ŸÅŸä ÿßŸÑtunisiana üòÜüòÜ ÿπŸÑÿßÿ¥ ŸÇŸÑÿ™ Tunisiana ÿ®ÿßŸÑÿ∞ÿßÿ™ÿüÿü ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ŸÅŸä 2011 ÿ™ŸÖŸÜŸäÿ™ ŸÜÿπŸÖŸÑ ŸÅŸäŸáÿß stage d\u0026rsquo;√©t√© ŸàŸÖÿßŸÜÿ¨ŸÖÿ™ÿ¥ ÿ≠ÿ™Ÿâ ŸÜÿµÿ® demande.. ÿßŸÑŸÑŸä ÿπŸÜÿØŸà les √©paules ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸäŸá ŸàÿßŸÑÿ®ÿßŸÇŸä ŸÅŸä ÿßŸÑÿ£ŸÖÿßŸÜ.. ÿßŸÑÿ≠ÿßÿµŸÑ tgata3t Ÿàÿ≥ŸáŸÑŸä ÿ±ÿ®Ÿä ŸÅŸä stage ŸÅŸä ÿßŸÑBFI ŸäÿπŸÖŸÑ 66 ŸÉŸäŸÅ ŸàÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá..\nÿ®ÿπÿØ ÿßŸÑÿßÿ±Ÿáÿßÿ® ÿßŸÑŸÑŸä ÿ±Ÿäÿ™Ÿà ŸÅŸä ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÑŸâ stage d\u0026rsquo;√©t√© ÿÆŸÖŸÖÿ™ ÿ®ÿßÿ¥ ŸÜŸÖÿ¥Ÿä ŸÜÿπŸÖŸÑ stage ÿßŸÑÿ®ÿ±ÿ©.. ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑŸÖÿ™ŸÇÿØŸÖ ŸÖÿßŸäÿπÿ™ÿ±ŸÅÿ¥ ÿ®ÿßŸÑÿ£ŸÉÿ™ÿßŸÅ ŸÉŸäŸÖÿß ÿ™ŸàŸÜÿ≥.. Ÿàÿ≠ÿ™Ÿâ ŸÉÿßŸÜ ŸÅŸÖÿß ŸÖÿ≠ÿßÿ®ÿßÿ© ŸÖÿßŸáŸäÿ¥ ÿ®ÿßÿ¥ ÿ™ŸÉŸàŸÜ ŸÉŸäŸÅ ÿ™ŸàŸÜÿ≥.. ŸàŸáÿ∞ÿß ÿ£ŸÉŸäÿØ ÿ¨ÿØÿßÿß..\nÿ®ÿØŸäÿ™ ÿ±ÿ≠ŸÑÿ© ÿßŸÑÿ®ÿ≠ÿ´ ŸÅŸä ÿ®ÿØÿßŸäÿ© ÿ£Ÿàÿ™ 2011.. ÿ®ÿØŸäÿ™ ŸÜŸÑŸàÿ¨ ÿπŸÑŸâ¬†stage d√©veloppement ŸÅŸä ÿ£Ÿä ÿ≠ÿßÿ¨ÿ©.. ŸàŸáÿßÿ∞Ÿä ŸÖŸÜ ÿ£ŸÉÿ®ÿ± ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑŸÑŸä ÿπŸÖŸÑÿ™Ÿáÿß.. ÿ∂Ÿäÿπÿ™ ÿ®ÿ±ÿ¥ÿ© ŸàŸÇÿ™ ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ŸÖÿßÿ≠ÿ∑Ÿäÿ™ÿ¥ ŸáÿØŸÅ Ÿàÿßÿ∂ÿ≠ ŸÖŸÜ ÿßŸÑÿ£ŸàŸÑ.. ÿÆŸÖŸÖÿ™ ŸÉŸäŸÅ ÿ£Ÿä Ÿàÿßÿ≠ÿØ Ÿäÿ≠ÿ® ŸäÿÆÿ±ÿ¨ ŸÖŸÜ ÿ®ÿßŸÑÿ®ŸÑÿßÿØ.. ÿ£ÿπÿ∑ŸäŸÜŸä ŸÜÿÆÿ±ÿ¨ ŸàŸÜÿÆÿØŸÖ ÿ®ÿ£Ÿä¬†technologie.. ÿ≠ÿ™Ÿâ ŸÉÿßŸÜ Ÿäÿπÿ∑ŸàŸÜŸä stage assembleur 8086 ÿ±ÿßŸÜŸä ŸÇŸÑÿ™ ÿßŸä..\nÿßŸÑstage d\u0026rsquo;√©t√© ÿßŸÑŸÑŸä ÿπŸÖŸÑÿ™Ÿà ŸÅŸä ÿßŸÑBFI ŸÉÿßŸÜ Java.. ŸàŸÅŸä ÿßŸÑISG¬†ÿ£ŸÖ ÿßŸÑÿØŸÜŸäÿß ŸÉŸÜÿ™ ŸÖÿ±ŸÉÿ≤ ÿ®ÿ±ÿ¥ÿ© ŸÖÿπ ÿßŸÑJava.. ŸàŸÉŸäŸÅ ÿµÿ®Ÿäÿ™ des candidatures ÿµÿ®Ÿäÿ™ ŸÅŸä PHP Ÿà ++C .. ÿ¥Ÿäÿ° ŸÖÿß ŸäŸÇÿßÿ®ŸÑÿ¥ ÿ®ÿπÿ∂Ÿà ÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ©.. ŸàŸáÿ∞ÿß ŸÉÿßŸÜ ŸÖŸÜ ÿßŸÑÿ£ÿ≥ÿ®ÿßÿ® ÿßŸÑŸÑŸä ÿÆŸÑÿ™ŸÜŸä ŸÖÿßÿ™ÿ¨ŸäŸÜŸäÿ¥ des r√©ponses Ÿàÿ≠ÿ™Ÿâ ŸÉÿßŸÜ ÿ¨ÿßÿ™ r√©ponse ÿ™ÿ¨Ÿä n√©gative..\nÿ®ÿßŸáŸäÿ© ÿ≠ŸÉÿßŸäÿßÿ™ ÿßŸÑÿπÿ±ŸàŸä ÿ£ŸÖÿß ŸáŸäÿß ŸÜÿ™ÿπÿØŸà en mode guide ÿ®ÿßÿ¥ titre ÿßŸÑpost ŸäŸÉŸàŸÜ ÿ≠ŸÇŸäŸÇŸä ŸÖŸàÿ¥ ÿ∂ÿ±ÿ®ÿßŸÜ ŸÑÿ∫ÿ© üòÑ\n1- ÿ≠ÿØÿØ ÿßÿÆÿ™ÿµÿßÿµŸÉ\nŸáÿßÿ∞Ÿä ÿßŸáŸÖ ÿÆÿ∑Ÿàÿ© ŸÅŸä ÿßŸÑÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÉŸÑ.. ÿßÿ∞ÿß ŸÉÿßŸÜ ÿ™ÿ™ÿÆŸäŸÑ ÿßŸÑŸä ŸÅŸÖÿß ÿ¥ÿ±ŸÉÿ© ÿ®ÿßÿ¥ ÿ™ÿ¨Ÿäÿ®ŸÉ ÿ™ÿπŸÖŸÑ stage ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿßÿ™ÿ≥ÿ™ŸÅÿßÿØ ŸÖŸÜŸÉ ÿ±ÿßŸÉ ÿ∫ÿßŸÑÿ∑.. ŸÖÿßŸÅŸÖÿßÿ¥ ÿ≠ÿ™Ÿâ ÿ¥ÿ±ŸÉÿ© ÿ™ÿπŸÖŸÑ activit√© ŸÖÿßÿ™ÿ±ÿ®ÿ≠ÿ¥ ŸÖŸÜŸáÿß.. ŸÅŸä ÿßŸÑbusiness ŸÖÿßŸÅŸÖÿßÿ¥ ÿ≠ÿßÿ¨ÿ© ÿßÿ≥ŸÖŸáÿß ÿ±ÿ¨ŸÑÿ© ŸàŸÑŸàÿ¨Ÿá ÿßŸÑŸÑŸá.. ÿßŸä ÿ¥ÿ±ŸÉÿ© ÿ®ÿßÿ¥ ÿ™ÿßÿÆÿ∞ stagiaire ÿ®ÿßÿ¥ ÿ™ÿÆŸÑŸäŸá ŸäÿÆÿØŸÖ ŸÉŸäŸÅ ÿßŸä ŸÖŸáŸÜÿØÿ≥.. ÿ≠ÿ™Ÿâ ŸÉÿßŸÜ ÿ®ÿßÿ¥ ŸäŸÉŸàŸÜ productif ÿ£ŸÇŸÑ.. ÿ£ŸÖÿß ÿØŸäŸÖÿß ŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ productif..\nÿ£ŸàŸÑ ÿ≥ÿ§ÿßŸÑ ŸÑÿßÿ≤ŸÖ ÿ™ÿ≥ÿ£ŸÑŸà ŸÑÿ±Ÿàÿ≠ŸÉ ŸàŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÅŸÖÿß r√©ponse Ÿàÿ≠ÿØÿ© ŸàŸàÿßÿ∂ÿ≠ÿ©: ÿßŸÜÿß ÿ¥ŸÜŸäÿ© ÿßÿÆÿ™ÿµÿßÿµŸä ÿü ÿ¥ŸÜŸäÿ© ÿßŸÑposte ÿßŸÑŸÑŸä ŸäŸÜÿßÿ≥ÿ® les comp√©tences ŸÖÿ™ÿßÿπŸä ÿüÿü\nÿßÿ∞ÿß ŸÉÿßŸÜ ŸÉŸÑŸÖÿ© comp√©tences ŸÅÿ¨ÿπÿßÿ™ŸÉ.. ÿ£ŸÉŸäÿØ ÿ±ÿßŸÉ ÿÆÿßŸäŸÅ ŸàŸÖÿ™ÿÆŸäŸÑ ÿßŸÑŸä ÿßŸÑ stage ŸàÿßŸÑÿß ÿßŸÑÿÆÿØŸÖÿ© ŸáŸä ÿ∫ŸàŸÑ ÿ®ÿßÿ¥ ŸäÿßŸÉŸÑŸÉ..\nŸÜÿπÿ±ŸÅ ÿßŸÑŸÑŸä ÿ®ÿ±ÿ¥ÿ© ÿ®ÿßÿ¥ ŸäŸÇŸàŸÑŸàŸÑŸä ŸÖÿßÿ≤ŸÑŸÜÿß ŸÜŸÇÿ±Ÿà ŸàŸÖÿßÿ≤ÿßŸÑ ŸÖÿßÿπŸÜÿØŸäÿ¥ ÿÆÿ®ÿ±ÿ© ŸÅŸä ÿ≠ÿ™Ÿâ ÿ¥Ÿäÿ°.. ŸÉŸäŸÅÿßŸá ÿ™ÿ≠ÿ®ŸÜŸä ŸÜŸÇŸàŸÑ ÿπŸÜÿØŸä comp√©tence ŸÅŸä ÿ≠ÿßÿ¨ÿ© Ÿàÿ£ŸÜÿß ŸÖÿßÿ≤ÿßŸÑ ŸÖÿßÿπŸÜÿØŸäÿ¥ÿüÿü\nÿßŸÑÿßÿ¨ÿßÿ®ÿ© ÿ≥ÿßŸáŸÑÿ©.. ÿßŸÜÿ™ ÿπŸÜÿØŸÉ ÿ®ÿ±ÿ¥ÿ© des comp√©tences ÿßŸÑŸÑŸä ÿßŸä ŸÅÿ±ŸäŸÇ ÿπŸÖŸÑ Ÿäÿ≥ÿ™ÿ≠ŸÇŸáŸÖ.. ŸÅŸÖÿß ÿ¥ŸÉŸàŸÜ ÿπŸÜÿØŸà concentration ŸÉÿ®Ÿäÿ±ÿ©.. ŸÅŸÖÿß ÿ¥ŸÉŸàŸÜ ŸäŸÅŸáŸÖ ÿßŸä ŸÖÿ¥ŸÉŸÑÿ© ŸÅŸäÿ≥ÿπ.. ŸÅŸÖÿß ÿ¥ŸÉŸàŸÜ bon parleur ŸàŸäŸÜÿ¨ŸÖ ŸäŸàÿµŸÑ ÿßŸÑŸÅŸÉÿ±ÿ© ÿ®ÿ≥ŸáŸàŸÑÿ©.. ŸÅŸÖÿß ÿ¥ŸÉŸàŸÜ ÿπŸÜÿØŸà esprit d\u0026rsquo;√©quipe ÿ®ÿßŸáŸä ŸàŸäÿπÿ±ŸÅ ŸÉŸäŸÅÿßŸá Ÿäÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿßŸÑŸÜÿßÿ≥..\nŸáÿ∞ÿß humainement.. ŸàÿßŸÑŸÑŸä ŸáŸà ŸÖŸáŸÖ ÿ®ÿ±ÿ¥ÿ©.. ÿßŸÑŸÑŸä ŸÜŸÅÿ≥Ÿäÿ™Ÿà ÿ®ÿßŸáŸäÿ© ŸäŸÜÿ¨ŸÖ ŸäŸÜÿ¨ÿ≠.. ŸàÿßŸÑŸÑŸä ŸÇŸÑÿ®Ÿà ŸÖÿ≥ŸàÿØ ŸÖŸÑŸàÿØ msawed mlawed üòÇ ŸÖÿß ÿπŸÜÿØŸà ŸàŸäŸÜ ŸäŸàÿµŸÑ.. Ÿàÿ≠ÿ™Ÿâ ŸÉÿßŸÜ ŸäŸàÿµŸÑ ŸäŸàŸÑŸä expert ŸàÿßŸÑÿß manager ŸÉÿ®Ÿäÿ± ÿØŸäŸÖÿß Ÿäÿ®ŸÇŸâ ŸÖŸÉÿ±ŸàŸá ŸàŸÖÿßŸáŸàÿ¥ ŸÜÿßÿ¨ÿ≠ ŸÖŸáŸÜŸäÿß..\nŸÜÿ¨Ÿà ŸÑŸÑŸÜŸÇÿ∑ÿ© ÿßŸÑŸÑŸä ŸÖÿßÿ™ÿÆŸÑŸäŸÉÿ¥ ÿßÿØÿ®ÿ± stage ÿ®ÿßŸáŸä ŸàŸÖÿßÿ™ÿÆŸÑŸäŸÉÿ¥ ÿßÿØÿ®ÿ± ÿÆÿØŸÖÿ©.. ÿßŸÑcomp√©tence technique ÿßŸÑŸÑŸä ŸÖÿØŸàÿÆÿ© ÿßŸä recruteur..\nŸÉŸäŸÅ ŸÉŸäŸÅ.. ÿßŸÜÿ™Ÿä ÿßÿÆÿ™Ÿä ÿßŸÑŸÑŸä ÿ≥ÿ£ŸÑÿ™ ÿ®ŸÉÿ±Ÿä ÿπŸÑŸâ \u0026ldquo;ŸÖŸÜŸäŸÜ ŸÜÿ¨Ÿäÿ®Ÿáÿß ŸÖÿßÿ≤ŸÑÿ™ ŸÜŸÇÿ±Ÿâÿü\u0026rdquo; .. ŸàÿßŸÜÿ™ ÿ≤ÿßÿØÿ© Ÿäÿß ÿßŸÑŸÑŸä ŸÇŸÑÿ™ \u0026ldquo;ÿ™Ÿä ÿ£ŸÜÿß ŸÜŸÑŸàÿ¨ ÿπŸÑŸâ stage ÿ®ÿßÿ¥ ŸÜÿ™ÿπŸÑŸÖ ŸÖŸÜŸà\u0026rdquo; ÿ±ÿßŸÉ ÿ∫ÿßŸÑÿ∑ Ÿäÿß ÿµÿßÿ≠ÿ®Ÿä ŸàŸÖÿßŸÉÿ¥ ÿπÿßÿ±ŸÅ ŸÉŸäŸÅÿßŸá ŸäÿµŸäÿ± ŸÅŸä ÿßŸÑMarch√© IT\nŸÖÿßŸÜŸäÿ¥ ÿ®ÿßÿ¥ ŸÜÿ∑ŸàŸÑ ÿπŸÑŸäŸÉ.. ŸÉŸäŸÖÿß ŸäŸÉŸàŸÜ ŸÅŸäŸÉ comp√©tence humaine ÿ™ÿ®ŸÜŸä ŸÅŸäŸáÿß.. ÿ™ÿ≠ÿßŸàŸÑ ÿ™ŸÉŸàŸÜ calme Ÿàÿ™ŸÉŸàŸÜ positif ŸàÿßŸÑÿ¨Ÿà Ÿáÿ∞ÿßŸÉÿß ÿßŸÑŸÉŸÑ.. ŸÉÿ¥ÿ±ŸÉÿ©ÿå ŸÑÿßÿ≤ŸÖ ÿ≤ÿßÿØÿ© ÿ™ÿ¨ŸäŸÜŸä techniquement ÿ≠ÿßÿ∂ÿ± ÿ≤ÿßÿØÿ©..\n¬†ÿßÿ¥ ŸÖÿπŸÜÿßŸáÿß ÿ≠ÿßÿ∂ÿ±techniquement ÿüÿü\nŸÖÿßŸà ÿ≠ŸÉŸäŸÜÿß ÿ®ŸÉÿ±Ÿä.. ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿ®ÿßÿ¥ ÿßÿØÿÆŸÑŸÉ ÿ®ÿßÿ¥ ÿ™ÿ±ÿ®ÿ≠ ŸÖŸÜŸÉ.. ŸÉÿßŸÜ ÿ™ÿ¨ŸäŸáŸÖ ÿ∏ŸÑŸÖÿ© ŸàŸÖÿßÿ™ÿπÿ±ŸÅ ÿ¥Ÿäÿ°.. ŸÜÿ®ÿ¥ÿ±ŸÉ ÿßŸÑŸÑŸä ÿ≠ÿØ ŸÖÿß Ÿäÿπÿ∑ŸäŸÉ ŸÅÿ±ÿµÿ©.. ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ÿßŸÜÿ™ ÿ±Ÿàÿ≠ŸÉ ŸÖÿßÿπÿ∑Ÿäÿ™Ÿáÿßÿ¥ ŸÅÿ±ÿµÿ© ÿ™ŸÅŸáŸÖ Ÿàÿ™ÿπŸÑŸÖ..\nÿ®ÿßÿ¥ ÿ™ÿπŸÖŸÑ Stage Java ŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ ÿπŸÜÿØŸÉ ŸÖÿ≥ÿ™ŸàŸâ ÿ™ŸÇŸÜŸä ÿ®ÿßŸáŸä.. ŸÑÿßÿ≤ŸÖ ÿ™ÿπÿ±ŸÅ les fondamentaux ŸàŸÖÿßŸÅÿßŸáÿß ÿ®ÿßÿ≥ ÿ≤ÿßÿØÿ© ÿ≠ÿßÿ¨ÿßÿ™ avanc√©s.. ÿßŸÑŸÉÿ™ÿ® ŸàÿßŸÑŸÅŸäÿØŸäŸàŸäÿßÿ™ ŸÖŸàÿ¨ŸàÿØÿ© ŸàÿßŸÑÿÆŸäÿ± ŸÖŸÉÿØÿ≥.. ŸÑÿßÿ≤ŸÖ ÿ™ŸÇÿ±Ÿâ Ÿàÿ≠ÿØŸÉ Ÿàÿ™ÿ¨ÿ±ÿ® Ÿàÿ™ÿ™ÿπŸÑŸÖ Ÿàÿ™ÿπŸÖŸÑ ÿ≠ÿ™Ÿâ des projets ÿ≠ÿ∑ŸáŸÖ ŸÅŸä ÿßŸÑGithub Ÿàÿ≠ÿ∑ lien ŸÖÿ™ÿßÿπŸáŸÖ ŸÅŸä CV ŸÖÿ™ÿßÿπŸÉ.. Ÿàÿ™ÿπŸÑŸÖ ÿ™ÿπŸÖŸÑ √©valuation ÿµÿßÿØŸÇÿ© ŸÑÿ£Ÿä projet ÿπŸÖŸÑÿ™Ÿà.. ÿπŸÑŸâ ÿÆÿßÿ∑ÿ± ŸÉÿßŸÜ ÿ™ŸÜŸÅÿÆ projet ÿµÿ∫Ÿäÿ± ŸÇÿØÿßŸÖ expert technique ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑŸÑŸä ÿ™ÿπÿ∑Ÿä ŸÅŸäŸáÿß ÿπŸÑŸâ ÿ±Ÿàÿ≠ŸÉ ŸáŸä ÿßŸÜŸà ÿßŸÜÿ™ ÿπŸäŸÜŸÉ ÿµÿ∫Ÿäÿ±ÿ© ŸÖŸÑÿßŸáÿß projet ÿµÿ∫Ÿäÿ± ŸàŸÖŸàÿ¥ trendy..\nÿπŸÑŸâ ŸÅŸÉÿ±ÿ© ŸÉŸÑŸÖÿ© projet ÿµÿ∫Ÿäÿ± ÿ±ÿßŸà ÿπŸÑŸâ les technologies ÿßŸÑŸÑŸä ŸÅŸäŸá ŸÖŸàÿ¥ ÿπÿØÿØ ÿßŸÑÿ£ÿ≥ÿ∑ÿ±ÿ©..\nÿ®ÿ±ÿ¥ÿ© ÿ∑ŸÑÿ®ÿ© ŸäŸÇŸÑŸÉ ŸÖÿßŸÑŸÇŸäÿ™ÿ¥ ŸàŸÇÿ™ ÿ®ÿßÿ¥ ŸÜÿ™ÿπŸÑŸÖ Ÿàÿ≠ÿØŸä.. ŸÖÿßÿπÿ±ŸÅÿ™ÿ¥.. ŸàŸáÿßŸÉ ÿßŸÑÿ¨Ÿà.. ŸàÿßŸÑŸÑŸá ŸÜÿ®ŸÇŸâ ŸÖÿ≥ÿ™ÿ∫ÿ±ÿ®.. ÿßŸÑŸÉÿ™ÿ® ŸÖŸàÿ¨ŸàÿØÿ©.. ÿßŸÑŸäŸàÿ™ŸäŸàÿ® ŸÖŸàÿ¨ŸàÿØ.. les events ŸÖŸàÿ¨ŸàÿØŸäŸÜ ŸàŸÅŸä ÿ™ŸàŸÜÿ≥ ÿ™Ÿàÿ© ÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá ŸàŸÑÿßÿ™ ŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™ Ÿàÿßÿ≠ÿØ ŸäŸÜÿ¨ŸÖ ŸäŸÖÿ¥ŸäŸÑŸáÿß.. Ÿàÿ≠ÿ™Ÿâ ÿßŸÑŸÑŸä ŸÖÿßŸäŸÜÿ¨ŸÖÿ¥ ŸäŸÖÿ¥Ÿä ŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© des events ŸäÿµŸäÿ±Ÿà ŸàŸäÿ™ÿπÿØÿßŸà live ÿπÿßŸÑÿßŸÜÿ™ÿ±ŸÜÿßÿ™..\nÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑrythme ÿßŸÑŸÑŸä Ÿàÿßÿ≠ÿØ ŸäŸÜÿ¨ŸÖ Ÿäÿ™ÿπŸÑŸÖ ÿ®ŸäŸá.. ÿ≥ÿßÿπÿ™ŸäŸÜ ŸÉŸÑ ŸäŸàŸÖ ŸàÿßŸÑŸÑŸá ÿßŸÑÿπÿ≤.. ÿ™ÿ±Ÿàÿ≠ ŸÑŸÑÿØÿßÿ±.. ÿ™ÿ±ŸÉÿ≠ ÿßŸàŸÖŸàÿ±ŸÉ.. ÿ™ÿπÿ∑ŸäŸáÿß ŸÖŸÜ 21:00 ŸÑŸÑ 23:00 ÿ™ÿ≥ŸÉÿ± ÿ™ŸäŸÑŸäŸÅŸàŸÜŸÉ ÿ®ÿßÿ¥ ÿßŸÑŸÅÿßŸäÿ≥ÿ®ŸàŸÉ ŸÖÿß ŸäŸÇÿµÿ¥ ÿπŸÑŸäŸÉ.. Ÿàÿ™ÿπÿ∑ŸäŸáÿß ÿ™ŸÇÿ±Ÿâ ŸÅŸä ŸÉÿ™ÿßÿ® ŸàÿßŸÑÿß ÿßÿ™ÿ®ÿπ¬†vid√©o Ÿàÿßÿ∑ÿ®ŸÇ ÿßŸÑŸÑŸä ÿ™ÿ¥ŸàŸÅ ŸÅŸäŸá ÿßŸÑŸÉŸÑ.. ŸàŸÉÿßŸÜ ŸÖÿßŸÉÿ¥ tepratiki ŸÅŸä ÿßŸÑŸÑŸä ÿ™ŸÇÿ±Ÿâ ŸÅŸäŸá.. Ÿàÿ±ÿßÿ≥ ÿßŸÖŸÉ ŸÑÿßŸÉ ÿ®ÿßÿ¥ ÿ™ŸÅŸáŸÖ ÿ¥Ÿäÿ°.. ÿ™ÿ∂Ÿäÿπ ŸÅŸä ŸàŸÇÿ™ŸÉ..\nŸÉÿßŸÜ ÿ™ÿ®ÿπ ÿßŸÑrythme Ÿáÿ∞ÿß ŸÖÿØÿ© 3 ÿßÿ¥Ÿáÿ±.. juste ŸÖÿØÿ© 5 ÿßŸäÿßŸÖ ŸÅŸä ÿßŸÑÿ¨ŸÖÿπÿ©.. ÿ™ŸÉŸàŸÜ ÿπŸÖŸÑÿ™ ÿ®ÿ≥ŸáŸàŸÑÿ© 120 ÿ≥ÿßÿπÿ© programmation ü§©¬†ÿßŸÑÿπÿ≤ ŸàÿßŸÑŸÑŸá Ÿäÿß ŸÖÿπŸÑŸÖ üòÑ ÿµÿØŸÇŸÜŸä Ÿáÿßÿ∞Ÿä ŸÖÿØÿ© ŸÉÿßŸÅŸäÿ© ÿßŸÜŸÉ ÿ™ÿ≥ŸÑŸÉŸáÿß ŸÅŸä ÿßŸä entretien ŸÖÿ™ÿßÿπ stage.. ÿßŸÑŸÖÿØÿ© Ÿáÿßÿ∞Ÿä ÿ™ŸÉŸàŸÜ ÿπŸÖŸÑÿ™ ŸÅŸäŸáÿß ÿ£ŸÇŸÑ ÿ≠ÿßÿ¨ÿ© ÿ≤Ÿàÿ≤ projiet ÿ®ÿßŸáŸäŸÜ ÿ™ÿ≠ÿ∑ŸáŸÖ ŸÅŸä Github ŸÖÿ™ÿßÿπŸÉ ŸàŸäŸÉŸàŸÜŸà r√©ference ÿπŸÑŸâ ŸÖÿ¨ŸáŸàÿØŸÉ.. ÿ®ÿßŸÑÿ∑ÿ®Ÿäÿπÿ© ŸÑÿßÿ≤ŸÖ ÿßŸÑcontenu ÿßŸÑŸÑŸä ÿ™ÿ™ÿπŸÑŸÖ ÿ®ŸäŸá ŸäŸÉŸàŸÜ ŸÖÿ¥ŸÉŸàÿ± ŸàŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿ≠ÿ≥ŸÜ ŸäŸÉŸàŸÜ ŸÜÿßÿµÿ≠ŸÉ ÿ®ŸäŸá ÿ≠ÿØ professionnel ŸÅŸä ÿßŸÑÿßÿÆÿ™ÿµÿßÿµ Ÿáÿ∞ÿßŸÉÿß.. ÿßŸÜ ÿ¥ÿßÿ° ÿßŸÑŸÑŸá ÿ®ÿßÿ¥ ŸÜÿ≠ÿßŸàŸÑ ŸÜÿπŸÖŸÑ page de r√©ference ÿµÿ∫Ÿäÿ±ÿ© ŸÅŸäŸáÿß les ressources ÿßŸÑÿ®ÿßŸáŸäŸÜ ŸÑŸÑJava ÿßŸÖ ÿßŸÑÿØŸÜŸäÿß..\nŸÜÿµŸäÿ≠ÿ© ÿßÿÆÿ±Ÿâ ŸÅŸä ÿ®ÿßÿ® ÿßŸÑcomp√©tence technique.. ÿßŸÑŸÑŸä Ÿäÿ™ÿπŸÑŸÖ ŸÑÿ∫ÿ© ÿ®ÿ±ŸÖÿ¨ÿ© ŸàÿßŸÑÿß technologie ŸÖÿπŸäŸÜÿ©.. ÿ≠ÿßŸàŸÑ ÿ™ÿÆŸÑŸäŸÉ up-to-date ÿ®ÿßŸÑŸÑŸä ÿµÿßŸäÿ± ŸÅŸä ÿßŸÑMonde IT.. ŸÅŸÖÿß des sites ŸÖÿ™ÿßÿπ ÿßÿÆÿ®ÿßÿ± ÿßŸÑIT ŸàÿßŸÑProgrammation ÿ≠ÿßŸàŸÑ ÿ™ÿ∑ŸÑ ÿπŸÑŸäŸá ÿ≠ÿ™Ÿâ ÿØÿ±ÿ¨ŸäŸÜ ŸÅÿßŸÑŸÜŸáÿßÿ± ŸÅŸäŸá ÿßŸÑÿ®ÿ±ŸÉÿ©.. ŸÖÿ´ŸÑÿß Dzone InfoQ JaxEnter Medium Quora Ÿàÿ®ÿ±ÿ¥ÿ© ŸÜÿßÿ≥ ÿπÿßŸÑTwitter ÿ™Ÿáÿ®ÿ∑ ÿßÿÆÿ®ÿßÿ± Ÿàdes liens Ÿäÿßÿ≥ÿ± ÿ≠ŸÑŸàŸäŸÜ ŸàŸÅÿßŸáŸÖ ŸÖŸÜŸÅÿπÿ©.. ÿßŸÑŸÖÿ™ÿßÿ®ÿπÿ© Ÿáÿßÿ∞Ÿä ÿ™ÿÆŸÑŸäŸÉ ÿ™ŸÜÿ¨ŸÖ ÿ™Ÿàÿ±Ÿä ÿßŸÑŸÑŸä ÿßŸÜÿ™ int√©ress√© ÿ®ÿßŸÑinformatique ŸàŸÖÿ∫ÿ±ŸàŸÖ ŸàŸÖŸàÿ¥ ÿßÿ™ÿ®ÿπ ŸÅŸäŸáÿß ÿßÿ®ÿ™ŸÑÿßÿ° ŸÖŸÜ ÿπŸÜÿØ ÿ±ÿ®Ÿä.. ŸáŸÉÿ© ÿßŸÑŸÑŸä ŸäÿπŸÖŸÑŸÉ ŸÅŸä ÿßŸÑentretien Ÿäÿ≠ÿ≥ ÿßŸÑŸÑŸä ÿßŸÜÿ™ ŸÖŸàÿ¥ ŸÖÿ¨ÿ±ÿØ √©tudiant ŸÇÿ±Ÿâ ÿßŸÑprogrammation ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸàŸÅŸä ÿßŸÑÿØÿßÿ±.. ŸÑÿß Ÿáÿ∞ÿß candidat ŸäŸÑŸàÿ¨ ÿπŸÑŸâ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ŸàŸäÿ≠ÿ® ŸäŸÉŸàŸÜ √† jour.. ŸàŸÖÿßŸÅŸÖÿßÿ¥ ÿÆŸäÿ± ŸÖŸÜ ŸáŸÉÿ©.. ü§î bon ŸÅŸÖÿ© ÿÆŸäÿ± ŸÖŸÜ ŸáŸÉÿ©.. ŸàŸáŸà ÿßŸÜŸÉ ÿ™ÿ≤ŸäÿØ ÿ™ÿ™ÿπŸÑŸÖ technologie ÿßÿÆÿ±Ÿâ ŸÖÿπ ÿßŸÑtechnologie ÿßŸÑÿßÿ≥ÿßÿ≥Ÿäÿ© ÿßŸÑŸÑŸä ŸÖÿ±ŸÉÿ≤ ŸÖÿπÿßŸáÿß.. ŸÖÿ´ŸÑÿß ŸÖÿπ Java ÿ™ÿ≤ŸäÿØ ÿ¥ŸàŸäÿ© Javascript ŸàÿßŸÑÿß Base de donn√©es.. ŸàÿßŸÑÿß ŸÖÿπ ÿßŸÑJava ÿ™ÿ≤ŸäÿØ Ÿàÿ≠ŸäÿØ Docker ÿ™ŸàŸÑŸä ÿ®ÿßŸÑÿ±ÿ≥ŸÖŸä ÿ™ÿπŸÖŸÑ 66 ŸÉŸäŸÅ..\nŸÜÿµŸäÿ≠ÿ© ÿßÿÆÿ±Ÿâ.. ŸÉÿßŸÜ ŸÖÿßŸÑŸÇŸäÿ™ÿ¥ ŸÖŸÜŸäŸÜ ÿ™ÿ®ÿØŸâ ÿßÿ≥ÿ£ŸÑ.. ŸÑŸàÿ¨.. ÿßÿ≠ŸÉŸä ŸÖÿπ ÿßŸÑŸÜÿßÿ≥ ÿßŸÑŸÑŸä ÿ™ÿÆÿØŸÖ.. ÿßÿ∑ŸÑÿ® ÿßŸÑŸÜÿµŸäÿ≠ÿ© ŸÖŸÜ ÿ£Ÿä Ÿàÿßÿ≠ÿØ.. ÿßŸÑŸÖŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ™ÿπŸÑŸÖ.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸÜ ÿßŸÑŸÑŸä Ÿäÿ∑Ÿäÿ≠ŸÑŸÉ ÿßŸÑMoral.. ÿßŸÑÿ¥ÿπÿ® ÿßŸÑÿ™ŸàŸÜÿ≥Ÿä ŸÖŸÜ ÿ£ŸÇŸàŸâ ÿ¥ÿπŸàÿ® ÿßŸÑÿπÿßŸÑŸÖ ŸÅŸä ÿ™ÿ∑ŸäŸäÿ≠ ÿßŸÑŸÖŸàÿ±ÿßŸÑ Ÿàÿ®ÿßŸÑÿ™ÿ¨ÿ±ÿ®ÿ© ŸàÿßŸÑŸÑŸá ÿπÿßŸÜŸäÿ™ ŸÖŸÜŸà ŸÖÿπÿßŸÜÿßÿ© ÿ≤ÿ±ŸÇÿßÿ° ŸàÿÆÿ∂ÿ±ÿßÿ° ÿ≤ÿßÿØÿ©..\nŸàÿ®ÿßŸÑŸÑŸá.. ŸÅŸä ŸÉŸÑ g√©n√©ration Ÿäÿ¨ŸäŸÉ Ÿàÿßÿ≠ÿØ Ÿäÿ≠ŸÉŸäŸÑŸÉ ÿπŸÑŸâ ÿ≠ÿØ Ÿäÿπÿ±ŸÅŸà ÿØÿ®ÿ± stage ŸÅŸä ÿ¥ÿ±ŸÉÿ© ŸàŸáŸà ÿ∏ŸÑŸÖÿ© ŸàÿßŸÑÿ¥ÿ±ŸÉÿ© ŸÇÿ®ŸÑÿ™Ÿà Ÿàÿ®ÿßÿ¥ ÿ™ÿπŸÖŸÑŸà formation ŸÉÿßŸÖŸÑÿ© Ÿàÿ™ÿ™ŸÑŸáŸâ ÿ®ŸäŸá.. ŸàÿßŸÑŸÑŸá ÿßŸÑŸÉÿ∞ÿ®ÿ© Ÿáÿßÿ∞Ÿä ŸÉŸÑ ÿπÿßŸÖ ÿ™ÿ™ÿπÿßŸàÿØ.. ŸàŸÖÿßŸÅŸÖÿß ÿ≠ÿ™Ÿâ soci√©t√© ÿ™ÿπŸÖŸÑ ŸáŸÉÿ© ŸÉÿßŸÜ ÿ¨ŸÖÿßÿπÿ© ÿßŸÑÿßŸÉÿ™ÿßŸÅ ÿ≠ÿßÿ¥ÿßŸÉŸÖ üòÅ ŸÉŸä Ÿäÿ¨ŸäŸÉ Ÿàÿßÿ≠ÿØ ŸäŸÇŸÑŸÉ ŸáÿßŸÑÿÆÿ±ÿßŸÅÿ© ÿ®ÿßŸÑŸÑŸá ÿßÿ¨ÿ®ÿØ ÿπŸÑŸäŸá Ÿàÿßÿ¥ŸÅŸäŸÑŸä ÿ∫ŸÑŸäŸÑŸä ŸÖŸÜŸà..\nŸÜÿ±ÿ¨ÿπŸà ŸÑŸÖŸàÿ∂ŸàÿπŸÜÿß.. ÿßŸäŸá ŸÉŸä ÿ£ŸÜÿß ÿ≠ÿßÿ∂ÿ± techniquement ÿßÿ¥ ÿ®ÿßÿ¥ Ÿäÿ≤ŸäÿØŸÜŸä ÿßŸÑstage ÿüÿü\nÿ≥ŸäÿØŸä Ÿàlalla.. ŸÉÿßŸÜ ÿ®ÿØŸäÿ™ ÿ™ÿ≠ÿ≥ ÿ±Ÿàÿ≠ŸÉ ŸÖÿπŸÑŸÖ Ÿàÿ®ÿØŸâ ÿßŸÑÿ∫ÿ±Ÿàÿ± Ÿäÿπÿ¥ÿ¥ ŸÅŸäŸÉ.. ÿ™ÿ∞ŸÉÿ± ÿßŸÑŸÑŸä ÿßŸÑinformatique ÿ®ÿ≠ÿ± ŸàÿßŸÑÿπŸàÿßŸÖÿ© ÿßŸÑÿµÿ≠ÿßÿ≠ ÿßŸÇŸàŸâ ŸÖŸÜ ÿßŸÑŸÑŸä ÿ™ÿ™ÿµŸàÿ±.. donc ŸÉŸÑ ŸÖÿßÿ™ÿ≠ÿ≥ ÿ±Ÿàÿ≠ŸÉ ŸÖÿπŸÑŸÖ ÿßŸÜÿπŸÑ ÿ®ŸÑŸäÿ≥ Ÿàÿ®ÿ±Ÿâ ÿ≤ŸäÿØ ÿßŸÇÿ±Ÿâ Ÿàÿ™ÿπŸÑŸÖ.. ŸàÿßŸÑŸÜŸáÿßÿ± ÿßŸÑŸÑŸä ÿ®ÿßÿ¥ ŸäŸÅŸàÿ™ŸÉ ŸàŸÖÿß ÿ™ÿ™ÿπŸÑŸÖÿ¥ ŸÅŸäŸá ŸÖÿßÿπÿßÿØÿ¥ Ÿäÿ™ÿπÿßŸàÿØ Ÿäÿß ÿÆÿ≥ÿßÿ±ÿ©.. ŸÖŸÜ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÜÿ∑ŸÑŸÇ.. ÿßŸÑstage ÿ®ÿßÿ¥ Ÿäÿ≤ŸäÿØŸÉ ŸÉŸÑ ÿ¥Ÿäÿ°.. ÿ®ÿßÿ¥ ÿ™ÿ™ÿπŸÑŸÖ ŸÉŸÑ ÿ¥Ÿäÿ°.. ŸÉŸäŸÅÿßŸá ÿ™ÿÆÿØŸÖ.. ŸÉŸäŸÅÿßŸá tcodi.. ŸÉŸäŸÅÿßŸá ÿ™ÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿßŸÑÿ®ÿßŸáŸä ŸàÿßŸÑÿÆÿßŸäÿ®.. ŸÉŸäŸÅÿßŸá ÿ™ÿ™ÿ¨ŸÜÿ® ÿ¥ÿ± ÿßŸÑÿÆÿßŸäÿ®.. ŸÉŸäŸÅÿßŸá ÿßÿ™ŸàŸÑŸä joueur ÿ®ÿßŸáŸä ŸÅŸä ŸÅÿ±ŸäŸÇ ÿ™ŸÇŸÜŸä.. ÿ®ÿßÿ¥ ÿ™ŸàŸÑŸä ÿπŸÜÿØŸÉ exp√©rience ÿ≠ŸÇŸäŸÇŸäÿ© Ÿàr√©ference ÿ™ÿ≠ÿ∑Ÿáÿß ŸÅŸä ÿßŸÑCV ŸÖÿ™ÿßÿπŸÉ.. ŸàÿßŸàŸÑ ÿÆÿ∑Ÿàÿ© ŸÅŸä ÿßŸÑcarri√®re ŸÖÿ™ÿßÿπŸÉ..\n2- ÿßÿπŸÖŸÑ CV ÿ®ÿßŸáŸä\nŸÅŸä ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÑŸä ŸÅÿßÿ™ÿ™ ÿµŸÜÿπŸÜÿß ÿ±Ÿàÿßÿ≠ŸÜÿß.. ÿ™Ÿàÿ© ŸÑÿßÿ≤ŸÖ ŸÜÿ®ŸäÿπŸàÿß ÿßŸÑprofil ÿßŸÑŸÑŸä ÿµŸÜÿπŸÜÿßŸá.. Ÿàÿ®ÿßÿ¥ ŸÜÿ¨ŸÖŸà ŸÜÿ®ŸäÿπŸàŸá ŸÑÿßÿ≤ŸÖ ŸÜÿØÿÆŸÑŸà ŸÅŸä ÿßŸÑMarketing.. ŸàŸÖŸàÿ¥ ÿßŸÑMarketing ÿßŸÑŸÉÿ∞ÿßÿ®.. Ÿáÿ∞ÿß Marketing ŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ ÿµÿßÿØŸÇ ÿ®ÿ±ÿ¥ÿ© ŸàÿπÿßŸÑÿßÿÆÿ± ÿ≤ÿßÿØÿ©.. ÿ®ÿßÿ¥ ÿßŸÑÿπŸàÿßŸÇÿ® ŸÖÿßÿ™ŸÉŸàŸÜÿ¥ ŸÉŸäŸÖÿß ŸäŸÇŸàŸÑŸàÿß.. ŸàÿÆŸäŸÖÿ©..\nÿßŸÑCV ŸáŸàvitrina ÿ®ÿßÿ¥ ÿ™ÿ®Ÿäÿπ ÿ®ÿßŸáÿß.. ÿπŸÑŸâ ŸÇÿØ ŸÖÿß ÿ™ŸÉŸàŸÜ ŸÖÿ≤ŸäÿßŸÜÿ© ÿπŸÑŸâ ŸÇÿØ ŸÖÿß ÿßŸÑÿ®ŸäŸàÿπ ŸäŸÉŸàŸÜ ÿÆŸäÿ±.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸàÿ¥ ÿßŸÑÿßŸÑŸàÿßŸÜ ŸàÿßŸÑLayout ŸáŸàŸÖÿß ŸÖŸÇŸäÿßÿ≥ ÿßŸÑCV.. ŸÉÿßŸÜ ŸáŸÉÿßŸÉÿ© ÿ±ÿßŸà ÿßŸÑŸÖŸÑŸàÿÆŸäÿ© ÿπŸÜÿØŸáÿß ÿßÿÆŸäÿ® CV..\nÿ®ÿßÿ¥ ŸÖÿßŸÜÿ∑ŸàŸÑÿ¥ ÿπŸÑŸäŸÉŸÖ.. ÿ®ÿßÿ¥ ÿ™ÿ™ŸÉÿ™ÿ® ÿßŸÑCV ŸÜŸÜÿµÿ≠ŸÉ ÿ®ŸÖŸàŸÇÿπ Ÿàÿßÿ≠ÿØ ŸáŸà ÿßŸÑEuropass\nÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸÜ ÿßÿÆÿ∑ÿßÿ° ÿßŸÑorthographe ÿπŸÜÿØŸÉ ÿßŸÑGoogle ÿßŸä ŸÉŸÑŸÖÿ© ŸäÿµŸÑÿ≠Ÿáÿß.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸÜ Espace ŸÜÿßŸÇÿµ ŸàÿßŸÑÿß ÿ≤ÿßŸäÿØ.. Ÿàÿ±ÿØ ÿ®ÿßŸÑŸÉ ŸÖŸÜ ÿßÿ≥ŸÖ logicielle ŸàÿßŸÑÿß ŸÖÿßÿ±ŸÉÿ© ŸàÿßŸÑÿß ŸÉŸÑŸÖÿ© ŸáÿßŸÖÿ© ÿ™ŸÉÿ™ÿ®Ÿáÿß ŸÉŸÑŸáÿß miniscule.. ÿßŸÑÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© Ÿáÿßÿ∞Ÿä ÿ™ÿπŸÖŸÑ ŸÅÿ±ŸÇ ŸÉÿ®ŸäŸäŸäŸäŸäŸäÿ±.. ÿ£ŸÜÿß Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿßŸÑŸÜÿßÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° Ÿáÿßÿ∞Ÿä ÿ™ŸÇŸÑŸä ÿßŸÑŸÑŸä ÿßŸÑŸÖÿ™ÿ±ÿ¥ÿ≠ ŸáŸà ÿßŸÜÿ≥ÿßŸÜ ŸÑÿßŸÖÿ®ÿßŸÑŸä ü§™\nÿßŸÑÿπÿßŸÖ ÿßŸÑŸÑŸä ŸÅÿßÿ™ ÿπŸÖŸÑÿ™ vid√©o ŸÖÿπ ÿµÿØŸäŸÇŸä ÿßŸÑÿπÿ≤Ÿäÿ≤ ÿ≠ÿ≥ÿßŸÖ ÿßŸÑÿØŸÑÿßÿπŸä ÿ≠ŸÉŸäŸÜÿß ŸÅŸäŸá ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿ≥ÿßÿπÿ© ŸÉŸäŸÅÿßŸá ÿ™ÿπŸÖŸÑ CV ÿ®ÿßŸáŸä.. ÿ¨ÿ±ÿßŸäÿ±Ÿà ŸàÿßŸÑŸÑŸá ÿπŸÖÿ±Ÿä ŸÖÿßÿπŸÖŸÑÿ™ vid√©o üòÇüòÇ\nhttps://www.youtube.com/watch?v=0od4O4g6k7g\n3- postuli partout\nÿ®ŸÖÿß ÿ£ŸÜŸà ŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© des sites ŸÖÿ™ÿßÿπ des offres d\u0026rsquo;emploi ŸàÿßŸÑÿß Ÿäÿ™ÿ≥ŸÖŸà job boards ÿ®ÿßŸÑÿßŸÜŸÇŸÑŸäÿ≤Ÿä Ÿäÿß ŸÖÿ±ÿ≥Ÿä.. ÿ≠ÿ∂ÿ±ÿ© ÿ≥ŸäÿßÿØÿ™ŸÉ ŸÑÿßÿ≤ŸÖ ÿ™ÿµÿ® ŸÅŸä ÿßŸÑmaximum ŸÖŸÜ les sites.. ŸàŸÅŸÖÿß ÿ®ÿ±ÿ¥ÿ© des sites ŸÖÿπÿ±ŸàŸÅŸäŸÜ ÿ®ÿßŸáŸäŸÜ ŸÉŸäŸÅ Linkedin ŸàÿßŸÑMonster .. Ÿàÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑ les stages ÿßŸÑsite ÿßŸÑŸÑŸä ŸÑŸÇŸäÿ™ ŸÅŸäŸá stage ÿßŸÑlicence Ÿàÿ≠ÿ™Ÿâ ÿßŸÑmaster¬†ŸáŸà Aidostage ŸÜŸÜÿµÿ≠ŸÉŸÖ ÿ®ŸäŸá..\nŸÉŸäŸÅ ÿ™ÿµÿ® ŸÖŸàÿ¥ ŸÖÿπŸÜÿßŸáÿß ÿ≥ŸÑŸÉÿ™Ÿáÿß Ÿàÿ®ÿßÿ¥ ÿ™ÿ¨ŸäŸÉ r√©ponse.. ŸÑÿß ŸÖŸàÿ¥ ÿ®ÿßŸÑŸÑÿßÿ≤ŸÖÿ©.. ÿ®ÿ±ÿ¥ÿ© ÿ¥ÿ±ŸÉÿßÿ™ Ÿäÿ¨ÿßŸàÿ®ŸàŸÉ ŸàŸäŸÇŸàŸÑŸàŸÑŸÉ ŸÑÿß Ÿàÿ®ÿ±ÿ¥ÿ© ŸÑŸÑÿ£ÿ≥ŸÅ Ÿäÿ∑ŸÅŸàŸÉ ÿ¨ŸÖŸÑÿ© Ÿàÿ™ŸÅÿµŸäŸÑ.. ŸÅŸä ÿßŸÑÿ≠ÿßŸÑÿ© Ÿáÿßÿ∞Ÿä ÿßÿ¥ ŸÜÿπŸÖŸÑÿü ŸÜŸÉÿ®ÿ¥ ŸàŸÜŸÉŸÖŸÑ ÿ±ÿ≠ŸÑÿ© ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿ™ÿßÿπŸä ŸàŸäÿß ÿ¨ÿ®ŸÑ ŸÖÿßŸäŸáŸÖŸÉ ŸÅŸä ÿ±Ÿäÿ≠..\nÿ™ŸàÿµŸÑÿ¥Ÿä ÿ™ÿµÿ® ÿßŸÑŸÅ demandes ŸÅŸä ÿßŸÑŸÜŸáÿßÿ± ÿüÿü ŸÖŸäÿ≥ÿßŸÑÿ¥ ŸÖÿßÿØÿßŸÖŸÉ ŸÖÿßÿ≤ŸÑÿ™ ÿ™ÿ™ŸÜŸÅÿ≥ ÿßÿ∂ÿ±ÿ® ÿπŸÑŸâ ÿßŸÑŸáÿØŸÅ ŸÖÿ™ÿßÿπŸÉ ŸÑŸÑÿ£ÿÆÿ±..\nÿ∫ŸÑÿ∑ÿ© ÿ¥ŸÜŸäÿπÿ© ŸÅÿ∂Ÿäÿπÿ© ŸÖÿ±Ÿäÿπÿ© ŸäÿπŸÖŸÑŸàŸáÿß ÿ®ÿ±ÿ¥ÿ© ŸÜÿßÿ≥.. Ÿäÿ¨Ÿä ÿ®ÿßÿ¥ Ÿäÿ®ÿπÿ´ candidature ŸÑÿπÿ¥ÿ±ÿ© ÿ¥ÿ±ŸÉÿßÿ™.. ÿ®ÿßŸÑÿ®ÿÆŸÑ ŸàÿßŸÑÿ®ŸáÿßŸÖÿ©.. Ÿäÿ®ÿπÿ´ ŸÖÿßŸäŸÑ Ÿàÿßÿ≠ÿØ ŸàŸäÿ≠ÿ∑ ÿßŸÑŸÜÿßÿ≥ ÿßŸÑŸÉŸÑ destinataire üòÇüòÇ Ÿáÿ∞ÿß ÿ£ŸÉÿ®ÿ± message ŸäŸÇŸàŸÑ ŸÅŸäŸá ÿ±ÿßŸÜŸä ÿ®ÿÆŸäŸÑ ŸàŸÖÿßŸÜÿ≠ÿ®ÿ¥ ÿßŸÑÿ™ÿπÿ®.. ŸÅÿßÿ¥ ŸÇÿßŸÖ ŸÉŸÑ Ÿàÿßÿ≠ÿØ ŸÜÿ®ÿπÿ´ŸÑŸà mail.. ŸáÿßŸà Ÿàÿßÿ≠ÿØ ŸÑŸÑŸÜÿßÿ≥ ÿßŸÑŸÉŸÑ ŸàŸÉÿßŸÜ ŸÅŸÖÿß ÿ¥ŸÉŸàŸÜ Ÿäÿ≠ÿ® ÿπŸÑŸâ stagiaire ÿ®ÿÆŸÑŸä ŸáÿßŸÜŸä ŸÖŸàÿ¨ŸàÿØ..\nÿßŸÑsignature ÿ≤ÿßÿØÿ© ÿßŸÑŸÑŸä Meilleures salutations ŸÖŸÉÿ™Ÿàÿ®ÿ© ŸÅŸäŸáÿß en dur ÿ≤ÿßÿØÿ© ÿ±ÿßŸáŸä ŸÖŸàÿ¥ ÿ®ÿßŸáŸäÿ©.. ÿ™ÿÆŸäŸÑ Ÿàÿßÿ≠ÿØ ÿ®ÿßÿÆŸÑ ÿßŸÜŸà Ÿäÿ≠ŸäŸäŸÉÿüÿü ÿ¥ŸÜŸäÿ© ŸÖÿπŸÜÿßŸáÿß ÿ®ÿßŸÑŸÑŸáÿüÿü\nmail ÿßŸÑ candidature ŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÇÿµŸäÿ± simple Ÿà efficace.. ŸàŸÉÿßŸÜ ÿ™ÿπÿ±ŸÅ ÿßŸÑÿ≥ŸäÿØ ÿßŸÑŸÑŸä ÿ®ÿßÿπÿ´ŸÑŸà ÿ™ÿ≠ÿ∑ŸÑŸà Dear Sir Touhemi ŸàÿßŸÑÿß Madame 3eljia.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ÿ™ÿ®ÿØŸâ ÿ®ÿßÿπÿ´ ŸÖÿßŸäŸÑ ŸÑŸÖÿ±ÿ£ÿ© ŸàŸÅŸä ÿßŸÑgreetings ÿ™ÿ®ÿØŸâ ŸÉÿßÿ™ÿ® sir/madame ÿüÿü ŸÉÿßŸÜ ÿßÿ≥ŸÖ ÿßŸÑÿ≥ŸäÿØ ÿ®ÿßŸäŸÜ ŸÅŸä ÿßŸÑadresse mail ÿßŸÖÿ¥Ÿä ÿ¥ŸàŸÅŸà ŸÅŸä ÿßŸÑÿßŸÜÿ™ÿ±ŸÜÿßÿ™ Ÿàÿ´ÿ®ÿ™ ŸÖÿ±ÿ£ÿ© ŸàÿßŸÑÿß ÿ±ÿßÿ¨ŸÑ ŸäŸáŸäÿØŸÉ..\n4- ÿÆŸÑŸä moralek ÿ∑ÿßŸÑÿπ Ÿàÿ´ŸÇÿ™ŸÉ ŸÅŸä ÿ±Ÿàÿ≠ŸÉ ŸÇŸàŸäÿ©\nÿßŸÑŸÑŸä ŸÇÿßŸÑ ÿ®ŸÉÿ±Ÿä ÿ®ÿ≠ÿ´ ÿµÿπŸäÿ® ŸÉÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßÿ®ÿ±ÿ© ŸÅŸä ŸÉŸàŸÖÿ© ŸÇÿ¥ ŸáŸà ÿßŸÜÿ≥ÿßŸÜ ŸÖÿßŸäÿπÿ±ŸÅÿ¥ ÿßŸÑŸÑŸä ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÑŸâ stage ÿßÿµÿπÿ® ÿ®Ÿäÿßÿ≥ÿ±.. ÿ®ÿ≠ÿ´ ŸÅŸäŸá ÿ®ÿ±ÿ¥ÿ© ÿ™ÿπÿ®.. ÿ®ÿ±ÿ¥ÿ© ÿ™ÿ∑ŸäŸäÿ≠ ÿßŸÑŸÖŸàÿ±ÿßŸÑ.. ÿ®ÿ±ÿ¥ÿ© ÿßŸÜÿ™ÿ∏ÿßÿ±.. ÿ®ÿ±ÿ¥ÿ© ÿ£ŸÖŸÑ Ÿàÿ®ÿ±ÿ¥ÿ© ÿßÿ≠ÿ®ÿßÿ∑.. ÿ®ÿ±ÿ¥ÿ© ŸÉŸÑÿßŸÖ Ÿàÿ®ÿ±ÿ¥ÿ© ŸàÿπŸàÿØ Ÿàÿ®ÿ±ÿ¥ÿ© programmet.. ŸÑŸÉŸÜ ÿßŸÑÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÑÿÆÿßŸäÿ®ÿ© Ÿáÿßÿ∞Ÿä ŸáŸä ÿ¥ÿ± ŸÑÿßÿ®ÿØ ŸÖŸÜŸà.. ÿßŸÑÿ¥ÿ¨ÿ±ÿ© ÿ®ÿßÿ¥ ÿ™ŸàŸÑŸä ŸÉÿ±ÿ≥Ÿä ŸàÿßŸÑÿß ÿ∑ÿßŸàŸÑÿ© ŸÑÿßÿ≤ŸÖŸáÿß ÿ™ÿ™ÿ¨ÿ±ÿ≠ ÿ®ÿßŸÑŸÖŸÜÿ¥ÿßÿ± Ÿàÿ™ÿÆÿ≥ÿ± ÿ®ÿ±ÿ¥ÿ© ŸÖŸÜ ÿ®ÿØŸÜŸáÿß.. ŸáŸÉÿßŸÉÿ© ÿßŸÑÿÆÿØŸÖÿ© ŸàÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸÖŸáŸÜŸäÿ©.. ŸÑÿßÿ≤ŸÖ ÿßŸÑÿ™ÿπÿ® ŸàÿßŸÑŸàÿ¨Ÿäÿπÿ© ÿ®ÿßÿ¥ ÿßŸÑŸÜÿ¨ÿßÿ≠ ŸäŸÉŸàŸÜ ÿπŸÜÿØŸà ÿ∑ÿπŸÖ ÿ≠ŸÑŸà..\nÿÆŸÑŸä moralek ÿ∑ÿßŸÑÿπ.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ Ÿäÿ∂ÿ±ÿ®ŸÉ virus ÿßŸÑŸäÿ£ÿ≥.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ÿ≠ÿØ Ÿäÿ∑Ÿäÿ≠ŸÑŸÉ moralek ŸàÿßŸÑÿß ŸäŸÖÿ≥ŸÑŸÉ ÿπÿ≤ŸäŸÖÿ™ŸÉ.. ÿÆŸÑŸäŸÉ ŸÇŸàŸä.. ÿßŸÑÿ≠ŸÑŸÖ ÿ≠ŸÑŸÖŸÉ ŸàÿßŸÜÿ™ ŸÇÿßÿπÿØ ÿ™ÿ™ÿπÿ® ÿ®ÿßÿ¥ ÿ™ÿ≠ŸÇŸÇŸà.. donc ÿ≠ÿ™Ÿâ Ÿàÿßÿ≠ÿØ ŸÖÿßÿπŸÜÿØŸà ÿßŸÑÿ≠ŸÇ ŸäŸÇÿ±ÿ® ŸÖŸÜ ÿ∑ŸÖŸàÿ≠ŸÉ..\nŸÉŸÑ ŸÖÿßÿ™ÿ≠ÿ≥ ÿßŸÑŸÑŸä ŸÇÿßÿπÿØ postuli Ÿàÿ≠ÿØ ŸÖÿßŸäÿ¨ÿßŸàÿ® ŸÅŸäŸÉ.. ÿßŸàŸÑ ÿ≠ÿßÿ¨ÿ© ŸÑÿßÿ≤ŸÖŸÉ ÿ™ÿ¥ŸÉ ŸÅŸä ÿßŸÑCV.. ŸÖŸÖŸÉŸÜ ÿ™ŸÉŸàŸÜ ÿπŸÖŸÑÿ™ ÿ∫ŸÑÿ∑ÿ©.. ŸÖŸÖŸÉŸÜ ŸÖŸàÿ¥ d√©taill√©.. ŸÖŸÖŸÉŸÜ ÿ®ÿßŸäŸÜ ŸÖÿ™ÿßÿπ ÿ≤ÿ±ÿ®ÿ© ŸàŸÖŸàÿ¥ ŸÖÿÆÿØŸàŸÖ ÿ®ÿßŸÑÿ®ÿßŸáŸä.. ŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑŸÉŸÑ ŸÑÿßÿ®ÿßÿ≥.. ÿ¥ŸàŸÅ ÿßŸÑmail ÿßŸÑŸÑŸä ÿ™postuli ÿ®ŸäŸá.. ŸäŸÜÿ¨ŸÖ ŸäŸÉŸàŸÜ ŸÖŸàÿ¥ ÿ®ÿßŸáŸä.. ÿßŸÑcopier coller Ÿäÿ®ÿßŸÜ ÿ±ÿßŸáŸà.. donc ŸÖÿßŸÅŸäŸáÿß ÿ®ÿßÿ≥ ŸÉÿßŸÜ ÿ™ÿ®ÿ∞ŸÑ ÿ¥ŸàŸäÿ© ÿ¨ŸáÿØ Ÿàÿ™ŸÉÿ™ÿ® ÿ®ŸäÿØŸÉ ÿÆŸäÿ± ŸÖŸÜ template ŸÜÿµ ÿßŸÑŸÉŸàŸÉÿ® Ÿäÿ≥ÿ™ÿπŸÖŸÑ ŸÅŸäŸáÿß..\n5- ÿÆŸÑŸäŸÉ ÿ≠ÿßÿ∂ÿ± ŸÜŸÅÿ≥ÿßŸÜŸäÿß\nŸáŸÜÿß ÿ®ÿßÿ¥ ŸÜÿ≠ŸÉŸà ÿπŸÑŸâ ÿ®ÿ±ÿ¥ÿ© ÿ≠ÿßÿ¨ÿßÿ™ ÿßŸÖÿß ÿ®ÿßÿÆÿ™ÿµÿßÿ±..\nÿØŸäŸÖÿß ÿÆŸÑŸäŸÉ ÿ≠ÿßÿ∂ÿ±.. ŸÉŸäŸÅ ÿ¥ÿ±ŸÉÿ© ÿ®ÿßÿ¥ ÿ™ŸÉŸÑŸÖŸÉ ÿπŸÑŸâ entretien ŸÑÿßÿ≤ŸÖ ÿ™ÿ®ÿØŸâ ÿ≠ÿßÿ∂ÿ± ŸàŸÖŸàÿ¨ŸàÿØ ŸàŸÅŸä ÿßŸÑŸàŸÇÿ™.. ŸäŸÜÿ¨ŸÖ ŸäŸÉŸàŸÜ ÿ®ÿßŸÑÿ™ŸÑŸäŸÅŸàŸÜ ŸàŸäŸÜÿ¨ŸÖ ŸäŸÉŸàŸÜ Skype.. ÿ±ÿØ ÿ®ÿßŸÑŸÉ ÿ™ÿ®ÿØÿßÿ¥ ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸàŸÇÿ™ ÿßŸÑŸÖÿ≠ÿØÿØ ŸàÿßŸÑÿß ÿ™ÿ≠ŸÑ camera ŸÅŸä ÿßŸÑskype ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖŸÜ interlocuteur ŸäŸÇŸÑŸÉ.. ŸàÿßŸÑÿß Ÿàÿßÿ≠ÿØ Ÿäÿ∑ŸÑÿ® Ÿäÿ¥ŸàŸÅŸÉ ŸÅŸä ÿßŸÑŸÉÿßŸÖ ÿ™ŸÇŸÑŸà ŸÑÿß.. ÿ£Ÿä Ÿàÿßÿ≠ÿØ ÿ®ÿßÿ¥ ŸäÿÆÿØŸÖ ŸÖÿπÿßŸÉ ŸÖŸÜ ÿ≠ŸÇŸà Ÿäÿ¥ŸàŸÅ Ÿàÿ¨ŸáŸÉ ÿπÿßŸÑÿ£ŸÇŸÑ ÿ®ÿßÿ¥ ŸäÿπŸÖŸÑ ŸÖÿπÿßŸÉ lien humain..\nŸÖÿßŸÜŸäÿ¥ ŸÜÿπŸÑŸÖ ŸÅŸäŸÉ ŸÅŸä ÿßŸÑetiquette ŸàÿßŸÑÿß ÿ¨Ÿà ÿßŸÑŸÑŸä Ÿäÿ¨Ÿä ŸàÿßŸÑÿß ŸÖÿßŸäÿ¨Ÿäÿ¥.. ÿ£ŸÖÿß ÿØŸäŸÖÿß ŸÉŸäŸÅ ÿ™ÿ¨Ÿä ÿ™ÿπŸÖŸÑ ÿ£Ÿä ÿ≠ÿßÿ¨ÿ© ÿ≠ÿ∑ ÿ±Ÿàÿ≠ŸÉ ŸÅŸä ÿ®ŸÑÿßÿµÿ© ÿßŸÑÿ≥ŸäÿØ ÿßŸÑŸÑŸä Ÿäÿ≠ŸÉŸä ŸÖÿπÿßŸÉ.. ŸÅŸä ÿ®ŸÑÿßÿØ ÿßÿÆÿ±Ÿâ ŸàÿπŸÇŸÑŸäÿ™Ÿá ŸÖŸàÿ¥ ŸÉŸäŸÅŸÉ.. ÿßÿÆÿ™ŸÑÿßŸÅ ÿßŸÑÿßŸÅŸÉÿßÿ± ŸÖÿ±ÿßÿ™ ŸäŸÉŸàŸÜ ÿµÿπŸäÿ® Ÿàÿ∑ÿ±ŸäŸÇÿ™ŸÉ ŸÅŸä ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπÿßŸá ŸáŸä ŸÖŸÜ ÿ£ÿ±Ÿàÿπ les signes ŸÖÿ™ÿßÿπ ŸÇŸàÿ© ÿ¥ÿÆÿµŸäÿ™ŸÉ..\nŸàŸÑÿß ŸÇÿØÿ± ÿßŸÑŸÑŸá ÿ®ÿπÿØ ŸÖÿß ÿπŸÖŸÑÿ™ Ÿáÿ∞ÿß ÿßŸÑŸÉŸÑ ŸàŸÖÿßŸÑŸÇŸäÿ™ÿ¥ stage.. ÿ≠ÿßŸàŸÑ ÿ™ÿ®ÿØŸÑ direction ŸÖÿ™ÿßÿπŸÉ.. ÿ≠ÿßŸàŸÑ ÿ™ÿ¥ŸàŸÅ ŸÅŸä ÿ®ŸÑÿßÿµÿ© ÿ£ÿÆÿ±Ÿâ.. ÿ≠ÿßŸàŸÑ ÿ™ÿ¥ŸàŸÅ ŸÅŸä ÿßÿÆÿ™ÿµÿßÿµ ÿßÿÆÿ±.. ÿßŸÑŸÖŸáŸÖ ÿ®ÿßŸÑŸÑŸá virus ÿßŸÑŸäÿ£ÿ≥ ŸÑÿß..\nŸÅŸä ÿßŸÑÿßÿÆÿ±.. ŸÅŸä ÿßŸÑÿ£ÿÆÿ± ŸÉŸÑŸÖÿ© ŸÇŸÑÿ™Ÿáÿß ŸÅŸä ÿßŸÑŸÅÿßŸäÿ≥ÿ®ŸàŸÉ ÿ®ÿßÿ¥ ŸÜÿπÿßŸàÿØŸáÿß ŸÅŸä ÿßŸÑpost Ÿáÿ∞ÿß..\nÿßÿ≠ŸÑŸÖŸàÿß.. ÿßÿÆÿØŸÖŸàÿß.. ÿ™ÿπŸÑŸÖŸàÿß.. ŸÅŸäÿØŸàÿß ÿ∫Ÿäÿ±ŸÉŸÖ.. ÿßÿ±ÿ≥ŸÖŸàÿß ŸÖÿ≥ÿ™ŸÇÿ®ŸÑŸÉŸÖ ÿ®ŸäÿØŸäŸÉŸÖ.. ŸàÿßŸÑÿßŸáŸÖ ÿÆŸÑŸäŸÉŸÖ ŸÖÿπ ÿßŸÑŸÜÿßÿ≥ ÿßŸÑŸÑŸä ÿßÿ∑ŸÑÿπŸÑŸÉ ÿßŸÑŸÖŸàÿ±ÿßŸÑ ŸÖŸàÿ¥ ÿßŸÑŸÜÿßÿ≥ ÿßŸÑŸáÿØÿßŸÖÿ©.. ÿ≠Ÿäÿßÿ™ŸÉ ÿ™ÿπŸäÿ¥Ÿáÿß ŸÖÿ±ÿ© Ÿàÿ≠ÿØÿ©.. ÿßÿ∂ÿ±ÿ® ÿπŸÑŸäŸáÿß ÿ®ÿßŸÑÿ®ŸàŸÜŸäÿ© ŸàÿµŸàÿ± ŸÅŸäŸáÿß ÿßŸÑÿ®ÿ≥ŸÖÿ© ÿ®ÿßŸäÿØŸÉ.. ÿ®ŸäÿØŸÉ ÿ™ÿ∂ŸàŸä ÿ´ŸÜŸäÿ™ŸÉ..\nÿ±ÿ®Ÿä ŸÖÿπÿßŸÉŸÖ Ÿàÿ®ÿßŸÑÿ™ŸàŸÅŸäŸÇ ÿ¥ÿßÿ° ÿßŸÑŸÑŸá..\n","permalink":"https://blog.nebrass.fr/le-petit-guide-pour-d%C3%A9crocher-un-stage-bel-tounsi/","summary":"ÿßŸÑŸäŸàŸÖ ÿ®ÿßÿ¥ ŸÜŸÉÿ™ÿ® post ŸÑÿßŸàŸÑ ŸÖÿ±ÿ© ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ© ÿ®ÿßÿ¥ ÿßŸÑŸÉŸÑÿßŸÖ ÿßŸÑŸÉŸÑ ŸäŸÉŸàŸÜ amical ŸàŸÖŸàÿ¥ formel.. üòÅ\nÿßŸÑstage ŸàŸÖÿß ÿ£ÿØÿ±ÿßŸÉ ŸÖŸÜ ÿßŸÑstage ŸÖŸÜ ÿ£ŸÉÿ®ÿ± ÿßŸÑŸÖÿπÿ∂ŸÑÿßÿ™ ÿßŸÑŸÑŸä ÿ™ŸÇÿßÿ®ŸÑ ÿ£Ÿä ÿ∑ÿßŸÑÿ® ŸÖŸáŸÖÿß ŸÉÿßŸÜ ÿßÿÆÿ™ÿµÿßÿµŸà ŸàÿßŸÑÿß diplome ŸÖÿ™ÿßÿπŸà.. ŸÇÿ®ŸÑ ŸÉÿßŸÜ ŸÖÿßÿ¥Ÿä ŸÅŸä ÿ®ÿßŸÑŸä ÿßŸÑŸÑŸä ŸÉÿßŸÜ ÿßŸÑLMD ÿ™ÿßÿπÿ®ŸäŸÜ ÿ®ÿßÿ¥ ŸäŸÑŸÇŸà stage.. ÿ£ŸÖÿß ŸÉŸäŸÅ ÿØÿÆŸÑÿ™ ŸÑŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸÖŸáŸÜŸäÿ©ÿå ÿπÿ±ŸÅÿ™ ÿßŸÑŸÑŸä ÿ≠ÿ™Ÿâ ÿ∑ŸÑÿ®ÿ© ÿßŸÑŸáŸÜÿØÿ≥ÿ© ÿ®ÿßÿ¥ ŸäÿØÿ®ÿ±Ÿà stage ÿ≠ÿßŸÑÿ™ŸáŸÖ ŸÖÿ™ÿπÿ®ÿ© ÿ£ÿµŸÑ..\nÿßŸÑÿ≠ÿßŸÑÿ© ÿßŸÑÿ™ÿπŸäÿ≥ÿ© Ÿáÿßÿ∞Ÿä ÿßŸÑÿ≥ÿ®ÿ® ŸÅŸäŸáÿß ŸáŸà ŸÜŸÅÿ≥ ÿ≥ÿ®ÿ® ÿ™ÿØŸÜŸä ÿßŸÑŸÖŸáŸÜ ŸÅŸä ÿ™ŸàŸÜÿ≥ ŸàŸáŸà ÿ≥ÿ®ÿ® ŸÖÿπÿ±ŸàŸÅ: ÿßŸÑŸÖÿ≠ÿßÿ®ÿßÿ© ŸàŸáŸà ŸÖÿßŸäÿπÿ±ŸÅ ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© \u0026ldquo;ÿ®ÿßŸÑÿ£ŸÉÿ™ÿßŸÅ\u0026rdquo;.","title":"Le petit guide pour d√©crocher un stage bel Tounsi"},{"content":"I attended the Microsoft OpenHack Kafka from the 3rd to the 5th of June in London. I had some free time to do a small visit to the City.¬†As always, my Lumix G7 was travelling with me üòÜ\nThe first day, when I just arrived, I wanted to visit the Big Ben and the Palace of Westminster. Unfortunately, the Big Ben is under maintenance.¬†I couldn\u0026rsquo;t see it üò≠üò≠ But, I could the chance to shoot the üá∫üá∏ US Marine One ! ü§©ü§©\n When, I crossed the Westminster Bridge in the direction of the London Eye, I saw some helicopters coming from the North East in the direction of¬†the Buckingham Palace.\nI am a fan of Aircrafts \u0026amp; Helicopters, I tried to take some shots of the approaching Helicopters. The big surprise was when I looked into the Viewfinder üòÜüòÜüòÜ The helicopter heading the group was the \u0026ldquo;Marine One Sikorsky VH-3D\u0026rdquo; !! Yeah the üá∫üá∏üá∫üá∏ US Presidential helicopter which was escorted by a military and police helicopters.\n I thought it was just a film shooting or something else, but some minutes ago, I heard people talking about the visit of the US president Trump to London, which shows this Daily Mail youtube video:\n  This is was the first time that I take a picture of something very original üòÅ I thing that we didn\u0026rsquo;t meet the üá∫üá∏ US Marine One üá∫üá∏ everyday ! ü§™\nLike every trip, here is a small Flickr album:\nhttps://www.flickr.com/photos/nebrass78/albums/72157708950824296\n","permalink":"https://blog.nebrass.fr/london-trip-june-2019/","summary":"I attended the Microsoft OpenHack Kafka from the 3rd to the 5th of June in London. I had some free time to do a small visit to the City.¬†As always, my Lumix G7 was travelling with me üòÜ\nThe first day, when I just arrived, I wanted to visit the Big Ben and the Palace of Westminster. Unfortunately, the Big Ben is under maintenance.¬†I couldn\u0026rsquo;t see it üò≠üò≠ But, I could the chance to shoot the üá∫üá∏ US Marine One !","title":"London Trip - June 2019"},{"content":"To attend the Groundbreaker Awards ceremony during the Oracle Code NY on May 21st, I was in a very great trip to New York City.\nI got the CityPass New York that led me to visit New York\u0026rsquo;s top 6 attractions\n Empire State Building American Museum of Natural History The Metropolitan Museum of Art Top of the Rock Observation Deck Ferry Access to Statue of Liberty and Ellis Island Intrepid Sea, Air \u0026amp; Space Museum  I also visited:\n Battery Park Financial District Central Park  I am sure that you are asking about the Times Square üòÜüòÜ I was living there in the The Pearl Hotel, which was great ü•∞\nI also did a fabulous Helicopter flight all over many great monuments and buildings:\n  Here are some photos on Flickr that I took using my new Panasonic Lumix G7 üòçü•∞\nhttps://www.flickr.com/photos/nebrass78/albums/72157708818874501\n","permalink":"https://blog.nebrass.fr/nyc-trip-may-2019/","summary":"To attend the Groundbreaker Awards ceremony during the Oracle Code NY on May 21st, I was in a very great trip to New York City.\nI got the CityPass New York that led me to visit New York\u0026rsquo;s top 6 attractions\n Empire State Building American Museum of Natural History The Metropolitan Museum of Art Top of the Rock Observation Deck Ferry Access to Statue of Liberty and Ellis Island Intrepid Sea, Air \u0026amp; Space Museum  I also visited:","title":"NYC Trip - May 2019"},{"content":"To celebrate the Groundbreaker Award and Oracle Code New York, my book \u0026ldquo;Playing with Java Microservices on Kubernetes and OpenShift\u0026rdquo; will be FREE forever !\nThis is a gift is for the community that voted for me and helped me to achieve this great award ! ü§©\nThe book is available on the Leanpub website:\n Thank you so much for voting for me !! ü•≥\n","permalink":"https://blog.nebrass.fr/big-up-my-book-is-free-now/","summary":"To celebrate the Groundbreaker Award and Oracle Code New York, my book \u0026ldquo;Playing with Java Microservices on Kubernetes and OpenShift\u0026rdquo; will be FREE forever !\nThis is a gift is for the community that voted for me and helped me to achieve this great award ! ü§©\nThe book is available on the Leanpub website:\n Thank you so much for voting for me !! ü•≥","title":"Big up! My book is FREE now !!"},{"content":"I\u0026rsquo;m thrilled‚ö° to announce that I\u0026rsquo;m one of the four winners of the Oracle Groundbreaker Awards for this year ü§©\nhttps://twitter.com/OracleDevs/status/1129159643148611584\nThis is one of the happiest moments of my life ü•∞\nhttps://twitter.com/OracleDevs/status/1129842891009007616\nHere is the video of the ceremony ü§©üòçü•∞\n  Thank you for all those who believed and voted for me !! Thank you for your support that made me win !!\n","permalink":"https://blog.nebrass.fr/officially-im-an-oracle-groundbreaker/","summary":"I\u0026rsquo;m thrilled‚ö° to announce that I\u0026rsquo;m one of the four winners of the Oracle Groundbreaker Awards for this year ü§©\nhttps://twitter.com/OracleDevs/status/1129159643148611584\nThis is one of the happiest moments of my life ü•∞\nhttps://twitter.com/OracleDevs/status/1129842891009007616\nHere is the video of the ceremony ü§©üòçü•∞\n  Thank you for all those who believed and voted for me !! Thank you for your support that made me win !!","title":"Officially: I'm an Oracle Groundbreaker ü§©ü•≥"},{"content":"I got the chance to visit Porto city, Portugal, quickly for two days ü§© yeah üòÖ for only 48 hours !\nHere some photos from this great \u0026amp; authentic city:\nhttps://www.flickr.com/photos/nebrass78/albums/72157708840233892\n","permalink":"https://blog.nebrass.fr/porto-trip-may-2019/","summary":"I got the chance to visit Porto city, Portugal, quickly for two days ü§© yeah üòÖ for only 48 hours !\nHere some photos from this great \u0026amp; authentic city:\nhttps://www.flickr.com/photos/nebrass78/albums/72157708840233892","title":"Porto Trip - May 2019"},{"content":"Some days ago, I got an idea to make a personal project about \u0026ldquo;Connected Devices\u0026rdquo;. This idea led me to discover some Artificial Intelligence and especially Machine Learning.\nThe first thing that I did: I started seeking for a good course in many websites: edx, coursera and udemy.. These are the most popular sites of elearning that I am usually referencing to when I want to attack new technology or any CS field.\nBut, and it\u0026rsquo;s a bit shameful to say, I found that the ML courses are 95% based on Python üò´ and some 4% based on R üôÑ and for Java.. NOTHING !! Yeah baby ! NOTHING !!\nThis is was very scaring for me to see such as fact. But AI Guys weren\u0026rsquo;t in love with Java üòÇ\nSo I started looking for something else: a full learning path mixing ML with a Python training or an ML track based on Java.\nTo be honnest, I didn\u0026rsquo;t find something good, with many positive reviews and recommendations. I found that many Java developers went to learning Python/Scala before starting playing with Machine Learning and AI. So, I decided to do the same.\nAfter some Googlings, I found a great website called DataCamp:\n This website was very useful ! I could start a Skill Track called Python Programming Track:\nIt\u0026rsquo;s a practical 15 Hours learning path of Python dedicated for the Data Science, composed of 4 courses:\n Introduction to Python - 4 hours Intermediate Python for Data Science - 4 hours Python Data Science Toolbox (Part 1) - 3 hours Python Data Science Toolbox (Part 2) - 4 hours  The learning process were extremely interesting and the trainers are really very professional !\nThe course is a mix of videos, learning exercices and even extra practice exercices to master the subject.\nWhen you finish each part, you get an accomplishment certificate, which looks like:\n When I finished all the track, I got a certificate of accomplishment:\n After I finished the tooling track, I started the second part of my ML learning trip: Machine Learning with Python.\nThis is a 20 hours track composed of 5 courses:\n Supervised Learning with scikit-learn - 4 hours Unsupervised Learning in Python - 4 hours Linear Classifiers in Python - 4 hours Machine Learning with the Experts: School Budgets - 4 hours Deep Learning in Python - 4 hours    When I finished all the track, I got a certificate of accomplishment:\n UPDATE 08/06/2019: You can get free 2 months on Datacamp from Microsoft. You need just to have an Outlook/Hotmail/Live account and then go to this page:\n Next, click on \u0026ldquo;Join or access now\u0026rdquo; to access the benefits page, and there you will find the link of the free 2months access to Datacamp, and even a 1 month free access to Pluralsight:\nEnjoy !!\nFinal words I have tested many learning platforms since I started studying CS on 2009, and I can say really this is one of the most wonderful websites that can catch you to a great learning path without any pain and any complications: smooth design, great trainers, very up-to-dated content, practice exercices and projects, and even more !!\n","permalink":"https://blog.nebrass.fr/discovering-machine-learning/","summary":"Some days ago, I got an idea to make a personal project about \u0026ldquo;Connected Devices\u0026rdquo;. This idea led me to discover some Artificial Intelligence and especially Machine Learning.\nThe first thing that I did: I started seeking for a good course in many websites: edx, coursera and udemy.. These are the most popular sites of elearning that I am usually referencing to when I want to attack new technology or any CS field.","title":"Discovering Machine Learning ?!"},{"content":"Thursday March 28th, 2019,¬†night, I was flying from Paris to Tunis.¬†When I landed and the network came back, I got a mysterious Twitter notification: I am tagged in a tweet published by @OracleDevs :\n I was extremely happy in a way that I thought maybe I was dreaming. The plane landed nearly 00:30, I was tired, so I was convinced maybe a DayDream, or Early Night Dream, as it was midnight !\nI asked my friend Achref Jouida, who was travelling with me, to read the tweet and to explain it to me üòÜ I was extremely happy ! This is a moment I was waiting since I started my carrer ! Attending the finals in an international competition program of a company such as Oracle !\nCool ! But ..\nwhat is the Oracle Groundbreaker awards? The Groundbreaker Awards recognise the outstanding work of talented developers everywhere; those who break boundaries and build new realities every day. All Groundbreaker Award winners will have the opportunity to share their story on stage at Oracle Code in New York on May 21, 2019.\nHow did I entered into the competition ? This kind of competitions are based on nominations. Any Oracle community member can nominate the developers whose work has had the most impact on the community and craft.\nNomination were opened between February 25, 2019 and March 22, 2019.\nWho are the finalists for the 2019 Edition ? In this year, there are many big great famous names, such as :\n  Dr Venkat Subramaniam: My favourite Java speaker\n  Kent Beck: The father of extreme programming\n  Marissa Mayer: CEO at Yahoo and VP, Search/UX, at Google\n  Mitchell Baker: Executive Chairwoman of the Mozilla Foundation\n  How it¬†works? How CAN I Win ? OracleDevs Teams select the profiles and filter the nominations, and proceeded to announce the finalists list on March 28th. Then, the community members and developers proceed for voting for their favourite candidates.\nThe voting link: https://developer.oracle.com/gbvoting\n When clicking on the candidate that you like (I hope it will be me üòÅ), a Twitter new post window will be open to share a pre-filled text:\n I\u0026rsquo;m casting my vote for Nebrass Lamouchi @NebrassLamouchi to win a #GroundbreakerAward, presented by @OracleDevs: https://developer.oracle.com/newfaceofgreatness\n Voting will be closed on April 19th. So harry up ! Time is going on ! üòÅ\nFinal words üòá Being finalist in the Groundbreaker Awards was one of the greatest moments of my life ! Being in competition with people such as Dr Venkat, Marissa Mayer and Mitchell Baker is a huge honor for me ! I am extremely happy to get such as chance.. to be in the same challenge with such as great professionals.\n","permalink":"https://blog.nebrass.fr/im-a-finalist-in-the-oracle-groundbreaker-awards/","summary":"Thursday March 28th, 2019,¬†night, I was flying from Paris to Tunis.¬†When I landed and the network came back, I got a mysterious Twitter notification: I am tagged in a tweet published by @OracleDevs :\n I was extremely happy in a way that I thought maybe I was dreaming. The plane landed nearly 00:30, I was tired, so I was convinced maybe a DayDream, or Early Night Dream, as it was midnight !","title":"I'm a finalist in the Oracle Groundbreaker Awards !"},{"content":"What is Istio? Google¬†presents¬†Istio¬†as an open platform to connect, monitor, and secure microservices.\nIstio¬†is a service mesh implementation that provides many cloud-native capabilities like:\n Traffic management: Service Discovery, Load balancing, Failure recovery, A/B testing, Canary releases, etc‚Ä¶ Observability: Request Tracing, Metrics, Monitoring, Auditing, Logging, etc‚Ä¶ Security: ACLs, Access control, Rate limiting, End-to-end authentication, etc‚Ä¶  Istio¬†delivers all these great features without any changes to the code of the microservices running with it on the same¬†Kubernetes¬†cluster.\nIn our case, we already implemented many of these features and capabilities when we were writing our microservices. If we had¬†Istio¬†at the beginning, we could save so much effort and time, by delegating all of these capabilities to¬†Istio.\nIstio Architecture Istio¬†service mesh is composed of two parts:\n The¬†data plane¬†is responsible for establishing, securing, and controlling the traffic through the Service Mesh. The management components that instruct the data plane how to behave is known as the \u0026ldquo;control plane\u0026rdquo;. The¬†control plane¬†is the brains of the mesh and exposes an API for operators to manipulate the network behaviors.   Istio Components From the¬†Istio Architecture¬†diagram, we can see different components, located in different areas of the ecosystem:\nEnvoy Sidecar proxies per microservice to handle ingress/egress traffic between services in the cluster and from a service to external services. The proxies form a¬†secure microservice mesh¬†providing a rich set of functions like discovery, rich layer-7 routing, circuit breakers, policy enforcement and telemetry recording/reporting functions.\n The service mesh is not an overlay network. It simplifies and enhances how microservices in an application talk to each other over the network provided by the underlying platform.\n Envoy is deployed as a sidecar to the relevant microservice in the same Kubernetes pod. This deployment allows Istio to extract a wealth of signals about traffic behavior as attributes. Istio can, in turn, use these attributes in Mixer to enforce policy decisions, and send them to monitoring systems to provide information about the behavior of the entire mesh.\nMixer Mixer is a central component that is leveraged by the proxies and microservices to enforce policies such as authorization, rate limits, quotas, authentication, request tracing and telemetry collection.\nMixer includes a flexible plugin model. This model enables Istio to interface with a variety of host environments and infrastructure backends. Thus, Istio abstracts the Envoy proxy and Istio-managed services from these details.\nPilot Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.).\nPilot converts high level routing rules that control traffic behavior into Envoy-specific configurations, and propagates them to the sidecars at runtime. Pilot abstracts platform-specific service discovery mechanisms and synthesizes them into a standard format that any sidecar conforming with the Envoy data plane APIs can consume. This loose coupling allows Istio to run on multiple environments such as Kubernetes, Consul, or Nomad, while maintaining the same operator interface for traffic management.\nCitadel Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management. You can use Citadel to upgrade unencrypted traffic in the service mesh. Using Citadel, operators can enforce policies based on service identity rather than on network controls. Starting from release 0.5, you can use Istio‚Äôs authorization feature to control who can access your services.\nGalley Galley validates user authored Istio API configuration on behalf of the other Istio control plane components. Over time, Galley will take over responsibility as the top-level configuration ingestion, processing and distribution component of Istio. It will be responsible for insulating the rest of the Istio components from the details of obtaining user configuration from the underlying platform (e.g. Kubernetes).\nGetting started with Istio Requirements We will be playing with¬†Istio¬†on¬†Kubernetes. For testing the solution, you need to have, as usual, a running¬†Minikube¬†on your machine.\nThe example of this chapter will be running on¬†Minikube¬†v0.35.0¬†with a custom config:¬†4 CPUs with 8Go of Memory.\nGet \u0026amp; Install Istio To start downloading and installing Istio, just enter the following command:\n1  $ curl -L https://git.io/getLatestIstio | sh -   By the end of the command execution, you will see some message like this one:\n1 2 3  Add /Users/n.lamouchi/istio-1.0.6/bin to your path; e.g copy paste in your shell and/or ~/.profile: $ export PATH=\u0026#34;$PATH:/Users/n.lamouchi/istio-1.0.6/bin\u0026#34;   Next, we will move to Istio package directory:\n1  $ cd istio-1.0.6/   As the first step, you have to install Istio‚Äôs Custom Resources Definition:\n1  $ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml    What is Custom Resources Definition ?\nCustom resources definition (CRD) is a powerful feature introduced in Kubernetes¬†1.7 which enables users to add their own/custom objects to the¬†Kubernetes¬†cluster and use it like any other native¬†Kubernetes objects.\n Next, we need to install Istio‚Äôs core components. We have four different options to do this:\n Option 1: Install Istio WITHOUT mutual TLS authentication between sidecars Option 2: Install Istio WITH default mutual TLS authentication Option 3: Render Kubernetes manifest with Helm and deploy with kubectl Option 4: Use Helm and Tiller to manage the Istio deployment   For a production setup of Istio, it‚Äôs recommended to install with the Helm Chart (Option 4), to use all the configuration options. This permits customization of Istio to operator specific requirements.\n In this tutorial, we will be using the¬†Option 1:\nTo install Istio:\n1  $ kubectl apply -f install/kubernetes/istio-demo.yaml   Verifying the installation\nTo be sure that the Istio components were correctly installed, these Kubernetes Services needs to be installed:¬†istio-pilot,¬†istio-ingressgateway,¬†istio-policy,¬†istio-telemetry,¬†prometheus,¬†istio-galley, and (optionally)¬†istio-sidecar-injector:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.102.186.225 \u0026lt;none\u0026gt; 3000/TCP 35m istio-citadel ClusterIP 10.108.239.218 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 58m istio-egressgateway ClusterIP 10.103.151.29 \u0026lt;none\u0026gt; 80/TCP,443/TCP 58m istio-galley ClusterIP 10.96.55.14 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 58m istio-ingressgateway LoadBalancer 10.110.91.248 \u0026lt;pending\u0026gt; 80:31380/TCP.. 58m istio-pilot ClusterIP 10.104.95.143 \u0026lt;none\u0026gt; 15010/TCP.. 58m istio-policy ClusterIP 10.100.19.140 \u0026lt;none\u0026gt; 9091/TCP.. 58m istio-sidecar-injector ClusterIP 10.101.13.203 \u0026lt;none\u0026gt; 443/TCP 58m istio-telemetry ClusterIP 10.103.135.98 \u0026lt;none\u0026gt; 9091/TCP.. 58m jaeger-agent ClusterIP None \u0026lt;none\u0026gt; 5775/UDP.. 35m jaeger-collector ClusterIP 10.110.82.2 \u0026lt;none\u0026gt; 14267/TCP,14268/TCP 35m jaeger-query ClusterIP 10.101.54.162 \u0026lt;none\u0026gt; 16686/TCP 35m prometheus ClusterIP 10.101.210.170 \u0026lt;none\u0026gt; 9090/TCP 58m servicegraph ClusterIP 10.99.60.12 \u0026lt;none\u0026gt; 8088/TCP 35m tracing ClusterIP 10.98.62.125 \u0026lt;none\u0026gt; 80/TCP 35m zipkin ClusterIP 10.100.54.120 \u0026lt;none\u0026gt; 9411/TCP 35m   For the Kubernetes¬†Services¬†already listed, we will find corresponding¬†Pods:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ kubectl get svc -n istio-system NAME READY STATUS RESTARTS AGE grafana-59b8896965-n4rvr 1/1 Running 0 91m istio-citadel-6f444d9999-xc27q 1/1 Running 0 91m istio-cleanup-secrets-k2dzg 0/1 Completed 0 91m istio-egressgateway-6d79447874-8twkk 1/1 Running 0 91m istio-galley-685bb48846-tht5x 1/1 Running 0 91m istio-grafana-post-install-rmflr 0/1 Completed 0 91m istio-ingressgateway-5b64fffc9f-56tm2 1/1 Running 0 91m istio-pilot-7f558fc848-2fscl 2/2 Running 0 91m istio-policy-547d64b8d7-skpzm 2/2 Running 0 91m istio-security-post-install-nlfmd 0/1 Completed 0 91m istio-sidecar-injector-5d8dd9448d-rdbq8 1/1 Running 0 91m istio-telemetry-c5488fc49-b8sv8 2/2 Running 0 91m istio-tracing-6b994895fd-6wxvx 1/1 Running 0 91m prometheus-76b7745b64-2tgkw 1/1 Running 0 91m servicegraph-cb9b94c-2x5cb 1/1 Running 1 91m   All¬†Pods¬†need to be in the¬†Running¬†status, except the¬†istio-cleanup-secrets-*,¬†istio-grafana-post-install-*¬†and¬†istio-security-post-install-*¬†Pods, which will be in the¬†Completed¬†status. These three¬†Completed¬†Pods¬†are started and executed at a post-installation phase, to do post-installation tasks like cleaning the installation secrets, etc‚Ä¶\nEnvoy Sidecar Injection In the service mesh world, a sidecar is a utility container in the pod, and its purpose is to support the main container. In the Istio case, the sidecar will be an Envoy proxy that will be deployed to each pod. The process of adding Envoy into a pod is called¬†Sidecar Injection. This action can be done in two ways:\n Automatically using the Istio Sidecar Injector : Automatic injection injects at pod creation time. The controller resource is unmodified. Sidecars can be updated selectively by manually deleting a pods or systematically with a deployment rolling update. Manually using the¬†Istioctl-CLI¬†tool: Manual injection modifies the controller configuration, e.g. deployment. It does this by modifying the pod template spec such that all pods for that deployment are created with the injected sidecar. Adding, Updating or Removing the sidecar requires modifying the entire deployment.  Automatic Sidecar Injection To enable the¬†Automatic Sidecar Inject¬†just add the¬†istio-injection¬†label to the Kubernetes namespace: For example to enable it in the¬†default¬†namespace:\n1  $ kubectl label namespace default istio-injection=enabled --overwrite   Now, when a pod will be created, the Envoy sidecar is automatically injected inside it.\nManual Sidecar Injection Inject the sidecar into the deployment using the in-cluster configuration. \u0026lt;1\u0026gt;\n1 2 3 4  $ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml \\  --output bookinfo-injected.yaml $ kubectl apply -f bookinfo-injected.yaml    BookInfo Sample Application\nThis example deploys a sample application composed of four separate microservices used to demonstrate various Istio features. The lication displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a cription of the book, book details (ISBN, number of pages, and so on), and a few book reviews.\nThe Bookinfo application is broken into four separate microservices:\n productpage: The productpage microservice calls the details and reviews microservices to populate the page. details: The details microservice contains book information. reviews: The reviews microservice contains book reviews. It also calls the ratings microservice. ratings: The ratings microservice contains book ranking information that accompanies a book review.  There are 3 versions of the reviews microservice:\n Version v1 doesn‚Äôt call the ratings service. Version v2 calls the ratings service, and displays each rating as 1 to 5 black stars. Version v3 calls the ratings service, and displays each rating as 1 to 5 red stars.   These two commands can be done in one command:\n1  $ kubectl create -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)   or also:\n1  $ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml | kubectl apply -f -   These commands will inject the Istio Envoy sidecar into the Kubernetes¬†Deployment¬†object.\nFor the sample¬†Deployment¬†of the¬†details-v1¬†microservice which looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145  apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:details-v1spec:replicas:1template:metadata:labels:app:detailsversion:v1spec:containers:- name:detailsimage:istio/examples-bookinfo-details-v1:1.8.0imagePullPolicy:IfNotPresentports:- containerPort:9080The¬†_Deployment_¬†after the injection of Istio will look like:apiVersion:extensions/v1beta1kind:Deploymentmetadata:creationTimestamp:nullname:details-v1spec:replicas:1strategy:{}template:metadata:annotations:sidecar.istio.io/status:\u0026#39;...\u0026#39;creationTimestamp:nulllabels:app:detailsversion:v1spec:containers:- name:detailsimage:istio/examples-bookinfo-details-v1:1.8.0imagePullPolicy:IfNotPresentports:- containerPort:9080resources:{}- args:- proxy- sidecar- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- details- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istio-pilot.istio-system:15007- --discoveryRefreshDelay- 1s- --zipkinAddress- zipkin.istio-system:9411- --connectTimeout- 10s- --proxyAdminPort- \u0026#34;15000\u0026#34;- --controlPlaneAuthPolicy- NONEenv:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace- name:INSTANCE_IPvalueFrom:fieldRef:fieldPath:status.podIP- name:ISTIO_META_POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:ISTIO_META_INTERCEPTION_MODEvalue:REDIRECT- name:ISTIO_METAJSON_LABELSvalue:|{\u0026#34;app\u0026#34;:\u0026#34;details\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;}image:docker.io/istio/proxyv2:1.0.6imagePullPolicy:IfNotPresentname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPresources:requests:cpu:10msecurityContext:readOnlyRootFilesystem:truerunAsUser:1337volumeMounts:- mountPath:/etc/istio/proxyname:istio-envoy- mountPath:/etc/certs/name:istio-certsreadOnly:trueinitContainers:- args:- -p- \u0026#34;15001\u0026#34;- -u- \u0026#34;1337\u0026#34;- -m- REDIRECT- -i- \u0026#39;*\u0026#39;- -x- \u0026#34;\u0026#34;- -b- \u0026#34;9080\u0026#34;- -d- \u0026#34;\u0026#34;image:docker.io/istio/proxy_init:1.0.6imagePullPolicy:IfNotPresentname:istio-initresources:{}securityContext:capabilities:add:- NET_ADMINprivileged:truevolumes:- emptyDir:medium:Memoryname:istio-envoy- name:istio-certssecret:optional:truesecretName:istio.defaultstatus:{}  All the extra paramters and configuration are added via¬†istioctl kube-inject¬†command.\nAfter executing the command:\n1 2 3 4 5 6 7 8 9 10 11 12  $ kubectl create -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) service/details created deployment.extensions/details-v1 created service/ratings created deployment.extensions/ratings-v1 created service/reviews created deployment.extensions/reviews-v1 created deployment.extensions/reviews-v2 created deployment.extensions/reviews-v3 created service/productpage created deployment.extensions/productpage-v1 created   We can verify that the¬†sidecar¬†is deployed in the same¬†Deployment¬†as the microservice, just type:\n1 2 3 4  $ kubectl get deployment details-v1 -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR details-v1 1/1 1 1 63m details,istio-proxy ... ...   We can even see that there are two containers in the¬†details-v1-*¬†pod:\n To be sure that everything is ok, we need to verify that the¬†BookInfo¬†services \u0026amp; pods are here:\n1 2 3 4 5 6 7 8  $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.106.82.61 \u0026lt;none\u0026gt; 9080/TCP 4h51m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 11h productpage ClusterIP 10.100.126.93 \u0026lt;none\u0026gt; 9080/TCP 4h51m ratings ClusterIP 10.98.201.254 \u0026lt;none\u0026gt; 9080/TCP 4h51m reviews ClusterIP 10.110.241.59 \u0026lt;none\u0026gt; 9080/TCP 4h51m   And :\n1 2 3 4 5 6 7 8 9  $ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-55bc45969c-99xj2 2/2 Running 0 4h52m productpage-v1-5b597ff459-pfpmt 2/2 Running 0 4h52m ratings-v1-7877895db5-vchw2 2/2 Running 0 4h52m reviews-v1-699587b49b-bt7z5 2/2 Running 0 4h52m reviews-v2-cc7cd59cc-k6l6j 2/2 Running 0 4h52m reviews-v3-6fbcf56df8-nc2lf 2/2 Running 0 4h52m   Now, we can go on to next steps and enjoy the great Istio features :)\nTraffic Management Istio Gateway \u0026amp; VirtualService Now that the Bookinfo services are up and running, we need to make the Services accessible from outside of your Kubernetes cluster. An¬†Istio Gateway¬†object is used for this purpose.\nAn¬†Istio Gateway¬†configures a load balancer for¬†HTTP/TCP¬†traffic at the edge of the service mesh and enables¬†Ingress¬†traffic for an application. Unlike Kubernetes¬†Ingress,¬†Istio Gateway¬†only configures the¬†L4-L6¬†functions (for example, ports to expose, TLS configuration). Users can then use standard Istio rules to control¬†HTTP¬†requests as well as¬†TCP¬†traffic entering a¬†Gateway¬†by binding a¬†VirtualService¬†to it.\nWe can define the¬†Ingress gateway¬†for the¬†Bookinfo¬†application using the sample gateway configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# use istio default controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;  A¬†VirtualService¬†defines the rules that control how requests for a service are routed within an Istio service mesh. For example, a¬†VirtualService¬†could route requests to different versions of a service or to a completely different service than was requested. Requests can be routed based on the request source and destination,¬†HTTP¬†paths and header fields, and weights associated with individual service versions.\nThe¬†VirtualService¬†configuration looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \u0026#34;*\u0026#34;gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- uri:exact:/logout- uri:prefix:/api/v1/productsroute:- destination:host:productpageport:number:9080  Let‚Äôs create the¬†Istio Gateway¬†and the¬†VirtualService:\n1  $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml   Confirm the gateway has been created:\n1 2 3 4  $ kubectl get gateway NAME AGE bookinfo-gateway 59m   Let‚Äôs export the¬†INGRESS_HOST, the¬†INGRESS_PORT¬†and the¬†GATEWAY_URL:\n1 2 3 4 5 6  $ export INGRESS_HOST=$(minikube ip) $ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway \\  -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39;) $ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT   To test the Gateway:\n1  $ curl -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; http://$GATEWAY_URL/productpage   You must have¬†200¬†as response code.\nYou can also point your browser to¬†http://$GATEWAY_URL/productpage¬†to view the Bookinfo web page. If you refresh the page several times, you should see different versions of reviews shown in¬†productpage, presented in a round robin style (red stars, black stars, no stars), since we haven‚Äôt yet used Istio to control the version routing.\n If we refresh the web page some times, we will get different versions of the¬†reviews¬†structure: One version has red stars (the one that we got in the screenshot), an other one that have back stars and a third one without starts. This is due to the availability of three versions of the¬†reviews¬†microservice deployed in our sample¬†BookInfo¬†application.\nYou can verify the availability of the three versions of the¬†reviews¬†microservice:\n1 2 3 4 5 6  $ kubectl get pods -l app=reviews NAME READY STATUS RESTARTS AGE reviews-v1-699587b49b-v44wr 2/2 Running 0 5h31m reviews-v2-cc7cd59cc-rgc86 2/2 Running 0 5h31m reviews-v3-6fbcf56df8-wndtz 2/2 Running 0 5h31m   We got different versions of¬†reviews¬†structure while refreshing because Istio, by default, dispatches access to loadbalanced services using a¬†Round Robin¬†scheduling. One of the great features of Istio is to route the traffic to some dedicated version of a service, or even using sliced dispatching of requests.\nIn the next steps, we will see how to route the traffic based on version.\nDestination Rules Before we can use Istio to control the¬†Bookinfo¬†version routing, we need to define the available versions of an application, called¬†subsets. These¬†subsets¬†are defined in an Istio object called¬†DestinationRule/The choice of version to display can be decided based on criteria (headers, URL, etc‚Ä¶) defined to each version. We can enjoy this flexibility of criterias to do Blue-green Deployments, A/B Testing, and Canary Releases.\n1 2 3 4 5 6 7 8 9 10  apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:productpagespec:host:productpage \u0026lt;1\u0026gt;subsets:- name:v1 \u0026lt;2\u0026gt;labels:version:v1 \u0026lt;2\u0026gt;   The Kubernetes Service name on which we will be routing the traffic The¬†subset¬†element name The¬†subset¬†element version  We will create the default destination rules for the Bookinfo services, using the sample¬†destination-rule-all.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:productpagespec:host:productpagesubsets:- name:v1labels:version:v1---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:reviewsspec:host:reviewssubsets:- name:v1labels:version:v1- name:v2labels:version:v2- name:v3labels:version:v3---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:ratingsspec:host:ratingssubsets:- name:v1labels:version:v1- name:v2labels:version:v2- name:v2-mysqllabels:version:v2-mysql- name:v2-mysql-vmlabels:version:v2-mysql-vm---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:detailsspec:host:detailssubsets:- name:v1labels:version:v1- name:v2labels:version:v2---  Run the following command to create default Destination Rules:\n1  $ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml   Wait a few seconds for the destination rules to propagate.\nYou can verify that the destination rules are correctly created, using the following command:\n1  $ kubectl get destinationrules -o yaml   Now, we will change the default round-robin behavior for traffic routing: we will route all traffic to¬†reviews-v1, using a new¬†VirtualServices¬†configuration that looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12  apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviewsspec:hosts:- reviewshttp:- route:- destination:host:reviewssubset:v1  The¬†BookInfo¬†sample brings a¬†virtual-service-all-v1.yaml¬†that holds the necessary configuration of the new¬†VirtualServices. To deploy it, just type:\n1  $ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml   You can verify that the¬†review¬†VirtualService¬†is correctly created, using the following command:\n1  $ kubectl get virtualservices reviews -o yaml   Try now to reload the page multiple times, and note how only¬†reviews:v1¬†is displayed each time:\n Next, we will change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service¬†reviews:v2.\nNext, we will be routing traffic for a specific user to a specific service version. In our example, we will be routing all the traffic for a user named¬†jason¬†to the service¬†reviews:v2.\nThe¬†virtual-service-reviews-test-v2.yaml¬†configuration covers this case:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  spec:hosts:- reviewshttp:- match:- headers:end-user:exact:jasonroute:- destination:host:reviewssubset:v2- route:- destination:host:reviewssubset:v1  To deploy it, just type:\n1  $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml   You can verify that the¬†review¬†VirtualService¬†is correctly created, using the following command:\n1  $ kubectl get virtualservices reviews -o yaml   To test the new¬†VirtualService, just click on the¬†Sign¬†button and login using¬†jason¬†as username and any value as password. Now, we will see¬†reviews:v2. If you logout, we will get back the¬†reviews:v1:\n Next, we will see how to gradually migrate traffic from one version of a microservice to another one. In our example, we will send 50% of traffic to¬†reviews:v1¬†and 50% to¬†reviews:v3:\n1  $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml   Let‚Äôs verify the content of the new¬†VirtualService:\n1  $ kubectl get virtualservice reviews -o yaml   The¬†subset¬†is set to¬†50%¬†of traffic to the¬†v1¬†and¬†50%¬†to the¬†v3:\n1 2 3 4 5 6 7 8 9 10 11 12 13  spec:hosts:- reviewshttp:- route:- destination:host:reviewssubset:v1weight:50- destination:host:reviewssubset:v3weight:50  Try now to reload the page multiple times, you will see diversely¬†reviews:v1¬†and¬†reviews:v3.\nObservability Distributed Tracing Hello Jaeger To access the¬†Jaeger Dashboard, establish port forwarding from local port¬†16686¬†to the¬†Tracing¬†instance:\n1 2 3  $ kubectl port-forward -n istio-system \\  $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\  16686:16686   In your browser, go to¬†http://127.0.0.1:16686\nFrom the¬†Services¬†menu, select¬†productpage¬†service.\nScroll to the bottom and click on¬†Find Traces¬†button to see traces:\n If you click on a trace, you should see more details. The page should look something like this:\n When invoking the¬†/productpage, many BookInfo services are called. These services correspond to¬†spans¬†and the page call itself corresponds to the¬†trace.\nAlthough Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Applications need to propagate the appropriate HTTP headers so that when the proxies send span information, the spans can be correlated correctly into a single trace.\nTo do this, an application needs to collect and propagate the following headers from the incoming request to any outgoing requests:\n x-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context   When you make downstream calls in your applications, make sure to include these headers.\n Trace sampling When using the Bookinfo sample application above, every time you access¬†/productpage¬†you see a corresponding trace in the Jaeger dashboard. This sampling rate (which is 100% in the BookInfo example) is suitable for a test or low traffic mesh, which is why it is used as the default for the demo installs.\nIn other configurations, Istio defaults to generating trace spans for 1 out of every 100 requests (sampling rate of of 1%).\nYou can control the trace sampling percentage in one of two ways:\nIn a running mesh, edit the¬†istio-pilot¬†deployment and change the environment variable with the following steps:\n To open your text editor with the deployment configuration file loaded, run the following command:  1  $ kubectl -n istio-system edit deploy istio-pilot   Find the¬†PILOT_TRACE_SAMPLING¬†environment variable, and change the value: to your desired percentage.  In both cases, valid values are from 0.0 to 100.0 with a precision of 0.01.\nGrafana The¬†Grafana¬†add-on in Istio is a preconfigured instance of¬†Grafana. The base image (grafana/grafana:5.0.4) has been modified to start with both a¬†Prometheus¬†data source and the¬†Istio Dashboard¬†installed. The base install files for¬†Istio, and¬†Mixer¬†in particular, ship with a default configuration of global (used for every service) metrics. The¬†Istio Dashboard¬†is built to be used in conjunction with the default Istio metrics configuration and a¬†Prometheus¬†backend.\nEstablish port forwarding from local port 3000 to the¬†Grafana¬†instance:\n1 2 3  $ kubectl -n istio-system port-forward \\  $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\  3000:3000   Browse to¬†http://localhost:3000¬†and navigate to the¬†Istio Mesh Dashboard:\n¬†Prometheus Mixer comes with a built-in¬†Prometheus¬†adapter that exposes an endpoint serving generated metric values. The¬†Prometheus¬†add-on is a¬†Prometheus¬†server that comes preconfigured to scrape Mixer endpoints to collect the exposed metrics. It provides a mechanism for persistent storage and querying of Istio metrics.\nTo access the¬†Prometheus Dashboard, establish port forwarding from local port¬†9090¬†to¬†Prometheus instance:\n1 2 3  $ kubectl -n istio-system port-forward \\  $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\  9090:9090   Browse to¬†http://localhost:9090/graph, and in the¬†\u0026ldquo;Expression\u0026rdquo;¬†input box, enter:¬†istio_request_byte_count. Click¬†Execute:\n¬†Service Graph The¬†ServiceGraph¬†service provides endpoints for generating and visualizing a graph of services within a mesh. It exposes the following endpoints:\n /force/forcegraph.html¬†As explored above, this is an interactive¬†D3.js¬†visualization. /dotviz¬†is a static¬†Graphviz¬†visualization. /dotgraph¬†provides a¬†DOT¬†serialization. /d3graph¬†provides a JSON serialization for¬†D3¬†visualization. /graph¬†provides a generic JSON serialization.  To access the¬†Service Graph Dashboard, establish port forwarding from local port¬†8088¬†to¬†Service Graph instance:\n1 2 3  $ kubectl -n istio-system port-forward \\  $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\  8088:8088   Browse to¬†http://localhost:8088/dotviz:\n¬†The¬†ServiceGraph¬†example is built on top of¬†Prometheus¬†queries and depends on the standard Istio metric configuration.\nConclusion That\u0026rsquo;s all folks! We have been presenting the main concepts and components of Istio Service Mesh. This introducing chapter is just for installation and core concepts of Istio. As we saw in the Traffic Management section, there are infinite scenarios and use cases that you want to cover, like fault injection, controlling Ingress and Egress traffic, circuit breakers, etc..\nWe did not have the opportunity to cover the Security capabilities of Istio, just because it needs so much chapters to be covered. üòõ\nStay tuned, I will be writing some quickies about more great capabilities of Istio ! üòé\n","permalink":"https://blog.nebrass.fr/playing-with-istio-service-mesh-on-kubernetes/","summary":"What is Istio? Google¬†presents¬†Istio¬†as an open platform to connect, monitor, and secure microservices.\nIstio¬†is a service mesh implementation that provides many cloud-native capabilities like:\n Traffic management: Service Discovery, Load balancing, Failure recovery, A/B testing, Canary releases, etc‚Ä¶ Observability: Request Tracing, Metrics, Monitoring, Auditing, Logging, etc‚Ä¶ Security: ACLs, Access control, Rate limiting, End-to-end authentication, etc‚Ä¶  Istio¬†delivers all these great features without any changes to the code of the microservices running with it on the same¬†Kubernetes¬†cluster.","title":"Playing with Istio Service Mesh on Kubernetes"},{"content":"I just updated my book ¬´ Playing with Java Microservices with Kubernetes and OpenShift ¬ª üòÉ\nThe book now is in General Availability.\nThank you so much for choosing my book and for the time that you gave to me !\n","permalink":"https://blog.nebrass.fr/new-book-update-#2/","summary":"I just updated my book ¬´ Playing with Java Microservices with Kubernetes and OpenShift ¬ª üòÉ\nThe book now is in General Availability.\nThank you so much for choosing my book and for the time that you gave to me !","title":"New book update #2"},{"content":"I just updated my book ¬´ Playing with Java Microservices with Kubernetes and OpenShift ¬ª üòÉ\nThe release changelog:\n Fixed some typos üôà I just finished writing the Chapter 14: Getting started with OpenShift üëå I started writing the Chapter 15: The OpenShift style üé∂ I started working on Istio Tutorial that will be the first bonus chapter in the book ‚ö°Ô∏è‚ö°Ô∏è  I will be covering more and more subjects soon üòÅ\nI am available for an on-demand chapters. So if you are interested in some specific subject about Java Microservices, just get in touch with me üòÅ\n","permalink":"https://blog.nebrass.fr/new-book-update-#1/","summary":"I just updated my book ¬´ Playing with Java Microservices with Kubernetes and OpenShift ¬ª üòÉ\nThe release changelog:\n Fixed some typos üôà I just finished writing the Chapter 14: Getting started with OpenShift üëå I started writing the Chapter 15: The OpenShift style üé∂ I started working on Istio Tutorial that will be the first bonus chapter in the book ‚ö°Ô∏è‚ö°Ô∏è  I will be covering more and more subjects soon üòÅ","title":"New book update #1"},{"content":"Tomorrow, I will give a talk for the Higher Colleges of Technology (HCT)¬†titled¬†\u0026ldquo;Introduction to the containerization security\u0026rdquo;. The talk is¬†intended for the students of Computer Sciences students. It aims to introduce the Containerization and the benefits of Containers in matters of Web Applications Security.\nThe talk is organized by Mrs Samia Kouki, a former Computing Sciences assistant professor in ISG Tunis and a current assistant professor in HCT UAE.\nThe content of my presentation:\n Introduction to Containers What is Docker? Docker Architecture How containers can improve security ?  You will find below the slides that I will be presenting:\n Many thanks for Mrs Samia Kouki for this opportunity. I hope the talk will be useful and helpful for the students attending my session.\n","permalink":"https://blog.nebrass.fr/my-talk-for-the-higher-colleges-of-technology-hct-uae/","summary":"Tomorrow, I will give a talk for the Higher Colleges of Technology (HCT)¬†titled¬†\u0026ldquo;Introduction to the containerization security\u0026rdquo;. The talk is¬†intended for the students of Computer Sciences students. It aims to introduce the Containerization and the benefits of Containers in matters of Web Applications Security.\nThe talk is organized by Mrs Samia Kouki, a former Computing Sciences assistant professor in ISG Tunis and a current assistant professor in HCT UAE.","title":"My talk for the Higher Colleges of Technology (HCT) - UAE"},{"content":"Playing with Java Microservices on Kubernetes and OpenShift is here ! Today, November 24th 2018, I released my new book Playing with Java Microservices on Kubernetes and OpenShift!! Finally, after ten months of work, tests, POCs and many edition iterations, the newborn is here !\nThe book is edited and sold on the LeanPub platform ü§©\nThe book is sold mainly in two different offers:\n The Book only The Book + one hour of training about Java Microservices and Kubernetes/OpenShift  I have some ideas about making some other packages, I will post about them soon ! Stay tuned !!\nI hope that this book satisfies all your needs about the Java Microservices in the Kubernetes and OpenShift ecosystem..\nMany other subjects will be covered.. Yeah ! LeanPub gives me the opportunity to deliver as many updates that I can write :D this is cool ! you pay once, you get everything !!\nYou can grab it from Leanpub or Amazon üòé\n","permalink":"https://blog.nebrass.fr/my-new-book-is-here/","summary":"Playing with Java Microservices on Kubernetes and OpenShift is here ! Today, November 24th 2018, I released my new book Playing with Java Microservices on Kubernetes and OpenShift!! Finally, after ten months of work, tests, POCs and many edition iterations, the newborn is here !\nThe book is edited and sold on the LeanPub platform ü§©\nThe book is sold mainly in two different offers:\n The Book only The Book + one hour of training about Java Microservices and Kubernetes/OpenShift  I have some ideas about making some other packages, I will post about them soon !","title":"My new book is here !!"},{"content":"The workshop \u0026ldquo;Playing with Java Microservices on Kubernetes\u0026rdquo; was held from 19/11 to 24/11, at the National School of Computer Sciences, in Mannouba, Tunisia.\nThe workshop was done on 18 Hours of training about Java, Spring Boot, DDD, Docker, Kubernetes, Cloud Patterns, CI/CD\u0026hellip;\nSome Github repositories used in the sessions:\n Monolith Example Microservices Examples  We got even a special guest, my techno-mate Houssem Dellai, the great Microsoft MVP, that presented, thru a 3 Hours session, Docker \u0026amp; Kubernetes and the CI/CD lifecycles with a great demo of Azure DevOps for the students of ENSI Tunisia.\n The event was sponsored by onepoint¬†the knowledge booster everywhere !!\n It was great for everyone to see the great turn out and the plan is to have other Java Workshops soon, in many schools.\nIf you missed this event, no worries, the next one will be announced here and elsewhere, planning will begin once everyone has recovered from the above event!\n","permalink":"https://blog.nebrass.fr/workshop-report-ensi-november-2018/","summary":"The workshop \u0026ldquo;Playing with Java Microservices on Kubernetes\u0026rdquo; was held from 19/11 to 24/11, at the National School of Computer Sciences, in Mannouba, Tunisia.\nThe workshop was done on 18 Hours of training about Java, Spring Boot, DDD, Docker, Kubernetes, Cloud Patterns, CI/CD\u0026hellip;\nSome Github repositories used in the sessions:\n Monolith Example Microservices Examples  We got even a special guest, my techno-mate Houssem Dellai, the great Microsoft MVP, that presented, thru a 3 Hours session, Docker \u0026amp; Kubernetes and the CI/CD lifecycles with a great demo of Azure DevOps for the students of ENSI Tunisia.","title":"Workshop Report: ENSI ‚Äì November 2018"},{"content":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), from November 19th to November 24th 2018, to animate a 15 Hours workshop, about Microservices in Java and how to deploy them as Docker Containers to Kubernetes. The event is organized by Mrs Rim Drira.\nThe workshop content:\n Part 1: The Monolithics Era Part 2: Coding the monolith Part 3: Microservices Era Part 4: Applying DDD to the code Part 5: Meeting \u0026amp; Implementing the ¬µservices concerns and patterns Part 6: Building the standalone ¬µservices Part 7: Packaging ¬µservices in containers Part 8: Falling in ‚ù§Ô∏è with container orchestrator: KUBERNETES üòç Part 9: Applying the Kubernetes Style  Many thanks for¬†Mrs Rim Drira for the invitation and the event\u0026rsquo;s organization.\n","permalink":"https://blog.nebrass.fr/playing-with-java-microservices-on-kubernetes-ensi-2018/","summary":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), from November 19th to November 24th 2018, to animate a 15 Hours workshop, about Microservices in Java and how to deploy them as Docker Containers to Kubernetes. The event is organized by Mrs Rim Drira.\nThe workshop content:\n Part 1: The Monolithics Era Part 2: Coding the monolith Part 3: Microservices Era Part 4: Applying DDD to the code Part 5: Meeting \u0026amp; Implementing the ¬µservices concerns and patterns Part 6: Building the standalone ¬µservices Part 7: Packaging ¬µservices in containers Part 8: Falling in ‚ù§Ô∏è with container orchestrator: KUBERNETES üòç Part 9: Applying the Kubernetes Style  Many thanks for¬†Mrs Rim Drira for the invitation and the event\u0026rsquo;s organization.","title":"Playing with Java Microservices on Kubernetes - ENSI 2018"},{"content":"Today, August 25th 2018 is one of the most wonderful days in my life. Today is my engagement day with my love.\n I love you, not only for what you are, but for what I am when I am with you. I love you, not only for what you have made of yourself but for what you are making of me.\u0026quot;\n\u0026ndash; Roy Croft\n ","permalink":"https://blog.nebrass.fr/engagement-day/","summary":"Today, August 25th 2018 is one of the most wonderful days in my life. Today is my engagement day with my love.\n I love you, not only for what you are, but for what I am when I am with you. I love you, not only for what you have made of yourself but for what you are making of me.\u0026quot;\n\u0026ndash; Roy Croft\n ","title":"Engagement Day"},{"content":"We organized, at Onepoint, our first Architecture Meetup, the 19th of July, in our beautiful headquarters in Trocad√©ro, Paris.\nOur first edition\u0026rsquo;s guest:\n Aymen El Amri : the founder of eralabs a consulting and training company and, a community hub. He help companies build and learn how to build modern and cloud native applications.\nHe\u0026rsquo;s written tech books like Painless Docker and founded online communities like DevOpsLinks and Kaptain.\nAymen\u0026rsquo;s talk was:¬†Architecture At The Cloud Native Era.\n From our teams in Onepoint:\nMahdi Zahraoui, Technical Leader working on Architecture \u0026amp; Big Data, gave a great talk about \u0026ldquo;Migrating from monolithics to microservices - Best practices and patterns\u0026rdquo;.\n Rached Jouida, Expert DevOps \u0026amp; OpenShift Evangelist, and me, have presented a talk about \u0026ldquo;The meeting of microservices and Kubernetes \u0026amp; OpenShift\u0026rdquo;.\n We are extremely happy because we had many many persons interested and attended the first edition. We got many great comments on Meetup.com and even on Linkedin :\n We will keep working on the next Meetup, which will be held on September. I will keep the subject and guest name secret.. Stay tuned ü§©\nBy the end of this post, I¬†would like to express my gratitude to Aymen for accepting my invitation, and for Mahdi \u0026amp; Rached for their great performance and for great talks.\n","permalink":"https://blog.nebrass.fr/meetup-report-architecture-in-the-cloud-era/","summary":"We organized, at Onepoint, our first Architecture Meetup, the 19th of July, in our beautiful headquarters in Trocad√©ro, Paris.\nOur first edition\u0026rsquo;s guest:\n Aymen El Amri : the founder of eralabs a consulting and training company and, a community hub. He help companies build and learn how to build modern and cloud native applications.\nHe\u0026rsquo;s written tech books like Painless Docker and founded online communities like DevOpsLinks and Kaptain.","title":"Meetup Report: Architecture in the Cloud Era"},{"content":"Le jeudi 19 juillet √† l\u0026rsquo;Atelier, la communaut√© Architecture exposera des REXs, des d√©mos, des workshops avec un invit√© mondialement reconnu sur le DevOps, le CloudNative et les architectures modernes : Aymen El Amri.\nNotre special guest :\nAymen El Amri - Meilleures pratiques de r√©alisation d\u0026rsquo;applications Cloud Native\nAymen El Amri a √©t√© reconnu par TechBeacon comme l\u0026rsquo;un des Top 100 leaders, praticiens, experts de DevOps √† suivre. Il est le fondateur d'eralabs, une soci√©t√© de conseil et de formation et un Community-Hub. Il aide les entreprises √† construire et apprendre √† construire des applications modernes et Cloud-Native. Il a √©crit des livres techniques tels que Painless Docker et fond√© des communaut√©s en ligne comme DevOpsLinks et Kaptain.\nMais aussi\u0026hellip;\nMahdi Zahraoui (Techlead chez onepoint ‚Äì Microservices Enthusiast) questionnera la transition du monolithique aux microservices.\nNebrass Lamouchi (Techlead chez onepoint ‚Äì OWASP Project Leader ‚Äì Apache NetBeans Committer ‚Äì Auteur) et Rached Jouida (DevOps chez onepoint ‚Äì OpenShift \u0026amp; Containers Evangelist) exposeront la rencontre des microservices Java et de Kubernetes/OpenShift\nLien d\u0026rsquo;inscription\n","permalink":"https://blog.nebrass.fr/first-architecture-meetup-onepoint-july-2018/","summary":"Le jeudi 19 juillet √† l\u0026rsquo;Atelier, la communaut√© Architecture exposera des REXs, des d√©mos, des workshops avec un invit√© mondialement reconnu sur le DevOps, le CloudNative et les architectures modernes : Aymen El Amri.\nNotre special guest :\nAymen El Amri - Meilleures pratiques de r√©alisation d\u0026rsquo;applications Cloud Native\nAymen El Amri a √©t√© reconnu par TechBeacon comme l\u0026rsquo;un des Top 100 leaders, praticiens, experts de DevOps √† suivre. Il est le fondateur d'eralabs, une soci√©t√© de conseil et de formation et un Community-Hub.","title":"First Architecture MeetUp @Onepoint - July 2018"},{"content":"The workshop \u0026ldquo;Playing with Spring Boot and Angular\u0026rdquo; was held on May 4th 2018, at¬†ENSIT**, Tunisia.**\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular   TypeScript Fundamentals Angular Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service  The event was sponsored by¬†onepoint.\n Many thanks to the ENSIT Microsoft Club¬†for the event organization.\nMany thanks to everybody involved, and hope it was enjoyable and informative for the attendees!\n","permalink":"https://blog.nebrass.fr/workshop-report-ensit-may-2018/","summary":"The workshop \u0026ldquo;Playing with Spring Boot and Angular\u0026rdquo; was held on May 4th 2018, at¬†ENSIT**, Tunisia.**\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular   TypeScript Fundamentals Angular Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service  The event was sponsored by¬†onepoint.","title":"Workshop Report: ENSIT - May 2018"},{"content":"The workshop \u0026ldquo;Playing with Java Microservices with Kubernetes and OpenShift\u0026rdquo; was held on May 3rd 2018, at¬†IntilaQ Center, El Ghazela Technopark, Tunisia.\nThe content of the workshop was :\n Introduction to Microservices Architectures Introduction to the 12-Factors methodology Introduction to Microservices concerns \u0026amp; Spring Cloud Libraries  Configuration Management Service Discovery Load Balancing API Gateway Service Security Centralized Logging \u0026amp; Metrics Resilience \u0026amp; Fault Tolerance Packaging, Deployment \u0026amp; Scheduling   Introduction to Kubernetes  The event was co-organized by Houssem Dellai and sponsored by¬†onepoint.\n Next time, the focus will be a lot more on hands on exercises on Kubernetes and OpenShift, so that attendees can actively participate in the sessions and go home with new techniques that will have been taught during the event.\n","permalink":"https://blog.nebrass.fr/workshop-report-intilaq-may-2018/","summary":"The workshop \u0026ldquo;Playing with Java Microservices with Kubernetes and OpenShift\u0026rdquo; was held on May 3rd 2018, at¬†IntilaQ Center, El Ghazela Technopark, Tunisia.\nThe content of the workshop was :\n Introduction to Microservices Architectures Introduction to the 12-Factors methodology Introduction to Microservices concerns \u0026amp; Spring Cloud Libraries  Configuration Management Service Discovery Load Balancing API Gateway Service Security Centralized Logging \u0026amp; Metrics Resilience \u0026amp; Fault Tolerance Packaging, Deployment \u0026amp; Scheduling   Introduction to Kubernetes  The event was co-organized by Houssem Dellai and sponsored by¬†onepoint.","title":"Workshop Report: IntilaQ - May 2018"},{"content":"The workshop \u0026ldquo;Playing with Spring Boot and Angular 5\u0026rdquo; was held on May 2nd 2018, at ISG Tunis, Tunisia.\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular 5   TypeScript Fundamentals Angular 5 Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service Router  The event was sponsored by¬†onepoint.\n Many thanks to Mr Chaouki Bayoudhi for the event organization.\nMany thanks to everybody involved, and hope it was enjoyable and informative for the attendees!\n","permalink":"https://blog.nebrass.fr/workshop-report-isg-tunis-may-2018/","summary":"The workshop \u0026ldquo;Playing with Spring Boot and Angular 5\u0026rdquo; was held on May 2nd 2018, at ISG Tunis, Tunisia.\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular 5   TypeScript Fundamentals Angular 5 Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service Router  The event was sponsored by¬†onepoint.","title":"Workshop Report: ISG Tunis - May 2018"},{"content":"The workshop \u0026ldquo;Playing with Java Microservices with Docker\u0026rdquo; was held on Wednesday, 25th of April 2018, at the National School of Computer Sciences, in Mannouba, Tunisia.\nYeah ! We did it !! Many persons said it will be difficult to make an 8-hours workshop, but we made it !\nThe coffee breaks were sponsored by onepoint. The day ended with a giveaway of some Java Books to students.\n It was great for everyone to see the great turn out and the plan is to have other Java Workshops soon, in many schools.\nIf you missed this event, no worries, the next one will be announced here and elsewhere, planning will begin once everyone has recovered from the above event!\n","permalink":"https://blog.nebrass.fr/workshop-report-ensi-april-2018/","summary":"The workshop \u0026ldquo;Playing with Java Microservices with Docker\u0026rdquo; was held on Wednesday, 25th of April 2018, at the National School of Computer Sciences, in Mannouba, Tunisia.\nYeah ! We did it !! Many persons said it will be difficult to make an 8-hours workshop, but we made it !\nThe coffee breaks were sponsored by onepoint. The day ended with a giveaway of some Java Books to students.\n It was great for everyone to see the great turn out and the plan is to have other Java Workshops soon, in many schools.","title":"Workshop Report: ENSI - April 2018"},{"content":"I will animate a workshop in my school, ISG Tunis, Tunisia, on the May 2nd, 2018. The workshop will be about Spring Boot \u0026amp; Angular 5 development.\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular 5   TypeScript Fundamentals Angular 5 Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service Router  Requirements:\n NetBeans 8.2¬†\u0026amp;¬†Visual Studio Code Java Development Kit 8 Nodejs/NPM  Subscription link @EventBrite\n","permalink":"https://blog.nebrass.fr/playing-with-spring-boot-and-angular-5-isg-tunis-2018/","summary":"I will animate a workshop in my school, ISG Tunis, Tunisia, on the May 2nd, 2018. The workshop will be about Spring Boot \u0026amp; Angular 5 development.\nThe workshop content:\n Introduction to Spring Boot   Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator  Introduction to Angular 5   TypeScript Fundamentals Angular 5 Basics Template Syntax Components Services \u0026amp; Dependency Injection RxJS and Observables Communicating with the Server using the HttpClient Service Router  Requirements:","title":"Playing with Spring Boot and Angular 5 - ISG Tunis 2018"},{"content":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), on April 25th 2018, to animate a full-day workshop, about Microservices in Java and how to deploy them as Docker Containers. The event is organized by the¬†Open Source Software ENSI Club.\nThe workshop content:\n Introduction to Enterprise Development \u0026amp; Architectures Introduction to Microservices Architectures Introduction to Spring Boot  Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator   Introduction to the 12-Factors methodology Introduction to Microservices concerns \u0026amp; Spring Cloud Libraries  Configuration Management Service Discovery Load Balancing API Gateway Service Security Centralized Loggin \u0026amp; Metrics Resilience \u0026amp; Fault Tolerance Packaging, Deployment \u0026amp; Scheduling   Introduction to Docker  Presentation of Containerization vs Virtualization Presentation of Docker\u0026rsquo;s terminology: Image; Container; Dockerfile; Docker-Machine Presentation of Docker-Compose, Docker-Swarm Presentation of Docker Hub \u0026amp; Continuous Delivery using Docker    Requirements:\n Bring your own laptop with Netbeans 8.2 and Java JDK 8 installed. Drink so much coffee before, during and after the workshop. Bring you water bottle, microservices are too hot ^^  Subscription link @EventBrite\nMany thanks for Houssem Eddine Gharbi and Hajer Bouchaala for the invitation and the event\u0026rsquo;s organization.\n","permalink":"https://blog.nebrass.fr/playing-with-java-microservices-with-docker-ensi-2018/","summary":"I will be in the National School of Computing Sciences of Tunisia (ENSI Tunisia), on April 25th 2018, to animate a full-day workshop, about Microservices in Java and how to deploy them as Docker Containers. The event is organized by the¬†Open Source Software ENSI Club.\nThe workshop content:\n Introduction to Enterprise Development \u0026amp; Architectures Introduction to Microservices Architectures Introduction to Spring Boot  Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator   Introduction to the 12-Factors methodology Introduction to Microservices concerns \u0026amp; Spring Cloud Libraries  Configuration Management Service Discovery Load Balancing API Gateway Service Security Centralized Loggin \u0026amp; Metrics Resilience \u0026amp; Fault Tolerance Packaging, Deployment \u0026amp; Scheduling   Introduction to Docker  Presentation of Containerization vs Virtualization Presentation of Docker\u0026rsquo;s terminology: Image; Container; Dockerfile; Docker-Machine Presentation of Docker-Compose, Docker-Swarm Presentation of Docker Hub \u0026amp; Continuous Delivery using Docker    Requirements:","title":"Playing with Java microservices with Docker - ENSI 2018"},{"content":"I. Introduction In this tutorial I will show you how to write a small Spring Boot CRUD application and how to deploy it on Kubernetes.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.\nKubernetes (commonly referred to as \u0026ldquo;K8s\u0026quot;) is an open-source system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the Cloud Native Computing Foundation. It aims to provide a \u0026ldquo;platform for automating deployment, scaling, and operations of application containers across clusters of hosts\u0026rdquo;. It works with a range of container tools, including Docker.\nDocker is an open source project that automates the deployment of applications inside software containers.\nThis tutorial is a getting started point to the Spring Boot \u0026amp; K8s Stack.\nII. Writing the application 1. Creating the skull Here we go ! We will start using the great features and options offered by the Spring Framework Teams at Pivotal Software, Inc.\nTo avoid the hard parts when creating new project and getting it started, the Spring Team has created the Spring Initializr Project.\nThe Spring Initializr is a useful project that can generate a basic Spring Boot project structure easily. You can choose either your project to be based on Maven or Gradle, to use Java or Kotlin or Groovy, and to choose which version of Spring Boot you want to pick.\nSpring Initializr can be used:\n Using a Web-based Interface http://start.spring.io Using Spring Tool Suite or other different IDEs like NetBeans \u0026amp; IntelliJ Using the Spring Boot CLI  Spring Initializr gives you the ability to add the core dependencies that you need, like JDBC drivers or Spring Boot Starters.\n ‚ö†Ô∏è Hey! What is Spring Boot Starters?\nSpring Boot Starters are a set of convenient dependency descriptors that you can include in your application. You get a one-stop-shop for all the Spring and related technology that you need, without having to hunt through sample code and copy paste loads of dependency descriptors. For example, if you want to get started using Spring and JPA for database access, just include the spring-boot-starter-data-jpa dependency in your project,and you are good to go.\nThe starters contain a lot of the dependencies that you need to get a project up and running quickly and with a consistent, supported set of managed transitive dependencies.\n‚ÑπÔ∏è Note\nYou can learn more about Spring Boot Starters here: https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#using-boot-starter\n For our case we will be using:\n Maven Java 8 Spring Boot 1.5.x  And for the Dependencies we will choose:\n JPA H2 Rest Repositories Actuator Lombok   The resulting pom will be like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.onepoint.labs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;myschool\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;MySchool\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.10.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-rest\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt;   Now, we need to add the embedded databse configuration to the application.properties:\n1 2 3 4 5  spring.datasource.driver-class-name=org.h2.Driver spring.jpa.hibernate.ddl-auto=create spring.datasource.url=jdbc:h2:mem:boutique;DB_CLOSE_DELAY=-1 spring.datasource.username=sa spring.datasource.password=   Next, we will start the implementation of our Java components.\n2. Presenting the domain The domain of our application will just contain only one entity: Student.\n 3. Implementing the domain The Student entity will look like this:\n1 2 3 4 5 6 7 8 9 10 11  @Entity @Data public class Student { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; private String family; }    @Data generates all the boilerplate that is normally associated with simple POJOs (Plain Old Java Objects) and beans: getters for all fields, setters for all non-final fields, and appropriate toString, equals and hashCode implementations.  Next, we will implement the JPA Repository:\n1 2 3 4 5 6 7  @RepositoryRestResource(collectionResourceRel = \u0026#34;students\u0026#34;, path = \u0026#34;students\u0026#34;) public interface StudentRepository extends PagingAndSortingRepository\u0026lt;Student, Long\u0026gt; { List\u0026lt;Student\u0026gt; findByName(String name); List\u0026lt;Student\u0026gt; findByNameAndFamily(String name, String family); }    We are using this annotation to customize the REST endpoint. We are extending the PagingAndSortingRepository Interface to get the paging \u0026amp; sorting features. We are implementing custom JPA Repository methods using the special Spring Data Query DSL.  4. Adding the Swagger 2 Capabilities Swagger 2 is an open source project used to describe and document RESTful APIs. Swagger 2 is language-agnostic and is extensible into new technologies and protocols beyond HTTP. The current version defines a set HTML, JavaScript and CSS assets to dynamically generate documentation from a Swagger-compliant API. These files are bundled by the Swagger UI project to display the API on browser. Besides rendering documentation, Swagger UI allows other API developers or consumers to interact with the API‚Äôs resources without having any of the implementation logic in place.\nThe Swagger 2 specification, which is known as OpenAPI specification has several implementations. We will be using the Springfox implementation in our project.\nTo enable the Swagger capabilities to our project, we will be:\n Adding the Maven Dependencies Adding the Java Configuration  The Maven Dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \u0026lt;!--DEPENDENCIES FOR SWAGGER DOCUMENTATION--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-data-rest\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   For the Java Configuration, we need to create a Spring @Bean that configure Swagger in the application context.\n1 2 3 4 5 6 7 8 9 10 11  @Configuration public class SwaggerConfig { @Bean public Docket productApi() { return new Docket(DocumentationType.SWAGGER_2).select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); } }   Also, we need to add the @EnableSwagger2 annotation to the SpringBootApplication Main Class. We also have to add the @Import({springfox.documentation.spring.data.rest.configuration.SpringDataRestConfiguration.class}) annotation, that enables the Springfox implementation to scan and parse the generated methods by Spring Data REST.\n5. Run it ! To run the Spring Boot Application, you just run: mvn spring-boot:run\nThe application will be running on the 8080 port. To access the application, just go to http://localhost:8080, you will see this:\n We will enjoy the game now: we will be using Swagger forms to insert some records.\nTo access the Swagger UI, just go to http://localhost:8080/swagger-ui.html#/\n We will be using the Student Entity menu ‚Üí POST operation called saveStudent ‚Üí Try it out ‚Üí in the form just mention the name and the family data that you want to store. For example:\n1 2 3 4  { \u0026#34;family\u0026#34;: \u0026#34;Lamouchi\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nebrass\u0026#34; }   The screen will be like this:\n Just hit Execute and the Responde body will be somthing like this:\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;name\u0026#34;: \u0026#34;Nebrass\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;Lamouchi\u0026#34;, \u0026#34;_links\u0026#34;: { \u0026#34;self\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students/1\u0026#34; }, \u0026#34;student\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students/1\u0026#34; } } }   And the Response headers will look like this:\n1 2 3 4 5  content-type: application/hal+json;charset=UTF-8 date: Sun, 04 Mar 2018 22:05:55 GMT location: http://localhost:8080/students/1 transfer-encoding: chunked x-application-context: application:local   We can use the findAllStudents menu of the Swagger UI to list all the students in the DB, to be sure that the record has been successfully created:\nThe Response body of the findAll operation will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  { \u0026#34;_embedded\u0026#34;: { \u0026#34;students\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Nebrass\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;Lamouchi\u0026#34;, \u0026#34;_links\u0026#34;: { \u0026#34;self\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students/1\u0026#34; }, \u0026#34;student\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students/1\u0026#34; } } } ] }, \u0026#34;_links\u0026#34;: { \u0026#34;self\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students{?page,size,sort}\u0026#34;, \u0026#34;templated\u0026#34;: true }, \u0026#34;profile\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/profile/students\u0026#34; }, \u0026#34;search\u0026#34;: { \u0026#34;href\u0026#34;: \u0026#34;http://localhost:8080/students/search\u0026#34; } }, \u0026#34;page\u0026#34;: { \u0026#34;size\u0026#34;: 20, \u0026#34;totalElements\u0026#34;: 1, \u0026#34;totalPages\u0026#34;: 1, \u0026#34;number\u0026#34;: 0 } }    üí° Tip\nYou can simply go to http://localhost:8080/students and you will get all the students persisted to the DB.\n III. Moving to the Containers Era To deploy our (so huge, so big) application üòÇ, we will be using Docker. We will deploy our code in a container so we can enjoy the great feature provided by Docker.\nDocker has become the standard to develop and run containerized applications.\nThis is great ! Using Docker is quitely simple, especially in development stages. Deploying containers in the same server (docker-machine) is simple, but when start thinking to deploy many containers to many servers, things become complicated (managing the servers, managing the container state, etc.‚Äã).\nHere come the orchestration system, which provides many features like orchestrating computing, networking and storage infrastructure on behalf of user workloads\n Scheduling: matching containers to machines based on many factors like resources needs, affinity requirements‚Ä¶‚Äã Replications Handling failures Etc‚Ä¶‚Äã  For our tutorial, we will choose Kubernetes, the star of container orchestration.\n1. What is Kubernetes ? Kubernetes (Aka K8s) was a project spun out of Google as a open source next-gen container scheduler designed with the lessons learned from developing and managing Borg and Omega.\nKubernetes is designed to have loosely coupled components centered around deploying, maintaining and scaling applications. K8s abstracts the underlying infrastructure of the nodes and provides a uniform layer for the deployed applications.\n1.1 Kubernetes Architecture In the big plan, a Kubernetes cluster is composed of two items:\n Master Nodes: The main control plane for Kubernetes. It contains an API Server, a Scheduler, a Controller Manager (K8s cluster manager) and a datastore to save the cluster state called Etcd. Worker Nodes: A single host, physical or virual machine, capable of running POD. They are managed by the Master nodes.   Let‚Äôs have a look inside a Master node:\n (Kube) API-Server: allows the communication, thru REST APIs, between the Master node and all its clients such as Worker Nodes, kube-cli, ‚Ä¶‚Äã (Kube) Scheduler: a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity to assign a Node to a newly created POD. (Kube) Controller Manager: a daemon that embeds the core control loops shipped with Kubernetes. A control loop is a permenent listener that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the API-Server and makes changes attempting to move the current state towards the desired state. Etcd: a strong, consistent and highly available key-value store used for persisting the cluster state.  Then, what about a Worker node?\n Kubelet: an agent that runs on each node in the cluster. It makes sure that containers are running in a POD. Kube-Proxy: enables the Kubernetes service abstraction by maintaining network rules on the host and perform networking actions.   üí° Tip\nThe Container Runtime that we will user is Docker. Kubernetes is compatible with many others like Cri-o, Rkt, ‚Ä¶‚Äã\n 1.2 Kubernetes Core Concepts The K8s ecosystem covers many concepts and components. We will try to introduce them briefly.\nKubectl The kubectl is a command line interface for running commands against Kubernetes clusters.\nCluster A collection of hosts that aggregate their resources (CPU, Ram, Disk, ‚Ä¶‚Äã) into a usable pool.\nNamespace A logical partitioning capability that enable one Kubernetes cluster to be used by multiple users, teams of users, or a single user with multiple applications without concern for undesired interaction. Each user, team of users, or application may exist within its Namespace, isolated from every other user of the cluster and operating as if it were the sole user of the cluster.\nList all Namespace:\n1  $ kubectl get namespace # or `kubectl get ns`   Label Key-value pairs that are used to identify and select related sets of objects. Labels have a strict syntax and defined character set.\nAnnotation Key-value pairs that contain non-identifying information or metadata. Annotations do not have the the syntax limitations as labels and can contain structured or unstructured data.\nSelector Selectors use labels to filter or select objects. Both equality-based (=, ==, !=) or simple key-value matching selectors are supported.\nUse case of Annotations, Labels and Selectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentannotations:description:\u0026#34;nginx frontend\u0026#34;labels:app:nginxspec:replicas:3selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80  Pod It is the basic unit of work for Kubernetes. Represent a collection of containers that share resources, such as IP addresses and storage.\nPod Example.\n1 2 3 4 5 6 7 8 9 10 11  apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappspec:containers:- name:myapp-containerimage:busyboxcommand:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026#39;]  To list all Pods:\n1  $ kubectl get pod # or `kubectl get po`   ReplicationController A framework for defining pods that are meant to be horizontally scaled. A replication controller includes a pod definition that is to be replicated, and the pods created from it can be scheduled to different nodes.\nReplicationController Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  apiVersion:v1kind:ReplicationControllermetadata:name:nginxspec:replicas:3selector:app:nginxtemplate:metadata:name:nginxlabels:app:nginxspec:containers:- name:nginximage:nginxports:- containerPort:80  List all ReplicationControllers:\n1  $ kubectl get replicationcontroller # or `kubectl get rc`   ReplicaSet An upgraded version of ReplicationController that supports set-based selectors.\nReplicaSet Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  apiVersion:apps/v1kind:ReplicaSetmetadata:name:frontendlabels:app:guestbooktier:frontendspec:replicas:3selector:matchLabels:tier:frontendmatchExpressions:- {key: tier, operator: In, values:[frontend]}template:metadata:labels:app:guestbooktier:frontendspec:containers:- name:php-redisimage:gcr.io/google_samples/gb-frontend:v3env:- name:GET_HOSTS_FROMvalue:dnsports:- containerPort:80  List all ReplicaSets:\n1  $ kubectl get replicaset # or `kubectl get rs`   Deployment Includes a Pod template and a replicas field. Kubernetes will make sure the actual state (amount of replicas, Pod template) always matches the desired state. When you update a Deployment it will perform a \u0026ldquo;rolling update\u0026rdquo;.\nDeployment Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxspec:replicas:3selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80  List all Deployments:\n1  $ kubectl get deployment   StatefulSet A controller tthat aims to manage Pods that must persist or maintain state. Pod identity including hostname, network, and storage will be persisted.\nStatefulSet Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  apiVersion:apps/v1kind:StatefulSetmetadata:name:webspec:selector:matchLabels:app:nginxserviceName:\u0026#34;nginx\u0026#34;replicas:3template:metadata:labels:app:nginxspec:terminationGracePeriodSeconds:10containers:- name:nginximage:k8s.gcr.io/nginx-slim:0.8ports:- containerPort:80name:webvolumeMounts:- name:wwwmountPath:/usr/share/nginx/htmlvolumeClaimTemplates:- metadata:name:wwwspec:accessModes:[\u0026#34;ReadWriteOnce\u0026#34;]storageClassName:\u0026#34;my-storage-class\u0026#34;resources:requests:storage:1Gi  List all StatefulSets:\n1  $ kubectl get statefulset   DaemonSet Ensures that an instance of a specific pod is running on all (or a selection of) nodes in a cluster.\nDaemonSet Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentd-elasticsearchnamespace:kube-systemlabels:k8s-app:fluentd-loggingspec:selector:matchLabels:name:fluentd-elasticsearchtemplate:metadata:labels:name:fluentd-elasticsearchspec:tolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentd-elasticsearchimage:gcr.io/google-containers/fluentd-elasticsearch:1.20terminationGracePeriodSeconds:30  List all DaemonSets:\n1  $ kubectl get daemonset # or `kubectl get ds`   Service Define a single IP/port combination that provides access to a pool of pods. It uses label selectors to map groups of pods and ports to a cluster-unique virtual IP.\nService Example.\n1 2 3 4 5 6 7 8 9 10 11 12  apiVersion:v1kind:Servicemetadata:name:my-nginxlabels:run:my-nginxspec:ports:- port:80protocol:TCPselector:run:my-nginx  List all Services:\n1  $ kubectl get service # or `kubectl get svc`   Ingress An ingress controller is the primary method of exposing a cluster service (usually http) to the outside world. These are load balancers or routers that usually offer SSL termination, name-based virtual hosting etc‚Ä¶‚Äã\nIngress Example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion:extensions/v1beta1kind:Ingressmetadata:name:test-ingressannotations:nginx.ingress.kubernetes.io/rewrite-target:/spec:rules:- http:paths:- path:/testpathbackend:serviceName:testservicePort:80  List all Ingress:\n1  $ kubectl get ingress   Volume Storage that is tied to the Pod Lifecycle, consumable by one or more containers within the pod.\nPersistentVolume A PersistentVolume (PV) represents a storage resource. PVs are commonly linked to a backing storage resource, NFS, GCEPersistentDisk, RBD etc. and are provisioned ahead of time. Their lifecycle is handled independently from a pod.\nList all PersistentVolumes:\n1  $ kubectl get persistentvolume` or `kubectl get pv`   PersistentVolumeClaim A PersistentVolumeClaim (PVC) is a request for storage that satisfies a set of requirements. Commonly used with dynamically provisioned storage.\nList all PersistentVolumeClaims:\n1  $ kubectl get persistentvolumeclaim` or `kubectl get pvc`   StorageClass Storage classes are an abstraction on top of an external storage resource. These will include a provisioner, provisioner configuration parameters as well as a PV reclaimPolicy.\nList all StorageClasses:\n1  $ kubectl get storageclass` or `kubectl get sc`   Job The job controller ensures one or more pods are executed and successfully terminates. It will do this until it satisfies the completion and/or parallelism condition.\nList all Jobs:\n1  $ kubectl get job`   CronJob An extension of the Job Controller, it provides a method of executing jobs on a cron-like schedule.\nList all CronJobs:\n1  $ kubectl get cronjob`   ConfigMap Externalized data stored within Kubernetes that can be referenced as a commandline argument, environment variable or injected as a file into a volume mount. Ideal for implementing the External Configuration Store pattern.\nList all ConfigMaps:\n1  $ kubectl get configmap` or `kubectl get cm`   Secret Functionally identical to ConfigMaps, but stored encoded as base64, and encrypted at rest (if configured).\nList all Secrets:\n1  $ kubectl get secret`   2. Run Kubernetes locally For our tutorial we will not build a real Kubernetes Cluster. We will use Minikube.\nMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\nFor Minikube installation : https://github.com/kubernetes/minikube\nAfter the installation, to start Minikube:\n1  $ minikube start   The minikube start command creates a kubectl context called minikube. This context contains the configuration to communicate with your minikube cluster.\nMinikube sets this context to default automatically, but if you need to switch back to it in the future, run:\nkubectl config use-context minikube\nTo access the Kubernetes Dashboard:\n1  $ minikube dashboard   The Dashboard will be opened in your default browser:\n The minikube stop command can be used to stop your cluster. This command shuts down the minikube virtual machine, but preserves all cluster state and data. Starting the cluster again will restore it to it‚Äôs previous state.\nThe minikube delete command can be used to delete your cluster. This command shuts down and deletes the minikube virtual machine. No data or state is preserved.\n3. Refactoring the application Now we want to move from H2 Databse to PostgreSQL. So we have to configure the application to use it by mentioning some properties like JDBC driver, url, username, password‚Ä¶‚Äã in the application.properties file and to add the PostgreSQL JDBC Driver in the pom.xml.\nFirst of all, we start by adding this dependency to our pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;   Next, we do some modifications to the application.properties:\n1 2 3 4 5  spring.datasource.driver-class-name=org.postgresql.Driver spring.jpa.hibernate.ddl-auto=create spring.datasource.url=jdbc:postgresql://${POSTGRES_SERVICE}:5432/${POSTGRES_DB_NAME} spring.datasource.username=${POSTGRES_DB_USER} spring.datasource.password=${POSTGRES_DB_PASSWORD}   We have used environment properties placeholders:\n POSTGRES_SERVICE : Host of PostgreSQL DB Server POSTGRES_DB_NAME : PostgreSQL DB Name POSTGRES_DB_USER : PostgreSQL Username POSTGRES_DB_PASSWORD : PostgreSQL Password  We will extract these values from a Kubernetes ConfigMap and Secret objects.\nCreate the ConfigMap We need to create the ConfigMap:\n1 2 3  $ kubectl create configmap postgres-config \\ \t--from-literal=postgres.service.name=postgresql \\ \t--from-literal=postgres.db.name=boutique   We can check the created ConfigMap:\n1  $ kubectl get cm postgres-config -o json   The output will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;postgres.db.name\u0026#34;: \u0026#34;boutique\u0026#34;, \u0026#34;postgres.service.name\u0026#34;: \u0026#34;postgresql\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;ConfigMap\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2018-03-25T16:42:39Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;postgres-config\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;195\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/api/v1/namespaces/default/configmaps/postgres-config\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;87d7481c-304b-11e8-889d-080027a8a37c\u0026#34; } }   Create the Secret Next we create the Secret:\n1 2 3  $ kubectl create secret generic db-security \\ \t--from-literal=db.user.name=nebrass \\ \t--from-literal=db.user.password=password   We can check the created Secret:\n1  $ kubectl get secret db-security -o json   The output will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;db.user.name\u0026#34;: \u0026#34;bmVicmFzcw==\u0026#34;, \u0026#34;db.user.password\u0026#34;: \u0026#34;cGFzc3dvcmQ=\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2018-03-25T16:56:36Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;db-security\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;714\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/api/v1/namespaces/default/secrets/db-security\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;7ac96df3-304d-11e8-889d-080027a8a37c\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Opaque\u0026#34; }    The credentials are encoded as base64. This is to protect the secret from being exposed accidentally to someone looking or from being stored in a terminal log. From kubernetes‚Äôs point of view the contents of this Secret is unstructured: it can contain arbitrary key-value pairs.  Deploy PostgreSQL to Kubernetes As the configuration is centralized and stored in the Kubernetes Cluster, we can share them between the Spring Boot Application and the PostgreSQL Service that we will create now.\nI already prepared the PostgreSQL resource file, in the src/main/assets/. This YAML file contains a Deployment and a Service resources.\nWe loaded the properties from our ConfigMap and Secret.\nThe content of postgres.yml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:postgresqlnamespace:defaultspec:replicas:1template:metadata:labels:app:postgresqlspec:volumes:- name:dataemptyDir:{}containers:- name:postgresimage:postgres:9.6.5env:- name:POSTGRES_USERvalueFrom:secretKeyRef:name:db-securitykey:db.user.name- name:POSTGRES_PASSWORDvalueFrom:secretKeyRef:name:db-securitykey:db.user.password- name:POSTGRES_DBvalueFrom:configMapKeyRef:name:postgres-configkey:postgres.db.nameports:- containerPort:5432volumeMounts:- name:datamountPath:/var/lib/postgresql/---apiVersion:v1kind:Servicemetadata:name:postgresqlnamespace:defaultspec:selector:app:postgresqlports:- port:5432   We will use the postgres:9.6.5 image The env block is used to load data in the container environment. Create an environment variable with a value loaded from a key called db.user.name in the secret called db-security. Create an environment variable with a value loaded from a key called db.user.password in the secret called db-security. Create an environment variable with a value loaded from a key called postgres.db.name in the configMap called postgres-config.  To apply this resource file to Kubernetes, we can do:\n1  $ kubectl create -f src/main/assets/postgres.yml   The output will be:\n1 2  deployment \u0026#34;postgresql\u0026#34; created service \u0026#34;postgresql\u0026#34; created   We can check the created Deployment:\n1  $ kubectl get deployment postgresql -o json   The output will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  { \u0026#34;apiVersion\u0026#34;: \u0026#34;extensions/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;deployment.kubernetes.io/revision\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;generation\u0026#34;: 1, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;postgresql\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;postgresql\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1025\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/extensions/v1beta1/namespaces/default/deployments/postgresql\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;84fa2f1a-304e-11e8-889d-080027a8a37c\u0026#34; }, \u0026#34;spec\u0026#34;: { ... }, \u0026#34;status\u0026#34;: { \u0026#34;availableReplicas\u0026#34;: 1, \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Deployment has minimum availability.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;MinimumReplicasAvailable\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ], \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;readyReplicas\u0026#34;: 1, \u0026#34;replicas\u0026#34;: 1, \u0026#34;updatedReplicas\u0026#34;: 1 } }   We can check the created Service:\n1  $ kubectl get service postgresql -o json   The output will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  { \u0026#34;apiVersion\u0026#34;: \u0026#34;extensions/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;deployment.kubernetes.io/revision\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;generation\u0026#34;: 1, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;postgresql\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;postgresql\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1025\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/extensions/v1beta1/namespaces/default/deployments/postgresql\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;84fa2f1a-304e-11e8-889d-080027a8a37c\u0026#34; }, \u0026#34;spec\u0026#34;: { ... }, \u0026#34;status\u0026#34;: { \u0026#34;availableReplicas\u0026#34;: 1, \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2018-03-25T17:04:03Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Deployment has minimum availability.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;MinimumReplicasAvailable\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ], \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;readyReplicas\u0026#34;: 1, \u0026#34;replicas\u0026#34;: 1, \u0026#34;updatedReplicas\u0026#34;: 1 } }   In this output, there is something interesting: the port and the target port:\n The port this service will be available on The container port the service will forward to  We already mentionned the port in the spring.datasource.url property.\n- What do you think if we use the powerful features of Kubernetes to resolve this port dynamically?\n- Ok but how? :)\nAfter creating these resources, the effective properties will like this :\n1 2 3 4 5  spring.datasource.driver-class-name=org.postgresql.Driver spring.jpa.hibernate.ddl-auto=create spring.datasource.url=jdbc:postgresql://postgresql:5432/boutique spring.datasource.username=nebrass spring.datasource.password=password   The Datasource URL is pointing to a host called postgresql. The resolution of the hostname to IP is done by Kubernetes.\nIf we check the postgresql service:\n1 2 3 4  $ kubectl get svc postgresql NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgresql ClusterIP 10.111.244.143 \u0026lt;none\u0026gt; 5432/TCP 1h   There is an other cool feature in Kubernetes, we can fetch data related to the service itself. We can for example get the port associated to this service.\nFor example:\n ${postgresql.service.host} will be resolved to 10.111.244.143. ${postgresql.service.port} will be resolved to 5432.  We can do it better ^^) we can merge the environment variables in the great holders that will be resolved by Kubernetes. They will become:\n ${postgresql.service.host} can be written ${${POSTGRES_SERVICE}.service.host} ${postgresql.service.port} can be written ${${POSTGRES_SERVICE}.service.port}  In this way, the internal placeholder will be resolved by the environment variable provided by the ConfigMap, and the external placeholder will be resolved by Kubernetes.\nThe resulting application.properties will look like:\n1 2 3 4 5  spring.datasource.driver-class-name=org.postgresql.Driver spring.jpa.hibernate.ddl-auto=create spring.datasource.url=jdbc:postgresql://${${POSTGRES_SERVICE}.service.host}:${${POSTGRES_SERVICE}.service.port}/${POSTGRES_DB_NAME} spring.datasource.username=${POSTGRES_DB_USER} spring.datasource.password=${POSTGRES_DB_PASSWORD}   Now, we are ConfigMaps addicts :) we will host our application.properties in a ConfigMap in Kubernetes. To do it, just do:\n1 2  $ kubectl create configmap app-config \\ \t--from-file=src/main/resources/application.properties   Now that the application.properties, how can our Spring Boot application use them?\nThe answer is so easy: the Spring Cloud Kubernetes plugin.\nWhat is Spring Cloud Kubernetes The Spring Cloud Kubernetes plug-in implements the integration between Kubernetes and Spring Boot. It provides access to the configuration data of a ConfigMap using the Kubernetes API.\nIt make so easy to integrate Kubernetes ConfigMap directly with the Spring Boot externalized configuration mechanism, so that Kubernetes ConfigMaps behave as an alternative property source for Spring Boot configuration.\nTo enable the great features of the plugin:\n  Add the Maven Dependency:Add this dependency to the pom.xml:\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.fabric8\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-kubernetes\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;     Create the Bootstrap file:Create a new file bootstrap.properties under src/main/resources:\n1 2  spring.application.name=${project.artifactId} spring.cloud.kubernetes.config.name=app-config    The ${project.artifactId} will be parsed and populated by maven-resources-plugin, that you will find in the pom.xml of the sample project hosted in the Github repository of this tutorial. The name of the ConfigMap where we stored our great application.properties.    That‚Äôs it! The Spring Cloud Kubernetes is correctly integrated to our application. When we deploy our application to Kubernetes, it will use the application.properties stored in the ConfigMap app-config.\nYou say deploy? Ok but how to do it?\nDeploy it to Kubernetes The deployment?! A dedicated full story, that can have many chapters. But we will try to keep it short and simple.\nBy definition, Kubernetes is a container orchestration solution. So deploying an application to Kubernetes means :\n Containerizing the application: creating an image embedding the application. Preparing the deployment resources (Deployment, ReplicaSet, etc‚Ä¶‚Äã). Deploying the container to Kubernetes.  These steps can take some time to be done, even if we try to automate this process, it will take us long time to implement it, and it will take more time to cover all the cases and variants of the apps.\nAs these tasks are so heavy, we need some tool that do all of this for easy.\nHere comes the super powerfull tool: Fabric8-Maven-Plugin.\n Fabric8-Maven-Plugin is a one-stop-shop for building and deploying Java applications for Docker, Kubernetes and OpenShift. It brings your Java applications on to Kubernetes and OpenShift. It provides a tight integration into maven and benefits from the build configuration already provided. It focuses on three tasks:\n Building Docker images Creating OpenShift and Kubernetes resources Deploy application on Kubernetes and OpenShift  The plugin will do all the heavy tasks ! Yes ! He will ! :)\nIt can be configured very flexibly and supports multiple configuration models for creating:\n Zero Configuration for a quick ramp-up where opinionated defaults will be pre-selected. Inline Configuration within the plugin configuration in an XML syntax. External Configuration templates of the real deployment descriptors which are enriched by the plugin. Docker Compose Configuration provide Docker Compose file and bring up docker compose deployments on a Kubernetes/OpenShift cluster.  To enable fabric8-maven-plugin on your project just add this to the plugins sections of your pom.xml:\n1 2 3 4 5  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.fabric8\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fabric8-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.38\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt;   Now in order to use fabric8-maven-plugin to build or deploy, make sure you have a Kubernetes cluster up and running.\nThe fabric8-maven-plugin supports a rich set of goals for providing a smooth Java developer experience. You can categorize these goals as follows:\n Build goals are used to create and manage the Kubernetes build artifacts like Docker images.  fabric8:build : Build Docker images fabric8:resource : Create Kubernetes resource descriptors fabric8:push : Push Docker images to a registry fabric8:apply : Apply resources to a running cluster   Development goals are used in deploying resource descriptors to the development cluster.  fabric8:run : Run a complete development workflow cycle fabric8:resource ‚Üí fabric8:build ‚Üí fabric8:apply in the foreground. fabric8:deploy : Deploy resources descriptors to a cluster after creating them and building the app. Same as fabric8:run except that it runs in the background. fabric8:undeploy : Undeploy and remove resources descriptors from a cluster. fabric8:watch : Watch for doing rebuilds and restarts    If you want to integrate the goals in the maven lifecycle phases, you can do it easily:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.fabric8\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fabric8-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.38\u0026lt;/version\u0026gt; \u0026lt;!-- This block will connect fabric8:resource and fabric8:build to lifecycle phases --\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;fmp\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;resource\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;build\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;    ‚ÑπÔ∏è Note\nFor lazyness purposes :p I will be referencing the fabric8-maven-plugin as f8mp.\n  ‚ö†Ô∏è Warning\nThe f8mp needs to access to the Docker environment of Minikube, to do this just start the command¬†eval $(minikube docker-env) . Without this command, Kubernetes will not find the Docker images built by f8mp.\n Now when we do mvn clean install for example, the plugin will build the docker images and will generate the Kubernetes resource descriptors in the ${basedir}/target/classes/META-INF/fabric8/kubernetes directory.\nLet‚Äôs check the generated resource descriptors.\n ‚ö†Ô∏è Warning\nWait ! Wait! We said that we will pass the ConfigMaps to the Spring Boot application. Where is that?!\n Yep! Before generating our resources descriptors, we have to tell this to f8mp.\nf8mp has an easy way to do this: the plugin can handle some Resource Fragments. It‚Äôs a piece of YAML code located in the src/main/fabric8 directory. Each resource get is own file, which contains some skeleton of a resource description. The plugin will pick up the resource, enriches it and then combines all the data. Within these descriptor files you are can freely use any Kubernetes feature.\nIn our case, we will deliver in the Resource Fragment the configuration of the environment variables to the Pod, where our Spring Boot Application will be executed. We will use a fragment of a Deployment, which will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:${project.artifactId}namespace:defaultspec:template:spec:containers:- name:${project.artifactId}env:- name:POSTGRES_SERVICEvalueFrom:configMapKeyRef:name:postgres-configkey:postgres.service.name- name:POSTGRES_DB_NAMEvalueFrom:configMapKeyRef:name:postgres-configkey:postgres.db.name- name:POSTGRES_DB_USERvalueFrom:secretKeyRef:name:db-securitykey:db.user.name- name:POSTGRES_DB_PASSWORDvalueFrom:secretKeyRef:name:db-securitykey:db.user.password   The name of our Deployment and the container. The environment variables that we are creating and populating from the ConfigMap and Secret.  Now, when the f8mp will try to generate the resources descriptors, it will find this resource fragment, combine it with the other data. The resulting output will be coherent with the fregment that we already provided.\nLet‚Äôs try it. Just run mvn clean install:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building MySchool 0.0.1-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] ... [INFO] [INFO] --- fabric8-maven-plugin:3.5.38:resource (fmp) @ myschool - [INFO] F8: Running in Kubernetes mode [INFO] F8: Running generator spring-boot [INFO] F8: spring-boot: Using Docker image fabric8/java-jboss-openjdk8-jdk:1.3 as base / builder [INFO] F8: using resource templates from /Users/n.lamouchi/MySchool/src/main/fabric8 [INFO] F8: fmp-service: Adding a default service \u0026#39;myschool\u0026#39; with ports [8080] [INFO] F8: spring-boot-health-check: Adding readiness probe on port 8080, path=\u0026#39;/health\u0026#39;, scheme=\u0026#39;HTTP\u0026#39;, with initial delay 10 seconds [INFO] F8: spring-boot-health-check: Adding liveness probe on port 8080, path=\u0026#39;/health\u0026#39;, scheme=\u0026#39;HTTP\u0026#39;, with initial delay 180 seconds [INFO] F8: fmp-revision-history: Adding revision history limit to 2 [INFO] F8: f8-icon: Adding icon for deployment [INFO] F8: f8-icon: Adding icon for service [INFO] F8: validating /Users/n.lamouchi/MySchool/target/classes/META-INF/fabric8/openshift/myschool-svc.yml resource [INFO] F8: validating /Users/n.lamouchi/MySchool/target/classes/META-INF/fabric8/openshift/myschool-deploymentconfig.yml resource [INFO] F8: validating /Users/n.lamouchi/MySchool/target/classes/META-INF/fabric8/openshift/myschool-route.yml resource [INFO] F8: validating /Users/n.lamouchi/MySchool/target/classes/META-INF/fabric8/kubernetes/myschool-deployment.yml resource [INFO] F8: validating /Users/n.lamouchi/MySchool/target/classes/META-INF/fabric8/kubernetes/myschool-svc.yml resource [INFO] ... [INFO] [INFO] --- fabric8-maven-plugin:3.5.38:build (fmp) @ myschool - [INFO] F8: Building Docker image in Kubernetes mode [INFO] F8: Running generator spring-boot [INFO] F8: spring-boot: Using Docker image fabric8/java-jboss-openjdk8-jdk:1.3 as base / builder [INFO] Copying files to /Users/n.lamouchi/MySchool/target/docker/nebrass/myschool/snapshot-180327-174802-0575/build/maven [INFO] Building tar: /Users/n.lamouchi/MySchool/target/docker/nebrass/myschool/snapshot-180327-174802-0575/tmp/docker-build.tar [INFO] F8: [nebrass/myschool:snapshot-180327-174802-0575] \u0026#34;spring-boot\u0026#34;: Created docker-build.tar in 283 milliseconds [INFO] F8: [nebrass/myschool:snapshot-180327-174802-0575] \u0026#34;spring-boot\u0026#34;: Built image sha256:61171 [INFO] F8: [nebrass/myschool:snapshot-180327-174802-0575] \u0026#34;spring-boot\u0026#34;: Tag with latest [INFO] ... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 30.505 s [INFO] Finished at: 2018-03-27T17:48:29+02:00 [INFO] Final Memory: 70M/721M [INFO] ------------------------------------------------------------------------    Generating the resources descriptors based on the detected configuration: Spring Boot application that is using the port 8080 with existing Actuator endpoints. Building the Docker image in Kubernetes mode (locally and not like the Openshift mode, which uses the Openshift S2I Mechanism for the build).  After building our project, we got these files in the ${basedir}/target/classes/META-INF/fabric8/kubernetes directory:\n my-school-deployment.yml my-school-svc.yml  Let‚Äôs check the Deployment:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  apiVersion:extensions/v1beta1kind:Deploymentmetadata:annotations:fabric8.io/git-commit:0120b762d7e26994e8b01d7e85f8941e5d095130fabric8.io/git-branch:masterfabric8.io/scm-tag:HEAD...labels:app:myschoolprovider:fabric8version:0.0.1-SNAPSHOTgroup:com.onepoint.labsname:myschoolnamespace:defaultspec:replicas:1revisionHistoryLimit:2selector:matchLabels:app:myschoolprovider:fabric8group:com.onepoint.labstemplate:metadata:annotations:fabric8.io/git-commit:0120b762d7e26994e8b01d7e85f8941e5d095130fabric8.io/git-branch:masterfabric8.io/scm-tag:HEAD...labels:app:myschoolprovider:fabric8version:0.0.1-SNAPSHOTgroup:com.onepoint.labsspec:containers:- env:- name:POSTGRES_SERVICEvalueFrom:configMapKeyRef:key:postgres.service.namename:postgres-config- name:POSTGRES_DB_NAMEvalueFrom:configMapKeyRef:key:postgres.db.namename:postgres-config- name:POSTGRES_DB_USERvalueFrom:secretKeyRef:key:db.user.namename:db-security- name:POSTGRES_DB_PASSWORDvalueFrom:secretKeyRef:key:db.user.passwordname:db-security- name:KUBERNETES_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespaceimage:nebrass/myschool:snapshot-180327-003059-0437imagePullPolicy:IfNotPresentlivenessProbe:httpGet:path:/healthport:8080scheme:HTTPinitialDelaySeconds:180name:myschoolports:- containerPort:8080name:httpprotocol:TCP- containerPort:9779name:prometheusprotocol:TCP- containerPort:8778name:jolokiaprotocol:TCPreadinessProbe:httpGet:path:/healthport:8080scheme:HTTPinitialDelaySeconds:10securityContext:privileged:false   Generated annotations that holds many usefull data, like the git-commit id or the git-branch Labels section holds the Maven Project groupId, artifactId and version information. Add to that, a label provider=fabric8 to tell you that this data is generated by f8mp The Docker Image, generated and built by f8mp. The suffix snapshot-180327-003059-0437 is the default format to assign a version tag. A liveness probe checks if the container in which it is configured is still up. A readiness probe determines if a container is ready to service requests.   üí° Tip\nThe liveness and readiness probes are generated because the f8mp has detected that the Spring-Boot-Actuator library in the classpath.\n At this point, we can deploy our application just using the command mvn fabric8:apply, the output will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  [INFO] --- fabric8-maven-plugin:3.5.38:apply (default-cli) @ myschool --- [INFO] F8: Using Kubernetes at https://192.168.99.100:8443/ in namespace default with manifest /Users/n.lamouchi/Downloads/MyBoutiqueReactive/target/classes/META-INF/fabric8/kubernetes.yml [INFO] Using namespace: default [INFO] Updating a Service from kubernetes.yml [INFO] Updated Service: target/fabric8/applyJson/default/service-myschool.json [INFO] Using namespace: default [INFO] Creating a Deployment from kubernetes.yml namespace default name myschool [INFO] Created Deployment: target/fabric8/applyJson/default/deployment-myschool.json [INFO] F8: HINT: Use the command `kubectl get pods -w` to watch your pods start up [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 16.003 s [INFO] Finished at: 2018-03-28T00:03:56+02:00 [INFO] Final Memory: 78M/756M [INFO] ------------------------------------------------------------------------   We can check all the resources that exists on our cluster\n1  $ kubectl get all   This command will list all the resources in the default namespace. The output will be something:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/myschool 1 1 1 1 6m deploy/postgresql 1 1 1 1 7m NAME DESIRED CURRENT READY AGE rs/myschool-5dd7cbff98 1 1 1 6m rs/postgresql-5f57747985 1 1 1 7m NAME READY STATUS RESTARTS AGE po/myschool-5dd7cbff98-w2wtl 1/1 Running 0 6m po/postgresql-5f57747985-8n9h6 1/1 Running 0 7m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 23m svc/myschool ClusterIP 10.106.72.231 \u0026lt;none\u0026gt; 8080/TCP 20m svc/postgresql ClusterIP 10.111.62.173 \u0026lt;none\u0026gt; 5432/TCP 21m    üí° Tip\nWe can list all these resources on the K8s Dashboard.\n Wow! Yes, these resources have been created during the steps that we did before :) Good job !\n5. It works ! Hakuna Matata ! It‚Äôs done ^^) we deployed the application and all its required resources; but how can we access the deployed application?\nThe application will be accessible thru the Kubernetes Service object called myschool.\nLet‚Äôs check what is the service myschool, type kubectl get svc myschool, the output will be:\n1 2  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myschool ClusterIP 10.106.72.231 \u0026lt;none\u0026gt; 8080/TCP 1d   The type of our service is ClusterIP. What is a ClusterIP ?\nClusterIP is the default ServiceType. It exposes the service on a cluster-internal IP so it will be only reachable from within the cluster.\nSo we cannot use this ServiceType because we need our service to be reachable from outisde the cluster. So is there any other type of service?\nYes! There are three other types of services, other than ClusterIP:\n NodePort: Exposes the service on each Node‚Äôs IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You‚Äôll be able to contact the NodePort service, from outside the cluster, by requesting \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;. LoadBalancer: Exposes the service externally using a cloud provider‚Äôs load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created. ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value.   üí° Tip\nIn our case, we will be using the LoadBalancer service, which redirects traffic across all the nodes. Clients connect to the LoadBalancer service through the load balancer‚Äôs IP.\n Ok :) The LoadBalancer will be our ServiceType. But how can we tell this to the f8mp ?\nWe have two solutions:\n The Resource Fragments as we did before. The Inline Configuration, which is XML based configuration of the f8mp plugin.  Let‚Äôs use this time the Inline Configuration to tell the f8mp that we want a LoadBalancer service:\nIn the configuration section of the f8mp plugin, we will declare an enricher.\nAn enricher is a component used to create and customize Kubernetes and Openshift resource objects. f8mp comes with a set of enrichers which are enabled by default. One of these enrichers, is the fmp-service which is used to customize the Services.\nThe f8mp with the configured enricher will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.fabric8\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fabric8-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.38\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;enricher\u0026gt; \u0026lt;config\u0026gt; \u0026lt;fmp-service\u0026gt; \u0026lt;type\u0026gt;LoadBalancer\u0026lt;/type\u0026gt; \u0026lt;/fmp-service\u0026gt; \u0026lt;/config\u0026gt; \u0026lt;/enricher\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;fmp\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;resource\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;build\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   Let‚Äôs build and a redeploy our project using mvn clean install fabric8:apply and see what is the type of the deployed service using kubectl get svc myschool:\n1 2  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myschool LoadBalancer 10.106.72.231 \u0026lt;pending\u0026gt; 8080:31246/TCP 2d    ‚ö†Ô∏è Warning\nThe \u0026lt;pending\u0026gt; shown in the EXTERNAL-IP column is due that we are using minikube.\n Cool ! How can we access the application now? How can we get the URL of the deployed application?\nEuuuuh! The answer is shorter than the question :D to get the URL of the deployed app on minikube just type:\n1  $ open $(minikube service myschool --url)   This command will open the URL of the Spring Boot Application in your default browser :)\n  üí° Tip\nThe command minikube service myschool --url will give us the path of the service myschool, which is pointing on our Spring Boot Application.\n How can I access the Swagger UI of my deployed App?\n1  $ open $(minikube service myschool --url)/swagger-ui.html   This command will open the URL of the Spring Boot Application in your default browser :)\n The full source code of this tutorial, can be found here.\nIV. Conclusion \u0026amp; final words: The main goal of this tutorial is to introduce you to the Kubernetes ecosystem and to let you start playing with Kubernetes.\nPutting your hands on practical exercices on Kubernetes will let you master this great platform. You will be able to feel the performance of Kubernetes when you experience many use cases.\nYou can consider the small application that we developed as a small microservice. You can develop other small apps and make interaction with them to start playing with the microservices architecture.\nWhat other areas around Java Microservices, Docker and Kubernetes would you like to know? Let me know at my mail. I plan on making additional tutorials on these topics. I will be happy to pick topics based on your feedback!\n","permalink":"https://blog.nebrass.fr/playing-with-spring-boot-on-kubernetes/","summary":"I. Introduction In this tutorial I will show you how to write a small Spring Boot CRUD application and how to deploy it on Kubernetes.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.\nKubernetes (commonly referred to as \u0026ldquo;K8s\u0026quot;) is an open-source system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the Cloud Native Computing Foundation.","title":"Playing with Spring Boot on Kubernetes"},{"content":"Content and details to be defined soon\u0026hellip;\nIf you have any special requests, please feel free to contact me by mail.\n","permalink":"https://blog.nebrass.fr/incoming-training-playing-with-spring-boot-on-kubernetes-bootcamp/","summary":"Content and details to be defined soon\u0026hellip;\nIf you have any special requests, please feel free to contact me by mail.","title":"Incoming training: Playing with Spring Boot on Kubernetes Bootcamp"},{"content":"Today, I would like to share with you the title of my new book: Playing with Java Microservices on Kubernetes and OpenShift.\nI started working on it, since the January 2018.\nPlaying with Java Microservices on Kubernetes and OpenShift will teach you how to build and design microservices using Java and the Spring platform.\nThis book covers topics related to creating Java microservices and deploy them to Kubernetes and OpenShift.\nTraditionally, Java developers have been used to developing large, complex monolithic applications. The experience of developing and deploying monoliths has been always slow and painful. This book will help Java developers to quickly get started with the features and the concerns of the microservices architecture. It will introduce Docker, Kubernetes and OpenShift to help them deploying their microservices.\nThe book is written for Java developers who wants to build microservices using the Spring Boot/Cloud stack and who wants to deploy them to Kubernetes and OpenShift.\nYou will be guided on how to install the appropriate tools to work properly. For those who are new to Enterprise Development using Spring Boot, you will be introduced to its core principles and main features thru a deep step-by-step tutorial on many components. For experts, this book offers some recipes that illustrate how to split monoliths and implement microservices and deploy them as containers to Kubernetes and OpenShift.\nThe following are some of the key challenges that we will address in this book:\n Introducing Spring Boot/Cloud for beginners Splitting a monolith using the Domain Driven Design approach Implementing the cloud \u0026amp; microservices patterns Rethinking the deployment process Introducing containerization, Docker, Kubernetes and OpenShift  By the end of reading this book, you will have practical hands-on experience of building microservices using Spring Boot/Cloud and you will master deploying them as containers to Kubernetes and OpenShift.\nThis book will be published on Leanpub. I hope that it will be fully published by September 2018. I think of making an Early Access by the end of May 2018 :)\nI hope that it will succeed as did my first one: Pairing Apache Shiro with Java EE 7\n","permalink":"https://blog.nebrass.fr/my-upcoming-book-playing-with-java-microservices-on-kubernetes-and-openshift/","summary":"Today, I would like to share with you the title of my new book: Playing with Java Microservices on Kubernetes and OpenShift.\nI started working on it, since the January 2018.\nPlaying with Java Microservices on Kubernetes and OpenShift will teach you how to build and design microservices using Java and the Spring platform.\nThis book covers topics related to creating Java microservices and deploy them to Kubernetes and OpenShift.","title":"My upcoming book: Playing with Java Microservices on Kubernetes and OpenShift"},{"content":"For the first time, since I started working, I animated a workshop on my lovely school, the Higher Institute of Management of Tunis, Tunisia. The greatest place that provided us with the love \u0026amp; the knowledge! The place where I coded my first lines of Java.\nThe event was organized by Aymen Delly, student \u0026amp; IT club president, and Mrs Nadia Yaacoubi, one of my greatest teachers.\n The content of the workshop, to be presented from 8:00 to 13:00 :\n Introduction to Spring Boot  Introduction to Spring Boot fundamentals Generating project using Spring Initializr Presentation of SpringBoot \u0026amp; autoconfigurations Creating CRUD application: Spring Data JPA + Spring Data REST Presentation of Spring Actuator   Introduction to Docker  Presentation of Containerization vs Virtualization Presentation of Docker\u0026rsquo;s terminology: Image; Container; Dockerfile; Docker-Machine Presentation of Docker-Compose, Docker-Swarm Presentation of Docker Hub \u0026amp; Continuous Delivery using Docker    They were more than 130 students that attended the event, whereas we have predicted to have 100 registered students on Eventbrite.\n I am so glad of this first edition of the workshop ! I hope this was useful for our students !\nThank you Onepoint for sponsoring this event and for making my dreams come true !\n","permalink":"https://blog.nebrass.fr/workshop-report-isg-tunis-november-2017/","summary":"For the first time, since I started working, I animated a workshop on my lovely school, the Higher Institute of Management of Tunis, Tunisia. The greatest place that provided us with the love \u0026amp; the knowledge! The place where I coded my first lines of Java.\nThe event was organized by Aymen Delly, student \u0026amp; IT club president, and Mrs Nadia Yaacoubi, one of my greatest teachers.\n The content of the workshop, to be presented from 8:00 to 13:00 :","title":"Workshop Report: ISG Tunis - November 2017"},{"content":"onepoint was present in the Comutec UTC 2017, which was held on October 19th, 2017, in Le Tigre space at Margny-l√®s-Compi√®gne.\nFor this event, we organized a small challenge for the UTC students: who can answer the questions that I have prepared ?\nThe questions are Java SE fundamentals related questions. The winner is the student that can have the highest correct answer.\nThe two winners got an Apple Watch 3rd generation ! Yeah ! 3rd generation !!\n See you next year @Comutec !!\n","permalink":"https://blog.nebrass.fr/comutec-utc-compi%C3%A8gne-october-2017/","summary":"onepoint was present in the Comutec UTC 2017, which was held on October 19th, 2017, in Le Tigre space at Margny-l√®s-Compi√®gne.\nFor this event, we organized a small challenge for the UTC students: who can answer the questions that I have prepared ?\nThe questions are Java SE fundamentals related questions. The winner is the student that can have the highest correct answer.\nThe two winners got an Apple Watch 3rd generation !","title":"Comutec UTC Compi√®gne - October 2017"},{"content":"Here we go ! Today, I start working¬†as Technical Leader for OnePoint Group, the Leader in Digital Transformation in France \u0026amp; Europe.\nOnepoint?  Founded on 2002 More than 2200 Employees 36 years old as average age 13 offices worldwide Communities \u0026amp; Brainstorming Driven Company  What will I be doing? I will be a member of the Architecture Community, to help architects deploy their solutions in Java. I will be also a member of the Banking, Finance \u0026amp; Insurance Community member. Here I will be working on helping our customers on development, performance \u0026amp; architecture problematics.\nI will also participate in events (organizing \u0026amp; animating) - trainings - mentoring newbies..\n Looking forward to exciting challenges ! Let\u0026rsquo;s do it !!\n","permalink":"https://blog.nebrass.fr/onepoint..-my-new-beginning../","summary":"Here we go ! Today, I start working¬†as Technical Leader for OnePoint Group, the Leader in Digital Transformation in France \u0026amp; Europe.\nOnepoint?  Founded on 2002 More than 2200 Employees 36 years old as average age 13 offices worldwide Communities \u0026amp; Brainstorming Driven Company  What will I be doing? I will be a member of the Architecture Community, to help architects deploy their solutions in Java. I will be also a member of the Banking, Finance \u0026amp; Insurance Community member.","title":"onepoint.. my new beginning.."},{"content":"Summer School is a unique opportunity to learn about developing web and mobile applications that meets the requirements of good design, high scalability and faster integration/deployment.\nTwo parallel tracks will run together and participants have to choose on the two.\nThe first track will be around Java EE enviroment.\nNabrass Lamouchi - Team Leader at Soci√©t√© G√©n√©rale \u0026amp; Netbeans Dream Team member\nDay 1\n Intro to Enterprise Development Intro to Java EE specifications Intro to Java 8 Intro to Spring Boot Legacy and SOA trendies Diving into Microservices architecture Getting into Agility for Microservices Development  Day 2\n Modeling the ecosystem Designing the application Tooling \u0026amp; tooling Coding, coding \u0026amp; coding  Day 3\n Continous Integration \u0026amp; Continous Delivery Docker \u0026amp; AWS wedding MetaData, Quality and availability monitoring Best practices  Requirements: Bring your own laptop with Netbeans or Eclipse installed, and Java JDK 8.\n The second track will be around .NET environment.\nHoussem Dellai - .NET Consultant at Soat \u0026amp; Microsoft MVP\nDay 1\n Intro to Xamarin Intro to Xamarin Forms Creating UI in Xamarin Forms Applying MVVM design pattern and IOC/DI Debugging .NET apps  Day 2\n Intro to ASP.NET Core Creating MVC web applications Developing web servives with Web API Connecting to the SQL Database Working with NoSQL databases (CosmosDB on Azure) Deploying the web app on Azure  Day 3\n Continuous Integration (CI) with VSTS DevOps for mobile apps with VS Mobile Center Unit and UI tests  Requirements: Bring your own laptop with Visual Studio 2015 or 2017 installed.\nInfo: Make sure you subscribe to one of the 2 tracks.\nFinal confirmation will be done by email or phone.\nSubscription link @EventBrite\n","permalink":"https://blog.nebrass.fr/announcement-summer-school-tunisia-2017/","summary":"Summer School is a unique opportunity to learn about developing web and mobile applications that meets the requirements of good design, high scalability and faster integration/deployment.\nTwo parallel tracks will run together and participants have to choose on the two.\nThe first track will be around Java EE enviroment.\nNabrass Lamouchi - Team Leader at Soci√©t√© G√©n√©rale \u0026amp; Netbeans Dream Team member\nDay 1\n Intro to Enterprise Development Intro to Java EE specifications Intro to Java 8 Intro to Spring Boot Legacy and SOA trendies Diving into Microservices architecture Getting into Agility for Microservices Development  Day 2","title":"Announcement: Summer School Tunisia 2017"},{"content":"Today marks my last day at Davidson Consulting, after a great experience of nearly three years. I have enjoyed working for Davidson, even I was excited on some projects.\nMany great achievements:\n Successful project realization for EthiKonsulting Successful contract with Airbus Defense \u0026amp; Space Successful contract with Canal+ Successful contract with Soci√©t√© g√©n√©rale Corporate and Investment Banking Successful trainings for more than 50 employees in many fields (Agile, SOA, Java SE/EE, Docker, \u0026hellip;) Partnership with Oracle, for sponsoring NetBeans Day France  I\u0026rsquo;m a Java EE developer, I believe and love the Continuous Integration and the Continuous Delivery, this lead me to seek the¬†Continuous Challenge \u0026amp; Learning. Bernard Shaw said:\n Progress is impossible without change, and those who cannot change their minds cannot change anything.\n\u0026ndash; Bernard Shaw\n As said¬†Casare:\n We do not remember days, we remember moments.\n\u0026ndash; Cesare Pavese\n Now looking forward to start with the new hire orientation soon!\n","permalink":"https://blog.nebrass.fr/good-bye-davidson-consulting/","summary":"Today marks my last day at Davidson Consulting, after a great experience of nearly three years. I have enjoyed working for Davidson, even I was excited on some projects.\nMany great achievements:\n Successful project realization for EthiKonsulting Successful contract with Airbus Defense \u0026amp; Space Successful contract with Canal+ Successful contract with Soci√©t√© g√©n√©rale Corporate and Investment Banking Successful trainings for more than 50 employees in many fields (Agile, SOA, Java SE/EE, Docker, \u0026hellip;) Partnership with Oracle, for sponsoring NetBeans Day France  I\u0026rsquo;m a Java EE developer, I believe and love the Continuous Integration and the Continuous Delivery, this lead me to seek the¬†Continuous Challenge \u0026amp; Learning.","title":"Good bye Davidson Consulting!"},{"content":"I had the honour to be the first guest of the NTIC Community, of Capgemini Nantes, to animate a \u0026ldquo;Two-Talks Event\u0026rdquo;, a new action made by the managers of NTIC Community to boost the brainstorming and motivation of the IT Managers and Technical Teams.\n The subject of the first edition¬†was \u0026ldquo;Security for Managers and Technicals\u0026rdquo;.\n My talks:\n Security for IT Managers : 14:30 - 15:30 Presentation of the OWASP Barbarus : 16:00 - 17:00  My Presentations:\n  Security for IT Managers\n   Presentation of the OWASP Barbarus\n   Many thanks for J√©r√¥me Salles \u0026amp; Fabien Bontemps¬†for the event organization \u0026amp; coordination.\n","permalink":"https://blog.nebrass.fr/owasp-day-capgemini-nantes/","summary":"I had the honour to be the first guest of the NTIC Community, of Capgemini Nantes, to animate a \u0026ldquo;Two-Talks Event\u0026rdquo;, a new action made by the managers of NTIC Community to boost the brainstorming and motivation of the IT Managers and Technical Teams.\n The subject of the first edition¬†was \u0026ldquo;Security for Managers and Technicals\u0026rdquo;.\n My talks:\n Security for IT Managers : 14:30 - 15:30 Presentation of the OWASP Barbarus : 16:00 - 17:00  My Presentations:","title":"OWASP Day - Capgemini Nantes"},{"content":"I participated today, the 6th of May, to the¬†first edition of the Modern Software Development Bootcamp¬†in Tunis, Tunisia.\nThe event was hosted by IntilaQ Center, El Ghazela Technopark, Tunisia.\n Presentation of the Bootcamp:\nThe event is organized by¬†Houssem Dellai, Microsoft MVP \u0026amp; Xamarin Lead Developer working at IntilaQ Center Tunisia \u0026amp; SOAT France. This event is dedicated to the computer engineering schools students, and aims to introduce them to the trendiest technologies of the market, such as Xamarin (C#/.NET), Java 8, SpringBoot, Docker, Microservices\u0026hellip;\nThe sessions are 3 Hours workshop that were done in parallel. The idea is to provide 100% practical workshops to participants, so they can make exercices live on the selected subject.\nThe planning of the event:\n Welcome and overview of the day (Houssem Dellai, Microsoft MVP) Workshop 1: Xamarin Overview (Houssem Dellai, Microsoft MVP) Workshop 2:¬†Playing with Spring Boot \u0026amp; Docker (Nebrass Lamouchi, Davidson SI)  The content of my workshop:\n Introductions  L\u0026rsquo;architecture Client-Serveur Java SE vs Java EE Java EE vs¬†Spring Framework Some terminologies   Introduction √† Spring Boot  Introduction des bases de d√©veloppement Spring Boot: G√©n√©ration de projet via Spring Initializr Pr√©sentation du SpringBoot \u0026amp; autoconfigurations Cr√©ation d\u0026rsquo;une application CRUD: Spring Data JPA + Spring Data REST Pr√©sentation de Spring Actuator   Introduction √† Docker  Pr√©sentation de Docker, de la virtualisation et du packaging Docker Containers Pr√©sentation de la terminologie Docker: Image; Container; Dockerfile; Docker-Machine Pr√©sentation de Docker-Compose, Docker-Swarm Pr√©sentation de Docker Hub et de la livraison continue via Docker    Some pictures from the event:\n// TO BE ADDED SOON\nMy presentation:\n Many thanks for¬†Houssem Dellai¬†for the event organization \u0026amp; coordination and for the students that were so motivated for the sessions. Good luck guys !\n","permalink":"https://blog.nebrass.fr/modern-software-development-bootcamp-tunis/","summary":"I participated today, the 6th of May, to the¬†first edition of the Modern Software Development Bootcamp¬†in Tunis, Tunisia.\nThe event was hosted by IntilaQ Center, El Ghazela Technopark, Tunisia.\n Presentation of the Bootcamp:\nThe event is organized by¬†Houssem Dellai, Microsoft MVP \u0026amp; Xamarin Lead Developer working at IntilaQ Center Tunisia \u0026amp; SOAT France. This event is dedicated to the computer engineering schools students, and aims to introduce them to the trendiest technologies of the market, such as Xamarin (C#/.","title":"Modern Software Development Bootcamp - Tunis"},{"content":"This article is originally published April 13th, 2017 at JaxEnter website: Link\nIn this tutorial¬†you will learn¬†how to write a Java EE application using two of the trendiest technologies: Spring Boot and Docker in NetBeans IDE.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.¬†Docker is an open source project that automates the deployment of applications inside software containers.\nToday I‚Äôd like to make¬†a tutorial in which a simple Spring Boot application is¬†deployed using Docker.\nRequirements  JDK 8 recommended We need¬†Docker, which only runs on 64-bit machines. See https://docs.docker.com/installation/#installation¬†for details on setting Docker up for your machine. Before proceeding further, verify you can run¬†docker¬†commands from the shell. If you are using boot2docker you need to run that¬†first. NetBeans IDE Development build, to be downloaded from http://bits.netbeans.org/download/trunk/nightly/latest/  For this tutorial I will be using the Development build of NetBeans IDE, as the Docker integration will be in the next release of NetBeans.\n Grab the Java EE bundle from the download section.\nDocker support The Docker node is available in the Services window.¬†To connect to a Docker instance, make sure that the Docker daemon is working.\nOn NetBeans go to the Services window:\n Next‚Ä¶\n After configuring the Docker instance, you can see all your images and containers, as well as¬†the state of each container:\n The Docker integration in NetBeans supports many Docker operations such as running or pushing images, or starting and stopping containers:\n Spring Boot support I recently found a great Spring Boot plugin: NB-SpringBoot created by Alex Falappa\nThe features provided by the plugin:\n Spring Boot new Maven project wizards:  Basic project Project generated by¬†Spring Initializr¬†service   Enhanced properties file editor:  completion and documentation of configuration properties names completion and documentation of configuration properties values (hints¬†in configuration metadata)   Spring Boot¬†file templates:  CommandlineRunner¬†annotated classes ApplicationRunner¬†annotated classes application.properties¬†files ConfigurationProperties¬†annotated classes additional-spring-configuration-metadata.json¬†files   Spring¬†file templates:  Component¬†annotated classes Configuration¬†annotated classes Service¬†annotated classes   Spring MVC¬†file templates:  Controller¬†annotated classes RestController¬†annotated classes   Spring Data¬†file templates:  interfaces extending¬†Repository   Code generators in¬†pom.xml¬†files:  Add spring configuration processor dependency Add¬†spring-devtools¬†\u0026amp; spring-actuator¬†dependencies    Creating our Boot application After installing the plugin, we can create a new project using the Spring Initializr directly from NetBeans:\n Next‚Ä¶\n Next, select the Spring Boot version and the needed dependencies.\n I just picked the Web¬†and¬†Actuator dependencies for my sample project.\nNext‚Ä¶\n So now we have created our project base using Spring Initializer.¬†The structure of the created project would be:\n Now we create a new RestController:\n The controller will be a classic \u0026ldquo;Hello World\u0026rdquo; Rest boundary:\n1 2 3 4 5 6 7 8  @RestController public class GreetingsController { @RequestMapping(path = \u0026#34;/time\u0026#34;, method = GET) public String sayHelloWorld(){ return \u0026#34;Hello World @ \u0026#34; + LocalTime.now(); } }   Run the project and go to http://localhost:8080/time to test the application:\n Another great feature of the SpringBootPlugin is the powerful code completion especially on properties files, decorated with the Javadoc of the suggested item:\n To deploy our application using Docker, we will need to prepare our container.\nNow we need to build our first container, which will run our sample microservice. We do this by defining the Dockerfile, which is a text file containing series of commands that tell Docker how to build an image. Once we write¬†a Dockerfile, we can then repeatedly build an image by running the docker build command.\nIn NetBeans, create the new Dockerfile:\n Create the \u0026ldquo;Dockerfile\u0026rdquo; in the \u0026ldquo;src/main/docker\u0026rdquo; folder.\n An example of a Dockerfile content that we can use :\n1 2 3 4 5 6  FROMjava:8VOLUME/tmpADD nb-springboot-docker.jar app.jarEXPOSE8080RUN sh -c \u0026#39;touch /app.jar\u0026#39;ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;]  As you can see, the Dockerfile is very simple. It consists of the following instructions:\n FROM ‚Äì the FROM instruction specifies the starting image. In this example, it is the Java 8 image mentioned above. The first time you build this image, Docker will download the Java 8 image from the central Docker registry VOLUME ‚Äì define that a volume named /tmp should exist MAINTAINER ‚Äì this instruction simply specifies the author ADD ‚Äì this instruction copies the JAR file to the specified location in the image. Note that working directory for the dockerfile/java image is /data so that‚Äôs why we are putting the JAR file there. EXPOSE ‚Äì this instruction tells Docker that this server process will listen on port 8080 RUN ‚Äì used to \u0026ldquo;touch\u0026rdquo; the jar file so that it has a file modification time (Docker creates all container files in an \u0026ldquo;unmodified\u0026rdquo; state by default). This actually isn‚Äôt important for the simple app that we wrote, but any static content (e.g. \u0026ldquo;index.html\u0026rdquo;) would require the file to have a modification time. ENTRYPOINT ‚Äì is the \u0026ldquo;what to run to ‚Äòstart‚Äô\u0026rdquo; command ‚Äî we run Java, setting properties, for example, a quick additional property to reduce¬†Tomcat startup time¬†we added a system property pointing to \u0026ldquo;/dev/urandom\u0026rdquo; as a source of entropy, and then point it at our jar.  To build the image you can use some tooling for Maven (Spotify) or Gradle (Transmode).\nFor our example we will be using Maven.\nBuild a Docker Image with Maven In the Maven pom.xml you should add a new plugin like this (see the Docker-Maven-Plugin documentation for more options):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.4.10\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;imageName\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/imageName\u0026gt; \u0026lt;dockerDirectory\u0026gt;src/main/docker\u0026lt;/dockerDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   The plugin configuration is simple:\n imageName: The name of the generated Docker image dockerDirectory: The directory where you can¬†find the Dockerfile resources: The resources files to copy from the target directory to the Docker build (alongside the Dockerfile)  You can build an image with the configurations mentioned above by running this command:\n1  $ mvn clean package docker:build   After the execution of the command, when you check the list of Docker images, you will discover¬†that the built image is added to the list¬†:\n¬†There is also the java:8 image added to the list of images. This is because the nebrass/nb-springboot-docker is based on the java:8 image.\nYou can run the built image just by clicking the¬†Run button.\n Next, configure your container¬†:\n Next, configure the ports binding¬†:\n A great feature is provided by NetBeans: ¬´¬†Add Exposed¬†¬ª will bind the exposed ports automatically in the created container.\nBy clicking the¬†¬´¬†Finish¬†¬ª button, the container will be launched, and a log window will appear in NetBeans to show you how your container is executed:\n Now, we check the¬†application¬†at http://192.168.99.100:8080/time\n Great¬†! This is working¬†!\nNext, we add a Maven Task that we will be used to run the container. It can be used for example to bind the task to some Maven Phase (package, deploy‚Ä¶).\nThe command line used to launch the container¬†:\n1  $ docker run ‚Äìd ‚Äìp 8080:8080 nebrass/nb-springboot-docker   So our Maven Task will execute the same command. We will use the exec-maven-plugin to launch the same command:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;DockerRun\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;exec\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;/usr/local/bin/docker\u0026lt;/executable\u0026gt; \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;run\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-d\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-p\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;8080:8080\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   So to run the container from Maven:\n1  $ mvn exec:exec   The output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  MacBook-Pro-de-Nebrass:nb-springboot-docker nebrass$ mvn exec:exec [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building nb-springboot-docker 0.0.1-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- exec-maven-plugin:1.3.2:exec (default-cli) @ nb-springboot-docker --- bf17bdaf0d9cb74e9fb94b20f9865d4424442413cd12a21804e252cce0976354 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.051 s [INFO] Finished at: 2016-07-06T22:25:25+02:00 [INFO] Final Memory: 14M/245M [INFO] ------------------------------------------------------------------------   The app now is deployed and running in the created Docker container and our Greetings service is accessible at http://DOCKER_HOST:8080/time (to see your running container docker ps)\nThe global Maven command that does the build, tests, creates the Docker image and runs it:\n1  $ mvn clean install docker:build exec:exec   This is the command that I added to my NetBeans as custom Maven Goal:\n Next..\n The output in NetBeans:\n Summary Congratulations! You have created a Spring Boot microservice that is executed and deployed in a Docker container¬†which can be configured and managed in the same place where we handle the Spring Boot application.\nThe Spring Boot plugin developed by Alex Falappa is a great add-on and it‚Äôs worth trying it, you will love it for sure.\n","permalink":"https://blog.nebrass.fr/new-version-of-playing-with-spring-boot-and-docker-in-netbeans-ide/","summary":"This article is originally published April 13th, 2017 at JaxEnter website: Link\nIn this tutorial¬†you will learn¬†how to write a Java EE application using two of the trendiest technologies: Spring Boot and Docker in NetBeans IDE.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.¬†Docker is an open source project that automates the deployment of applications inside software containers.","title":"New version of \"Playing with Spring Boot and Docker in NetBeans IDE\""},{"content":"This year, the NetBeans France Community decided to make¬†the NetBeans Day France 2017 in Grenoble City, at the 7th of February.\n The event was hosted by Grenoble Informatics Laboratory, Grenoble Alpes University.\nThe planning of the event:\n Welcome and overview of the day (Geertjan Wielenga, NetBeans) Session¬†1:¬†Eclipse Vert.x¬†\u0026amp;¬†OpenShift (Cl√©ment Escoffier, Red Hat) Session¬†2: Enterprise JavaScript Development with Oracle JET (Geertjan Wielenga, NetBeans) Session 3: Playing with Spring Boot \u0026amp; Docker (Nebrass Lamouchi, Davidson SI) Session 4: JBoss Forge, WildFly and Java EE (Emmanuel Hugonnet, Red Hat)  Some pictures from the event:\n// TO BE DONE SOON\nMy presentation:\n Many thanks for Emmanuel Hugonnet for the event organization \u0026amp; coordination.\nLink to the Github Repository of the Workshop\n","permalink":"https://blog.nebrass.fr/netbeans-day-france-2017-grenoble/","summary":"This year, the NetBeans France Community decided to make¬†the NetBeans Day France 2017 in Grenoble City, at the 7th of February.\n The event was hosted by Grenoble Informatics Laboratory, Grenoble Alpes University.\nThe planning of the event:\n Welcome and overview of the day (Geertjan Wielenga, NetBeans) Session¬†1:¬†Eclipse Vert.x¬†\u0026amp;¬†OpenShift (Cl√©ment Escoffier, Red Hat) Session¬†2: Enterprise JavaScript Development with Oracle JET (Geertjan Wielenga, NetBeans) Session 3: Playing with Spring Boot \u0026amp; Docker (Nebrass Lamouchi, Davidson SI) Session 4: JBoss Forge, WildFly and Java EE (Emmanuel Hugonnet, Red Hat)  Some pictures from the event:","title":"NetBeans Day France 2017 - Grenoble"},{"content":"This week, I tried the \u0026ldquo;Self¬†Paced¬†Online\u0026rdquo; courses provided by the Docker Community. These courses were built by many mentors \u0026amp; Docker experts, covering many basic \u0026amp; advanced Docker concepts.\nThe offered¬†courses:\nDeveloper - Beginner Linux Containers\nThis tutorial will guide you through the steps involved in setting up your computer, running your first containers, deploying a web application with Docker and running a multi-container voting app with Docker Compose.\n Developer - Beginner Windows Containers\nThis tutorial will walk you through setting up your environment, running basic containers and creating a Docker Compose multi-container application using Windows containers.\n Developer - Intermediate (both Linux and Windows)\nThis tutorial teaches you how to network your containers, how you can manage data inside and between your containers and how to use Docker Cloud to build your image from source and use developer tools and programming languages with Docker.\n Operations - Beginner\nThe beginner part of the Ops tutorial will teach you how to set up a swarm, how to use it to host your own registry, how to build your app container images and how to deploy and scale a distributed application called Dockercoins.\n Operations - Intermediate\nFrom global container scheduling, overlay networks troubleshooting, dealing with stateful services and node management, this tutorial will show you how to operate your swarm cluster at scale and take you on a swarm mode deep dive.\n After taking each course, there is a Quiz to validate acquired knowledge, and a certificate to proof course completion.\nOperations - Beginner course\nDeveloper - Beginner Linux Containers course\nDeveloper - Intermediate course\n I really appreciate the courses layout, and the content provided and divided by knowledge level.\nI recommend these courses for everyone interested in gaining knowledge on Docker.\nThe courses are available here.\nGood luck !\n","permalink":"https://blog.nebrass.fr/free-self-paced-courses-to-learn-docker/","summary":"This week, I tried the \u0026ldquo;Self¬†Paced¬†Online\u0026rdquo; courses provided by the Docker Community. These courses were built by many mentors \u0026amp; Docker experts, covering many basic \u0026amp; advanced Docker concepts.\nThe offered¬†courses:\nDeveloper - Beginner Linux Containers\nThis tutorial will guide you through the steps involved in setting up your computer, running your first containers, deploying a web application with Docker and running a multi-container voting app with Docker Compose.","title":"Free self-paced courses to learn Docker"},{"content":"In this tutorial¬†you will learn¬†how to write a Java EE application using two of the trendiest technologies: Spring Boot and Docker in NetBeans IDE.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.¬†Docker is an open source project that automates the deployment of applications inside software containers.\nToday I‚Äôd like to make¬†a tutorial in which a simple Spring Boot application is¬†deployed using Docker.\nRequirements  JDK 8 recommended We need Docker, which only runs on 64-bit machines. See https://docs.docker.com/installation/#installation for details on setting Docker up for your machine. Before proceeding further, verify you can run docker commands from the shell. If you are using boot2docker you need to run that first. NetBeans IDE Development build, to be downloaded from http://bits.netbeans.org/download/trunk/nightly/latest/  For this tutorial I will be using the Development build of NetBeans IDE, as the Docker integration will be in the next release of NetBeans.\n Grab the Java EE bundle from the download section.\nDocker support The Docker node is available in the Services window.¬†To connect to a Docker instance, make sure that the Docker daemon is working.\nOn NetBeans go to the Services window:\n Next‚Ä¶\n After configuring the Docker instance, you can see all your images and containers, as well as¬†the state of each container:\n The Docker integration in NetBeans supports many Docker operations such as running or pushing images, or starting and stopping containers:\n Spring Boot support I recently found a great Spring Boot plugin: NB-SpringBoot created by Alex Falappa\nThe features provided by the plugin:\n Spring Boot new Maven project wizards:  Basic project Project generated by Spring Initializr service   Enhanced properties file editor:  completion and documentation of configuration properties names completion and documentation of configuration properties values (hints in configuration metadata)   Spring Boot file templates:  CommandlineRunner annotated classes ApplicationRunner annotated classes application.properties files ConfigurationProperties annotated classes additional-spring-configuration-metadata.json files   Spring file templates:  Component annotated classes Configuration annotated classes Service annotated classes   Spring MVC file templates:  Controller annotated classes RestController annotated classes   Spring Data file templates:  interfaces extending Repository   Code generators in pom.xml files:  Add spring configuration processor dependency Add spring-devtools \u0026amp; spring-actuator dependencies    Creating our Boot application After installing the plugin, we can create a new project using the Spring Initializr directly from NetBeans:\n Next‚Ä¶\n Next, select the Spring Boot version and the needed dependencies.\n I just picked the Web¬†and¬†Actuator dependencies for my sample project.\nNext‚Ä¶\n So now we have created our project base using Spring Initializer.¬†The structure of the created project would be:\n Now we create a new RestController:\n The controller will be a classic \u0026ldquo;Hello World\u0026rdquo; Rest boundary:\n1 2 3 4 5 6 7 8  @RestController public class GreetingsController { @RequestMapping(path = \u0026#34;/time\u0026#34;, method = GET) public String sayHelloWorld(){ return \u0026#34;Hello World @ \u0026#34; + LocalTime.now(); } }   Run the project and go to http://localhost:8080/time to test the application:\n Another great feature of the SpringBootPlugin is the powerful code completion especially on properties files, decorated with the Javadoc of the suggested item:\n To deploy our application using Docker, we will need to prepare our container.\nNow we need to build our first container, which will run our sample microservice. We do this by defining the Dockerfile, which is a text file containing series of commands that tell Docker how to build an image. Once we write¬†a Dockerfile, we can then repeatedly build an image by running the docker buildcommand.\nIn NetBeans, create the new Dockerfile:\n Create the \u0026ldquo;Dockerfile\u0026rdquo; in the \u0026ldquo;src/main/docker\u0026rdquo; folder.\n ¬†An example of a Dockerfile content that we can use :\n1 2 3 4 5 6  FROMjava:8VOLUME/tmpADD nb-springboot-docker.jar app.jarEXPOSE8080RUN sh -c \u0026#39;touch /app.jar\u0026#39;ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;]  As you can see, the Dockerfile is very simple. It consists of the following instructions:\n FROM ‚Äì the FROM instruction specifies the starting image. In this example, it is the Java 8 image mentioned above. The first time you build this image, Docker will download the Java 8 image from the central Docker registry VOLUME ‚Äì define that a volume named /tmp should exist MAINTAINER ‚Äì this instruction simply specifies the author ADD ‚Äì this instruction copies the JAR file to the specified location in the image. Note that working directory for the dockerfile/java image is /data so that‚Äôs why we are putting the JAR file there. EXPOSE ‚Äì this instruction tells Docker that this server process will listen on port 8080 RUN ‚Äì used to \u0026ldquo;touch\u0026rdquo; the jar file so that it has a file modification time (Docker creates all container files in an \u0026ldquo;unmodified\u0026rdquo; state by default). This actually isn‚Äôt important for the simple app that we wrote, but any static content (e.g. \u0026ldquo;index.html\u0026rdquo;) would require the file to have a modification time. ENTRYPOINT ‚Äì is the \u0026ldquo;what to run to ‚Äòstart‚Äô\u0026rdquo; command ‚Äî we run Java, setting properties, for example, a quick additional property to reduce¬†Tomcat startup time¬†we added a system property pointing to \u0026ldquo;/dev/urandom\u0026rdquo; as a source of entropy, and then point it at our jar.  To build the image you can use some tooling for Maven (Spotify) or Gradle (Transmode).\nFor our example we will be using Maven.\nBuild a Docker Image with Maven In the Maven pom.xml you should add a new plugin like this (see the Docker-Maven-Plugin documentation for more options):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.4.10\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;imageName\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/imageName\u0026gt; \u0026lt;dockerDirectory\u0026gt;src/main/docker\u0026lt;/dockerDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   The plugin configuration is simple:\n imageName: The name of the generated Docker image dockerDirectory: The directory where you can¬†find the Dockerfile resources: The resources files to copy from the target directory to the Docker build (alongside the Dockerfile)  You can build an image with the configurations mentioned above by running this command:\n1  $ mvn clean package docker:build   After the execution of the command, when you check the list of Docker images, you will discover¬†that the built image is added to the list¬†:\n There is also the java:8 image added to the list of images. This is because the nebrass/nb-springboot-docker is based on the java:8 image.\nYou can run the built image just by clicking the¬†Run button.\n Next, configure your container¬†:\n Next, configure the ports binding¬†:\n A great feature is provided by NetBeans: ¬´¬†Add Exposed¬†¬ª will bind the exposed ports automatically in the created container.\nBy clicking the¬†¬´¬†Finish¬†¬ª button, the container will be launched, and a log window will appear in NetBeans to show you how your container is executed:\n Now, we check the¬†application¬†at http://192.168.99.100:8080/time\n Great¬†! This is working¬†!\nNext, we add a Maven Task that we will be used to run the container. It can be used for example to bind the task to some Maven Phase (package, deploy‚Ä¶).\nThe command line used to launch the container¬†:\n1  $ docker run ‚Äìd ‚Äìp 8080:8080 nebrass/nb-springboot-docker   So our Maven Task will execute the same command. We will use the exec-maven-plugin to launch the same command:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;DockerRun\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;exec\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;/usr/local/bin/docker\u0026lt;/executable\u0026gt; \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;run\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-d\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-p\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;8080:8080\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   So to run the container from Maven:\n1  $ mvn exec:exec   The output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  MacBook-Pro-de-Nebrass:nb-springboot-docker nebrass$ mvn exec:exec [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building nb-springboot-docker 0.0.1-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- exec-maven-plugin:1.3.2:exec (default-cli) @ nb-springboot-docker --- bf17bdaf0d9cb74e9fb94b20f9865d4424442413cd12a21804e252cce0976354 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.051 s [INFO] Finished at: 2016-07-06T22:25:25+02:00 [INFO] Final Memory: 14M/245M [INFO] ------------------------------------------------------------------------   The app now is deployed and running in the created Docker container and our Greetings service is accessible at http://DOCKER_HOST:8080/time (to see your running container docker ps)\nThe global Maven command that does the build, tests, creates the Docker image and runs it:\n1  $ mvn clean install docker:build exec:exec   This is the command that I added to my NetBeans as custom Maven Goal:\n Next..\n The output in NetBeans:\n Summary Congratulations! You have created a Spring Boot microservice that is executed and deployed in a Docker containerwhich can be configured and managed in the same place where we handle the Spring Boot application.\nThe Spring Boot plugin developed by Alex Falappa is a great add-on and it‚Äôs worth trying it, you will love it for sure.\n","permalink":"https://blog.nebrass.fr/playing-with-spring-boot-and-docker-in-netbeans-ide/","summary":"In this tutorial¬†you will learn¬†how to write a Java EE application using two of the trendiest technologies: Spring Boot and Docker in NetBeans IDE.\nSpring Boot is an innovative project that aims to make it easy to create Spring applications by simplifying the configuration and deployment actions through its convention over configuration based setup.¬†Docker is an open source project that automates the deployment of applications inside software containers.","title":"Playing with Spring Boot and Docker in NetBeans IDE"},{"content":"When securing systems, two elements of security are important: authentication and authorization. Though the two terms mean different things, they are sometimes used interchangeably because of their respective roles in application security.\nGet started with the fundamentals of web authentication and authorization using Apache Shiro Framework.\nApache Shiro is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management.\nLearn how to use Shiro in a JavaEE7 application and how to use it in a web application.\n\n This book will help you find out what Shiro actually is, and will help you to secure your Java EE project from scratch and to understand the security philosophy.\nYou will learn the big picture and how to set up Apache Shiro, which will give you a better understanding of the fundamentals of the framework. You will be introduced to the authentication and authorization flows and the different possible models of security.\nYou will get everything you need to start with Shiro immediately with just essential information.\nAll the associated code is available on github.\n Release Date:¬†May¬†2016 Language: English Pages:¬†74 ISBN:¬†978-1-365-12404-4  The book is available for free from InfoQ : here\n","permalink":"https://blog.nebrass.fr/pairing-apache-shiro-and-java-ee-7/","summary":"When securing systems, two elements of security are important: authentication and authorization. Though the two terms mean different things, they are sometimes used interchangeably because of their respective roles in application security.\nGet started with the fundamentals of web authentication and authorization using Apache Shiro Framework.\nApache Shiro is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management.\nLearn how to use Shiro in a JavaEE7 application and how to use it in a web application.","title":"Pairing Apache Shiro and Java EE 7"},{"content":"I\u0026rsquo;m excited to announce that I have been accepted to be a member of the NetBeans Dream Team, the team of highly skilled and passionate Java developers \u0026amp; NetBeans users. I\u0026rsquo;m so happy to be a member of such great team. It was one of my big dreams.\nPresentation of the NetBeans Dream Team The NetBeans Dream Team is a community-driven group of highly skilled NetBeans users and contributors. They participate at NetBeans developer events, on mailing lists and developer forums, providing new, interesting and informative content as well as developing new and creative ways to promote NetBeans.\nMission Statement The NetBeans Dream Team strives to make the NetBeans open source project more accessible to our user, contributor, and partner communities.\nRoles \u0026amp; Responsibilities Act \u0026amp; represent the interests of NetBeans in a positive, professional manner Spend a pre-determined (by each individual person) amount of time on NetBeans related activities a week. Can include: blogging, writing content, localization work, answering questions on mailing list \u0026amp; representing NetBeans on industry forums, NetCAT participation, and other forms of contributing. See the new members section. Act as a local representative and contact for NetBeans \u0026amp; their skill set Work to improve the NetBeans IDE and Platform.\n Many thanks for Geertjan Wielenga¬†for guiding me to the Great NetBeans path, many thanks¬†to the NetBeans team¬†for giving me this chance, and for the NetBeans Dream Team members for approving my membership request.¬†It will be a great opportunity to work with great team and great highly skilled people.¬†I hope, I will¬†meet your expectations and provide the necessary help to the team.\n","permalink":"https://blog.nebrass.fr/im-now-a-netbeans-dream-team-member/","summary":"I\u0026rsquo;m excited to announce that I have been accepted to be a member of the NetBeans Dream Team, the team of highly skilled and passionate Java developers \u0026amp; NetBeans users. I\u0026rsquo;m so happy to be a member of such great team. It was one of my big dreams.\nPresentation of the NetBeans Dream Team The NetBeans Dream Team is a community-driven group of highly skilled NetBeans users and contributors. They participate at NetBeans developer events, on mailing lists and developer forums, providing new, interesting and informative content as well as developing new and creative ways to promote NetBeans.","title":"I'm now a NetBeans Dream Team Member!"},{"content":"NetBeans Day Paris, for the first time in France, held on the October 16th, 2015. It was a very nice opportunity to meet NetBeans users and developers in Paris Area.\nMy presentation is¬†a live coding session showing how to make a sample app from scratch on NetBeans, using many trendy technologies and APIs:\n IDE: NetBeans 8.0.2 Server: Payara Server 4.1.153 Backend: Java EE 7 [JPA 2.1, EJB 3.2, CDI 1.1, JAX-RS 2.0], REST Standards, CORS Frontend: HTML5, AngularJS 1.4.x \u0026amp; Twitter Bootstrap 3.3.x Testing: JUnit 4.x Github for hosting the app code  The video (French):\n  The slides:\n Some feedbacks:\n Geertjan Wielenga - Oracle : NetBeans Day Paris in Tweets ! Emmanuel Hugonnet - Red Hat : NetBeans Day Paris 2015 Payara Consulting : Payara at the NetBeans Day Paris  Hope to see you again at NetBeans Day Paris next year ! √Ä tr√®s bient√¥t !\n","permalink":"https://blog.nebrass.fr/talk-report-from-javaee-to-angularjs-video-slides/","summary":"NetBeans Day Paris, for the first time in France, held on the October 16th, 2015. It was a very nice opportunity to meet NetBeans users and developers in Paris Area.\nMy presentation is¬†a live coding session showing how to make a sample app from scratch on NetBeans, using many trendy technologies and APIs:\n IDE: NetBeans 8.0.2 Server: Payara Server 4.1.153 Backend: Java EE 7 [JPA 2.1, EJB 3.2, CDI 1.","title":"Talk Report: From JavaEE to AngularJS - Video \u0026 Slides"},{"content":"Today I finished reading the Beginning NetBeans IDE for Java Developers, a masterpiece written by Geertjan Wielenga, a product manager in the NetBeans Team, and published by Apress (August, 2015).\n To make your work a masterpiece, first master the tools of your work.\n-‚Äî Chinese adage\n The book covers all its¬†subjects in only 280 pages, which is interesting! No painfully long introductions and debriefings.¬†Instead, it has only the content that actually matters.\nStructured Insights into Key Features of¬†NetBeans IDE During¬†NetBeans Day Paris, on October 16th, 2015, I received the book from Geertjan himself.¬†I skipped the first chapter, which describes installation, as I have been using NetBeans for more than three years, and so I‚Äôve already had it installed for a while now. Next,¬†chapter 2 is a soft hands-on procedure for getting started with¬†NetBeans¬†IDE, focused on creating¬†a \u0026ldquo;Hello World\u0026rdquo; application. When working on this simple application, Geertjan shows NetBeans IDE‚Äôs basic windows, commands, and menus.\n\n Chapter 3 covers the Java Editor, which is the most important component of¬†a Java IDE. That‚Äôs why the author allocates to¬†this chapter more pages and more screenshots than any other chapter of the book. In this chapter, Geertjan focuses on¬†the great editor features provided by NetBeans IDE, such as editor customizations,¬†code completion,¬†templates, formatting, as well as¬†search and replace features. Next, in chapter 4, you are provided with a general overview of the¬†most important and useful¬†wizards that are included in the IDE, such as those for creating a new project or file, as well as where and how to efficiently edit project properties.¬†Chapter 5 is very awesome! In this chapter, there is a step-by-step guide for fully working through the process of Java EE application creation. Before creating the application, there is an overview of setting up the tools and technologies that relate to Java EE, such as the application server and the database server.\nChapter 6 is a full showcase about the code analysis and refactoring features provided by NetBeans IDE, which are¬†great features needed by any software craftsman.¬†Chapter 7 is a test-driven chapter. I found in it many useful integration overviews of testing frameworks, including JUnit, TestNG, Arquilian, and more‚Ä¶ and even including¬†the great SonarQube.¬†Chapter 8 is my favorite chapter: Debugging! As debugging is such an¬†important feature of the IDE, this chapter has been written perfectly to help every NetBeans user fully understand¬†how to defeat bugs.¬†Chapter 9 is a great¬†user guide for profiling and tuning Java applications. Moreover, Geertjan crosses the barriers and shows even how to use the Apache JMeter plugin in¬†NetBeans IDE to guarantee a great coupling between the many developer tools.¬†Chapter 10 is the last chapter of the masterpiece,¬†where¬†we find a great user guide for using Git in NetBeans IDE.\nConclusion It is very helpful to master the basic concepts and features of the IDE that you are using. As this book dives right into what‚Äôs noteworthy for NetBeans IDE, all the great and useful features are explained in a very nice way with great screenshots and small examples.\n It‚Äôs always the small pieces that make the big picture.\n-‚Äî Unknown\n After three years of using NetBeans IDE, I think this book is the developer compass to correctly improve anyone‚Äôs¬†NetBeans experience.¬†The experience of reading this book was also¬†great. I think it is really a must-have for every NetBeans user, even those who are experienced¬†with¬†it.\nMy rating for the book¬†is‚Ä¶¬†9/10!\nFurther Reading\n Geertjan Wielenga @geertjanw: https://blogs.oracle.com/geertjan/ Buy the book at Apress @apress: http://www.apress.com/9781484212585 Buy the book at Amazon: http://amzn.to/1YpOACT  ","permalink":"https://blog.nebrass.fr/book-review-beginning-netbeans-ide-for-java-developers/","summary":"Today I finished reading the Beginning NetBeans IDE for Java Developers, a masterpiece written by Geertjan Wielenga, a product manager in the NetBeans Team, and published by Apress (August, 2015).\n To make your work a masterpiece, first master the tools of your work.\n-‚Äî Chinese adage\n The book covers all its¬†subjects in only 280 pages, which is interesting! No painfully long introductions and debriefings.¬†Instead, it has only the content that actually matters.","title":"Book Review: 'Beginning NetBeans IDE For Java Developers'"}]